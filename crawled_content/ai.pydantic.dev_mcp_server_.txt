PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
Server
Table of contents
MCP Server
Simple client
MCP Sampling
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
MCP Server
Simple client
MCP Sampling
Server
PydanticAI models can also be used within MCP Servers.
MCP Server
Here's a simple example of a
Python MCP server
using PydanticAI within a tool call:
mcp_server.py
from
mcp.server.fastmcp
import
FastMCP
from
pydantic_ai
import
Agent
server
=
FastMCP
(
'PydanticAI Server'
)
server_agent
=
Agent
(
'anthropic:claude-3-5-haiku-latest'
,
system_prompt
=
'always reply in rhyme'
)
@server
.
tool
()
async
def
poet
(
theme
:
str
)
->
str
:
"""Poem generator"""
r
=
await
server_agent
.
run
(
f
'write a poem about
{
theme
}
'
)
return
r
.
output
if
__name__
==
'__main__'
:
server
.
run
()
Simple client
This server can be queried with any MCP client. Here is an example using the Python SDK directly:
mcp_client.py
import
asyncio
import
os
from
mcp
import
ClientSession
,
StdioServerParameters
from
mcp.client.stdio
import
stdio_client
async
def
client
():
server_params
=
StdioServerParameters
(
command
=
'python'
,
args
=
[
'mcp_server.py'
],
env
=
os
.
environ
)
async
with
stdio_client
(
server_params
)
as
(
read
,
write
):
async
with
ClientSession
(
read
,
write
)
as
session
:
await
session
.
initialize
()
result
=
await
session
.
call_tool
(
'poet'
,
{
'theme'
:
'socks'
})
print
(
result
.
content
[
0
]
.
text
)
"""
Oh, socks, those garments soft and sweet,
That nestle softly 'round our feet,
From cotton, wool, or blended thread,
They keep our toes from feeling dread.
"""
if
__name__
==
'__main__'
:
asyncio
.
run
(
client
())
MCP Sampling
What is MCP Sampling?
See the
MCP client docs
for details of what MCP sampling is, and how you can support it when using Pydantic AI as an MCP client.
When Pydantic AI agents are used within MCP servers, they can use sampling via
MCPSamplingModel
.
We can extend the above example to use sampling so instead of connecting directly to the LLM, the agent calls back through the MCP client to make LLM calls.
mcp_server_sampling.py
from
mcp.server.fastmcp
import
Context
,
FastMCP
from
pydantic_ai
import
Agent
from
pydantic_ai.models.mcp_sampling
import
MCPSamplingModel
server
=
FastMCP
(
'PydanticAI Server with sampling'
)
server_agent
=
Agent
(
system_prompt
=
'always reply in rhyme'
)
@server
.
tool
()
async
def
poet
(
ctx
:
Context
,
theme
:
str
)
->
str
:
"""Poem generator"""
r
=
await
server_agent
.
run
(
f
'write a poem about
{
theme
}
'
,
model
=
MCPSamplingModel
(
session
=
ctx
.
session
))
return
r
.
output
if
__name__
==
'__main__'
:
server
.
run
()
# run the server over stdio
The
above
client does not support sampling, so if you tried to use it with this server you'd get an error.
The simplest way to support sampling in an MCP client is to
use
a Pydantic AI agent as the client, but if you wanted to support sampling with the vanilla MCP SDK, you could do so like this:
mcp_client_sampling.py
import
asyncio
from
typing
import
Any
from
mcp
import
ClientSession
,
StdioServerParameters
from
mcp.client.stdio
import
stdio_client
from
mcp.shared.context
import
RequestContext
from
mcp.types
import
CreateMessageRequestParams
,
CreateMessageResult
,
ErrorData
,
TextContent
async
def
sampling_callback
(
context
:
RequestContext
[
ClientSession
,
Any
],
params
:
CreateMessageRequestParams
)
->
CreateMessageResult
|
ErrorData
:
print
(
'sampling system prompt:'
,
params
.
systemPrompt
)
#> sampling system prompt: always reply in rhyme
print
(
'sampling messages:'
,
params
.
messages
)
"""
sampling messages:
[
SamplingMessage(
role='user',
content=TextContent(
type='text', text='write a poem about socks', annotations=None
),
)
]
"""
# TODO get the response content by calling an LLM...
response_content
=
'Socks for a fox.'
return
CreateMessageResult
(
role
=
'assistant'
,
content
=
TextContent
(
type
=
'text'
,
text
=
response_content
),
model
=
'fictional-llm'
,
)
async
def
client
():
server_params
=
StdioServerParameters
(
command
=
'python'
,
args
=
[
'mcp_server_sampling.py'
])
async
with
stdio_client
(
server_params
)
as
(
read
,
write
):
async
with
ClientSession
(
read
,
write
,
sampling_callback
=
sampling_callback
)
as
session
:
await
session
.
initialize
()
result
=
await
session
.
call_tool
(
'poet'
,
{
'theme'
:
'socks'
})
print
(
result
.
content
[
0
]
.
text
)
#> Socks for a fox.
if
__name__
==
'__main__'
:
asyncio
.
run
(
client
())
(This example is complete, it can be run "as is" with Python 3.10+)