PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.agent
Table of contents
agent
Agent
model
__init__
end_strategy
name
model_settings
output_type
instrument
instrument_all
run
iter
run_sync
run_stream
override
instructions
system_prompt
output_validator
tool
tool_plain
is_model_request_node
is_call_tools_node
is_user_prompt_node
is_end_node
run_mcp_servers
to_a2a
to_cli
to_cli_sync
AgentRun
ctx
next_node
result
__aiter__
__anext__
next
usage
AgentRunResult
output
all_messages
all_messages_json
new_messages
new_messages_json
usage
EndStrategy
RunOutputDataT
capture_run_messages
InstrumentationSettings
__init__
messages_to_otel_events
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
agent
Agent
model
__init__
end_strategy
name
model_settings
output_type
instrument
instrument_all
run
iter
run_sync
run_stream
override
instructions
system_prompt
output_validator
tool
tool_plain
is_model_request_node
is_call_tools_node
is_user_prompt_node
is_end_node
run_mcp_servers
to_a2a
to_cli
to_cli_sync
AgentRun
ctx
next_node
result
__aiter__
__anext__
next
usage
AgentRunResult
output
all_messages
all_messages_json
new_messages
new_messages_json
usage
EndStrategy
RunOutputDataT
capture_run_messages
InstrumentationSettings
__init__
messages_to_otel_events
pydantic_ai.agent
Agent
dataclass
Bases:
Generic
[
AgentDepsT
,
OutputDataT
]
Class for defining "agents" - a way to have a specific type of "conversation" with an LLM.
Agents are generic in the dependency type they take
AgentDepsT
and the output type they return,
OutputDataT
.
By default, if neither generic parameter is customised, agents have type
Agent[None, str]
.
Minimal usage example:
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
)
result
=
agent
.
run_sync
(
'What is the capital of France?'
)
print
(
result
.
output
)
#> Paris
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
1298
1299
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
1328
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
1343
1344
1345
1346
1347
1348
1349
1350
1351
1352
1353
1354
1355
1356
1357
1358
1359
1360
1361
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403
1404
1405
1406
1407
1408
1409
1410
1411
1412
1413
1414
1415
1416
1417
1418
1419
1420
1421
1422
1423
1424
1425
1426
1427
1428
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
1439
1440
1441
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
1461
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
1472
1473
1474
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
1534
1535
1536
1537
1538
1539
1540
1541
1542
1543
1544
1545
1546
1547
1548
1549
1550
1551
1552
1553
1554
1555
1556
1557
1558
1559
1560
1561
1562
1563
1564
1565
1566
1567
1568
1569
1570
1571
1572
1573
1574
1575
1576
1577
1578
1579
1580
1581
1582
1583
1584
1585
1586
1587
1588
1589
1590
1591
1592
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
1607
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
1625
1626
1627
1628
1629
1630
1631
1632
1633
1634
1635
1636
1637
1638
1639
1640
1641
1642
1643
1644
1645
1646
1647
1648
1649
1650
1651
1652
1653
1654
1655
1656
1657
1658
1659
1660
1661
1662
1663
1664
1665
1666
1667
1668
1669
1670
1671
1672
1673
1674
1675
1676
1677
1678
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
1689
1690
1691
1692
1693
1694
1695
1696
1697
1698
1699
1700
1701
1702
1703
1704
1705
1706
1707
1708
1709
1710
1711
1712
1713
1714
1715
1716
1717
1718
1719
1720
1721
1722
1723
1724
1725
1726
1727
1728
1729
1730
1731
1732
1733
1734
1735
1736
1737
1738
1739
1740
1741
1742
1743
1744
1745
1746
1747
1748
1749
1750
1751
1752
1753
1754
1755
1756
1757
1758
1759
1760
1761
1762
1763
1764
1765
1766
1767
1768
1769
1770
1771
1772
1773
1774
1775
1776
1777
1778
1779
1780
1781
1782
1783
1784
1785
1786
1787
1788
1789
1790
1791
1792
1793
1794
1795
1796
1797
1798
1799
1800
1801
1802
1803
1804
1805
1806
1807
1808
1809
1810
1811
1812
1813
1814
1815
1816
1817
1818
1819
1820
1821
1822
1823
1824
1825
1826
1827
1828
1829
1830
1831
1832
1833
1834
1835
1836
1837
1838
1839
1840
1841
1842
1843
1844
1845
1846
1847
1848
1849
1850
1851
@final
@dataclasses
.
dataclass
(
init
=
False
)
class
Agent
(
Generic
[
AgentDepsT
,
OutputDataT
]):
"""Class for defining "agents" - a way to have a specific type of "conversation" with an LLM.
Agents are generic in the dependency type they take [`AgentDepsT`][pydantic_ai.tools.AgentDepsT]
and the output type they return, [`OutputDataT`][pydantic_ai.output.OutputDataT].
By default, if neither generic parameter is customised, agents have type `Agent[None, str]`.
Minimal usage example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
result = agent.run_sync('What is the capital of France?')
print(result.output)
#> Paris
```
"""
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
"""The default model configured for this agent.
We allow `str` here since the actual list of allowed models changes frequently.
"""
name
:
str
|
None
"""The name of the agent, used for logging.
If `None`, we try to infer the agent name from the call frame when the agent is first run.
"""
end_strategy
:
EndStrategy
"""Strategy for handling tool calls when a final result is found."""
model_settings
:
ModelSettings
|
None
"""Optional model request settings to use for this agents's runs, by default.
Note, if `model_settings` is provided by `run`, `run_sync`, or `run_stream`, those settings will
be merged with this value, with the runtime argument taking priority.
"""
output_type
:
OutputSpec
[
OutputDataT
]
"""
The type of data output by agent runs, used to validate the data returned by the model, defaults to `str`.
"""
instrument
:
InstrumentationSettings
|
bool
|
None
"""Options to automatically instrument with OpenTelemetry."""
_instrument_default
:
ClassVar
[
InstrumentationSettings
|
bool
]
=
False
_deps_type
:
type
[
AgentDepsT
]
=
dataclasses
.
field
(
repr
=
False
)
_deprecated_result_tool_name
:
str
|
None
=
dataclasses
.
field
(
repr
=
False
)
_deprecated_result_tool_description
:
str
|
None
=
dataclasses
.
field
(
repr
=
False
)
_output_schema
:
_output
.
BaseOutputSchema
[
OutputDataT
]
=
dataclasses
.
field
(
repr
=
False
)
_output_validators
:
list
[
_output
.
OutputValidator
[
AgentDepsT
,
OutputDataT
]]
=
dataclasses
.
field
(
repr
=
False
)
_instructions
:
str
|
None
=
dataclasses
.
field
(
repr
=
False
)
_instructions_functions
:
list
[
_system_prompt
.
SystemPromptRunner
[
AgentDepsT
]]
=
dataclasses
.
field
(
repr
=
False
)
_system_prompts
:
tuple
[
str
,
...
]
=
dataclasses
.
field
(
repr
=
False
)
_system_prompt_functions
:
list
[
_system_prompt
.
SystemPromptRunner
[
AgentDepsT
]]
=
dataclasses
.
field
(
repr
=
False
)
_system_prompt_dynamic_functions
:
dict
[
str
,
_system_prompt
.
SystemPromptRunner
[
AgentDepsT
]]
=
dataclasses
.
field
(
repr
=
False
)
_prepare_tools
:
ToolsPrepareFunc
[
AgentDepsT
]
|
None
=
dataclasses
.
field
(
repr
=
False
)
_function_tools
:
dict
[
str
,
Tool
[
AgentDepsT
]]
=
dataclasses
.
field
(
repr
=
False
)
_mcp_servers
:
Sequence
[
MCPServer
]
=
dataclasses
.
field
(
repr
=
False
)
_default_retries
:
int
=
dataclasses
.
field
(
repr
=
False
)
_max_result_retries
:
int
=
dataclasses
.
field
(
repr
=
False
)
@overload
def
__init__
(
self
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
OutputDataT
]
=
str
,
instructions
:
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
Sequence
[
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
None
=
None
,
system_prompt
:
str
|
Sequence
[
str
]
=
(),
deps_type
:
type
[
AgentDepsT
]
=
NoneType
,
name
:
str
|
None
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
retries
:
int
=
1
,
output_retries
:
int
|
None
=
None
,
tools
:
Sequence
[
Tool
[
AgentDepsT
]
|
ToolFuncEither
[
AgentDepsT
,
...
]]
=
(),
prepare_tools
:
ToolsPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
mcp_servers
:
Sequence
[
MCPServer
]
=
(),
defer_model_check
:
bool
=
False
,
end_strategy
:
EndStrategy
=
'early'
,
instrument
:
InstrumentationSettings
|
bool
|
None
=
None
,
history_processors
:
Sequence
[
HistoryProcessor
[
AgentDepsT
]]
|
None
=
None
,
)
->
None
:
...
@overload
@deprecated
(
'`result_type`, `result_tool_name`, `result_tool_description` & `result_retries` are deprecated, use `output_type` instead. `result_retries` is deprecated, use `output_retries` instead.'
)
def
__init__
(
self
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
*
,
result_type
:
type
[
OutputDataT
]
=
str
,
instructions
:
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
Sequence
[
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
None
=
None
,
system_prompt
:
str
|
Sequence
[
str
]
=
(),
deps_type
:
type
[
AgentDepsT
]
=
NoneType
,
name
:
str
|
None
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
retries
:
int
=
1
,
result_tool_name
:
str
=
_output
.
DEFAULT_OUTPUT_TOOL_NAME
,
result_tool_description
:
str
|
None
=
None
,
result_retries
:
int
|
None
=
None
,
tools
:
Sequence
[
Tool
[
AgentDepsT
]
|
ToolFuncEither
[
AgentDepsT
,
...
]]
=
(),
prepare_tools
:
ToolsPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
mcp_servers
:
Sequence
[
MCPServer
]
=
(),
defer_model_check
:
bool
=
False
,
end_strategy
:
EndStrategy
=
'early'
,
instrument
:
InstrumentationSettings
|
bool
|
None
=
None
,
history_processors
:
Sequence
[
HistoryProcessor
[
AgentDepsT
]]
|
None
=
None
,
)
->
None
:
...
def
__init__
(
self
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
*
,
# TODO change this back to `output_type: _output.OutputType[OutputDataT] = str,` when we remove the overloads
output_type
:
Any
=
str
,
instructions
:
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
Sequence
[
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
None
=
None
,
system_prompt
:
str
|
Sequence
[
str
]
=
(),
deps_type
:
type
[
AgentDepsT
]
=
NoneType
,
name
:
str
|
None
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
retries
:
int
=
1
,
output_retries
:
int
|
None
=
None
,
tools
:
Sequence
[
Tool
[
AgentDepsT
]
|
ToolFuncEither
[
AgentDepsT
,
...
]]
=
(),
prepare_tools
:
ToolsPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
mcp_servers
:
Sequence
[
MCPServer
]
=
(),
defer_model_check
:
bool
=
False
,
end_strategy
:
EndStrategy
=
'early'
,
instrument
:
InstrumentationSettings
|
bool
|
None
=
None
,
history_processors
:
Sequence
[
HistoryProcessor
[
AgentDepsT
]]
|
None
=
None
,
**
_deprecated_kwargs
:
Any
,
):
"""Create an agent.
Args:
model: The default model to use for this agent, if not provide,
you must provide the model when calling it. We allow `str` here since the actual list of allowed models changes frequently.
output_type: The type of the output data, used to validate the data returned by the model,
defaults to `str`.
instructions: Instructions to use for this agent, you can also register instructions via a function with
[`instructions`][pydantic_ai.Agent.instructions].
system_prompt: Static system prompts to use for this agent, you can also register system
prompts via a function with [`system_prompt`][pydantic_ai.Agent.system_prompt].
deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully
parameterize the agent, and therefore get the best out of static type checking.
If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright
or add a type hint `: Agent[None, <return type>]`.
name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame
when the agent is first run.
model_settings: Optional model request settings to use for this agent's runs, by default.
retries: The default number of retries to allow before raising an error.
output_retries: The maximum number of retries to allow for result validation, defaults to `retries`.
tools: Tools to register with the agent, you can also register tools via the decorators
[`@agent.tool`][pydantic_ai.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain].
prepare_tools: custom method to prepare the tool definition of all tools for each step.
This is useful if you want to customize the definition of multiple tools or you want to register
a subset of tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]
mcp_servers: MCP servers to register with the agent. You should register a [`MCPServer`][pydantic_ai.mcp.MCPServer]
for each server you want the agent to connect to.
defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,
it's evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,
which checks for the necessary environment variables. Set this to `false`
to defer the evaluation until the first run. Useful if you want to
[override the model][pydantic_ai.Agent.override] for testing.
end_strategy: Strategy for handling tool calls that are requested alongside a final result.
See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.
instrument: Set to True to automatically instrument with OpenTelemetry,
which will use Logfire if it's configured.
Set to an instance of [`InstrumentationSettings`][pydantic_ai.agent.InstrumentationSettings] to customize.
If this isn't set, then the last value set by
[`Agent.instrument_all()`][pydantic_ai.Agent.instrument_all]
will be used, which defaults to False.
See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
history_processors: Optional list of callables to process the message history before sending it to the model.
Each processor takes a list of messages and returns a modified list of messages.
Processors can be sync or async and are applied in sequence.
"""
if
model
is
None
or
defer_model_check
:
self
.
model
=
model
else
:
self
.
model
=
models
.
infer_model
(
model
)
self
.
end_strategy
=
end_strategy
self
.
name
=
name
self
.
model_settings
=
model_settings
if
'result_type'
in
_deprecated_kwargs
:
if
output_type
is
not
str
:
# pragma: no cover
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
self
.
output_type
=
output_type
self
.
instrument
=
instrument
self
.
_deps_type
=
deps_type
self
.
_deprecated_result_tool_name
=
_deprecated_kwargs
.
pop
(
'result_tool_name'
,
None
)
if
self
.
_deprecated_result_tool_name
is
not
None
:
warnings
.
warn
(
'`result_tool_name` is deprecated, use `output_type` with `ToolOutput` instead'
,
DeprecationWarning
,
stacklevel
=
2
,
)
self
.
_deprecated_result_tool_description
=
_deprecated_kwargs
.
pop
(
'result_tool_description'
,
None
)
if
self
.
_deprecated_result_tool_description
is
not
None
:
warnings
.
warn
(
'`result_tool_description` is deprecated, use `output_type` with `ToolOutput` instead'
,
DeprecationWarning
,
stacklevel
=
2
,
)
result_retries
=
_deprecated_kwargs
.
pop
(
'result_retries'
,
None
)
if
result_retries
is
not
None
:
if
output_retries
is
not
None
:
# pragma: no cover
raise
TypeError
(
'`output_retries` and `result_retries` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_retries` is deprecated, use `max_result_retries` instead'
,
DeprecationWarning
,
stacklevel
=
2
)
output_retries
=
result_retries
default_output_mode
=
(
self
.
model
.
profile
.
default_structured_output_mode
if
isinstance
(
self
.
model
,
models
.
Model
)
else
None
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
self
.
_output_schema
=
_output
.
OutputSchema
[
OutputDataT
]
.
build
(
output_type
,
default_mode
=
default_output_mode
,
name
=
self
.
_deprecated_result_tool_name
,
description
=
self
.
_deprecated_result_tool_description
,
)
self
.
_output_validators
=
[]
self
.
_instructions
=
''
self
.
_instructions_functions
=
[]
if
isinstance
(
instructions
,
(
str
,
Callable
)):
instructions
=
[
instructions
]
for
instruction
in
instructions
or
[]:
if
isinstance
(
instruction
,
str
):
self
.
_instructions
+=
instruction
+
'
\n
'
else
:
self
.
_instructions_functions
.
append
(
_system_prompt
.
SystemPromptRunner
(
instruction
))
self
.
_instructions
=
self
.
_instructions
.
strip
()
or
None
self
.
_system_prompts
=
(
system_prompt
,)
if
isinstance
(
system_prompt
,
str
)
else
tuple
(
system_prompt
)
self
.
_system_prompt_functions
=
[]
self
.
_system_prompt_dynamic_functions
=
{}
self
.
_function_tools
=
{}
self
.
_default_retries
=
retries
self
.
_max_result_retries
=
output_retries
if
output_retries
is
not
None
else
retries
self
.
_mcp_servers
=
mcp_servers
self
.
_prepare_tools
=
prepare_tools
self
.
history_processors
=
history_processors
or
[]
for
tool
in
tools
:
if
isinstance
(
tool
,
Tool
):
self
.
_register_tool
(
tool
)
else
:
self
.
_register_tool
(
Tool
(
tool
))
self
.
_override_deps
:
ContextVar
[
_utils
.
Option
[
AgentDepsT
]]
=
ContextVar
(
'_override_deps'
,
default
=
None
)
self
.
_override_model
:
ContextVar
[
_utils
.
Option
[
models
.
Model
]]
=
ContextVar
(
'_override_model'
,
default
=
None
)
@staticmethod
def
instrument_all
(
instrument
:
InstrumentationSettings
|
bool
=
True
)
->
None
:
"""Set the instrumentation options for all agents where `instrument` is not set."""
Agent
.
_instrument_default
=
instrument
@overload
async
def
run
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AgentRunResult
[
OutputDataT
]:
...
@overload
async
def
run
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
],
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AgentRunResult
[
RunOutputDataT
]:
...
@overload
@deprecated
(
'`result_type` is deprecated, use `output_type` instead.'
)
async
def
run
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AgentRunResult
[
RunOutputDataT
]:
...
async
def
run
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AgentRunResult
[
Any
]:
"""Run the agent with a user prompt in async mode.
This method builds an internal agent graph (using system prompts, tools and result schemas) and then
runs the graph to completion. The result of the run is returned.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
async def main():
agent_run = await agent.run('What is the capital of France?')
print(agent_run.output)
#> Paris
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
if
infer_name
and
self
.
name
is
None
:
self
.
_infer_name
(
inspect
.
currentframe
())
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
async
with
self
.
iter
(
user_prompt
=
user_prompt
,
output_type
=
output_type
,
message_history
=
message_history
,
model
=
model
,
deps
=
deps
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
usage
=
usage
,
)
as
agent_run
:
async
for
_
in
agent_run
:
pass
assert
agent_run
.
result
is
not
None
,
'The graph run did not finish properly'
return
agent_run
.
result
@overload
def
iter
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
,
*
,
output_type
:
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AbstractAsyncContextManager
[
AgentRun
[
AgentDepsT
,
OutputDataT
]]:
...
@overload
def
iter
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
],
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AbstractAsyncContextManager
[
AgentRun
[
AgentDepsT
,
RunOutputDataT
]]:
...
@overload
@deprecated
(
'`result_type` is deprecated, use `output_type` instead.'
)
def
iter
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AbstractAsyncContextManager
[
AgentRun
[
AgentDepsT
,
Any
]]:
...
@asynccontextmanager
async
def
iter
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AsyncIterator
[
AgentRun
[
AgentDepsT
,
Any
]]:
"""A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.
This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an
`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are
executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the
stream of events coming from the execution of tools.
The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,
and the final result of the run once it has completed.
For more details, see the documentation of `AgentRun`.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
async def main():
nodes = []
async with agent.iter('What is the capital of France?') as agent_run:
async for node in agent_run:
nodes.append(node)
print(nodes)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1, request_tokens=56, response_tokens=1, total_tokens=57
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print(agent_run.result.output)
#> Paris
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
if
infer_name
and
self
.
name
is
None
:
self
.
_infer_name
(
inspect
.
currentframe
())
model_used
=
self
.
_get_model
(
model
)
del
model
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
deps
=
self
.
_get_deps
(
deps
)
new_message_index
=
len
(
message_history
)
if
message_history
else
0
output_schema
=
self
.
_prepare_output_schema
(
output_type
,
model_used
.
profile
)
output_type_
=
output_type
or
self
.
output_type
# Build the graph
graph
:
Graph
[
_agent_graph
.
GraphAgentState
,
_agent_graph
.
GraphAgentDeps
[
AgentDepsT
,
Any
],
FinalResult
[
Any
]]
=
(
_agent_graph
.
build_agent_graph
(
self
.
name
,
self
.
_deps_type
,
output_type_
)
)
# Build the initial state
usage
=
usage
or
_usage
.
Usage
()
state
=
_agent_graph
.
GraphAgentState
(
message_history
=
message_history
[:]
if
message_history
else
[],
usage
=
usage
,
retries
=
0
,
run_step
=
0
,
)
# We consider it a user error if a user tries to restrict the result type while having an output validator that
# may change the result type from the restricted type to something else. Therefore, we consider the following
# typecast reasonable, even though it is possible to violate it with otherwise-type-checked code.
output_validators
=
cast
(
list
[
_output
.
OutputValidator
[
AgentDepsT
,
RunOutputDataT
]],
self
.
_output_validators
)
model_settings
=
merge_model_settings
(
self
.
model_settings
,
model_settings
)
usage_limits
=
usage_limits
or
_usage
.
UsageLimits
()
if
isinstance
(
model_used
,
InstrumentedModel
):
instrumentation_settings
=
model_used
.
settings
tracer
=
model_used
.
settings
.
tracer
else
:
instrumentation_settings
=
None
tracer
=
NoOpTracer
()
agent_name
=
self
.
name
or
'agent'
run_span
=
tracer
.
start_span
(
'agent run'
,
attributes
=
{
'model_name'
:
model_used
.
model_name
if
model_used
else
'no-model'
,
'agent_name'
:
agent_name
,
'logfire.msg'
:
f
'
{
agent_name
}
run'
,
},
)
async
def
get_instructions
(
run_context
:
RunContext
[
AgentDepsT
])
->
str
|
None
:
parts
=
[
self
.
_instructions
,
*
[
await
func
.
run
(
run_context
)
for
func
in
self
.
_instructions_functions
],
]
model_profile
=
model_used
.
profile
if
isinstance
(
output_schema
,
_output
.
PromptedOutputSchema
):
instructions
=
output_schema
.
instructions
(
model_profile
.
prompted_output_template
)
parts
.
append
(
instructions
)
parts
=
[
p
for
p
in
parts
if
p
]
if
not
parts
:
return
None
return
'
\n\n
'
.
join
(
parts
)
.
strip
()
# Copy the function tools so that retry state is agent-run-specific
# Note that the retry count is reset to 0 when this happens due to the `default=0` and `init=False`.
run_function_tools
=
{
k
:
dataclasses
.
replace
(
v
)
for
k
,
v
in
self
.
_function_tools
.
items
()}
graph_deps
=
_agent_graph
.
GraphAgentDeps
[
AgentDepsT
,
RunOutputDataT
](
user_deps
=
deps
,
prompt
=
user_prompt
,
new_message_index
=
new_message_index
,
model
=
model_used
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
max_result_retries
=
self
.
_max_result_retries
,
end_strategy
=
self
.
end_strategy
,
output_schema
=
output_schema
,
output_validators
=
output_validators
,
history_processors
=
self
.
history_processors
,
function_tools
=
run_function_tools
,
mcp_servers
=
self
.
_mcp_servers
,
default_retries
=
self
.
_default_retries
,
tracer
=
tracer
,
prepare_tools
=
self
.
_prepare_tools
,
get_instructions
=
get_instructions
,
instrumentation_settings
=
instrumentation_settings
,
)
start_node
=
_agent_graph
.
UserPromptNode
[
AgentDepsT
](
user_prompt
=
user_prompt
,
instructions
=
self
.
_instructions
,
instructions_functions
=
self
.
_instructions_functions
,
system_prompts
=
self
.
_system_prompts
,
system_prompt_functions
=
self
.
_system_prompt_functions
,
system_prompt_dynamic_functions
=
self
.
_system_prompt_dynamic_functions
,
)
try
:
async
with
graph
.
iter
(
start_node
,
state
=
state
,
deps
=
graph_deps
,
span
=
use_span
(
run_span
)
if
run_span
.
is_recording
()
else
None
,
infer_name
=
False
,
)
as
graph_run
:
agent_run
=
AgentRun
(
graph_run
)
yield
agent_run
if
(
final_result
:=
agent_run
.
result
)
is
not
None
and
run_span
.
is_recording
():
run_span
.
set_attribute
(
'final_result'
,
(
final_result
.
output
if
isinstance
(
final_result
.
output
,
str
)
else
json
.
dumps
(
InstrumentedModel
.
serialize_any
(
final_result
.
output
))
),
)
finally
:
try
:
if
instrumentation_settings
and
run_span
.
is_recording
():
run_span
.
set_attributes
(
self
.
_run_span_end_attributes
(
state
,
usage
,
instrumentation_settings
))
finally
:
run_span
.
end
()
def
_run_span_end_attributes
(
self
,
state
:
_agent_graph
.
GraphAgentState
,
usage
:
_usage
.
Usage
,
settings
:
InstrumentationSettings
):
return
{
**
usage
.
opentelemetry_attributes
(),
'all_messages_events'
:
json
.
dumps
(
[
InstrumentedModel
.
event_to_dict
(
e
)
for
e
in
settings
.
messages_to_otel_events
(
state
.
message_history
)]
),
'logfire.json_schema'
:
json
.
dumps
(
{
'type'
:
'object'
,
'properties'
:
{
'all_messages_events'
:
{
'type'
:
'array'
},
'final_result'
:
{
'type'
:
'object'
},
},
}
),
}
@overload
def
run_sync
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AgentRunResult
[
OutputDataT
]:
...
@overload
def
run_sync
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AgentRunResult
[
RunOutputDataT
]:
...
@overload
@deprecated
(
'`result_type` is deprecated, use `output_type` instead.'
)
def
run_sync
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AgentRunResult
[
RunOutputDataT
]:
...
def
run_sync
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AgentRunResult
[
Any
]:
"""Synchronously run the agent with a user prompt.
This is a convenience method that wraps [`self.run`][pydantic_ai.Agent.run] with `loop.run_until_complete(...)`.
You therefore can't use this method inside async code or if there's an active event loop.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
result_sync = agent.run_sync('What is the capital of Italy?')
print(result_sync.output)
#> Rome
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
if
infer_name
and
self
.
name
is
None
:
self
.
_infer_name
(
inspect
.
currentframe
())
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
return
get_event_loop
()
.
run_until_complete
(
self
.
run
(
user_prompt
,
output_type
=
output_type
,
message_history
=
message_history
,
model
=
model
,
deps
=
deps
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
usage
=
usage
,
infer_name
=
False
,
)
)
@overload
def
run_stream
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AbstractAsyncContextManager
[
result
.
StreamedRunResult
[
AgentDepsT
,
OutputDataT
]]:
...
@overload
def
run_stream
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
],
*
,
output_type
:
OutputSpec
[
RunOutputDataT
],
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AbstractAsyncContextManager
[
result
.
StreamedRunResult
[
AgentDepsT
,
RunOutputDataT
]]:
...
@overload
@deprecated
(
'`result_type` is deprecated, use `output_type` instead.'
)
def
run_stream
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
)
->
AbstractAsyncContextManager
[
result
.
StreamedRunResult
[
AgentDepsT
,
RunOutputDataT
]]:
...
@asynccontextmanager
async
def
run_stream
(
# noqa C901
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AsyncIterator
[
result
.
StreamedRunResult
[
AgentDepsT
,
Any
]]:
"""Run the agent with a user prompt in async mode, returning a streamed response.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
async def main():
async with agent.run_stream('What is the capital of the UK?') as response:
print(await response.get_output())
#> London
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
# TODO: We need to deprecate this now that we have the `iter` method.
#   Before that, though, we should add an event for when we reach the final result of the stream.
if
infer_name
and
self
.
name
is
None
:
# f_back because `asynccontextmanager` adds one frame
if
frame
:=
inspect
.
currentframe
():
# pragma: no branch
self
.
_infer_name
(
frame
.
f_back
)
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
yielded
=
False
async
with
self
.
iter
(
user_prompt
,
output_type
=
output_type
,
message_history
=
message_history
,
model
=
model
,
deps
=
deps
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
usage
=
usage
,
infer_name
=
False
,
)
as
agent_run
:
first_node
=
agent_run
.
next_node
# start with the first node
assert
isinstance
(
first_node
,
_agent_graph
.
UserPromptNode
)
# the first node should be a user prompt node
node
=
first_node
while
True
:
if
self
.
is_model_request_node
(
node
):
graph_ctx
=
agent_run
.
ctx
async
with
node
.
_stream
(
graph_ctx
)
as
streamed_response
:
# pyright: ignore[reportPrivateUsage]
async
def
stream_to_final
(
s
:
models
.
StreamedResponse
,
)
->
FinalResult
[
models
.
StreamedResponse
]
|
None
:
output_schema
=
graph_ctx
.
deps
.
output_schema
async
for
maybe_part_event
in
streamed_response
:
if
isinstance
(
maybe_part_event
,
_messages
.
PartStartEvent
):
new_part
=
maybe_part_event
.
part
if
isinstance
(
new_part
,
_messages
.
TextPart
)
and
isinstance
(
output_schema
,
_output
.
TextOutputSchema
):
return
FinalResult
(
s
,
None
,
None
)
elif
isinstance
(
new_part
,
_messages
.
ToolCallPart
)
and
isinstance
(
output_schema
,
_output
.
ToolOutputSchema
):
# pragma: no branch
for
call
,
_
in
output_schema
.
find_tool
([
new_part
]):
return
FinalResult
(
s
,
call
.
tool_name
,
call
.
tool_call_id
)
return
None
final_result_details
=
await
stream_to_final
(
streamed_response
)
if
final_result_details
is
not
None
:
if
yielded
:
raise
exceptions
.
AgentRunError
(
'Agent run produced final results'
)
# pragma: no cover
yielded
=
True
messages
=
graph_ctx
.
state
.
message_history
.
copy
()
async
def
on_complete
()
->
None
:
"""Called when the stream has completed.
The model response will have been added to messages by now
by `StreamedRunResult._marked_completed`.
"""
last_message
=
messages
[
-
1
]
assert
isinstance
(
last_message
,
_messages
.
ModelResponse
)
tool_calls
=
[
part
for
part
in
last_message
.
parts
if
isinstance
(
part
,
_messages
.
ToolCallPart
)
]
parts
:
list
[
_messages
.
ModelRequestPart
]
=
[]
async
for
_event
in
_agent_graph
.
process_function_tools
(
tool_calls
,
final_result_details
.
tool_name
,
final_result_details
.
tool_call_id
,
graph_ctx
,
parts
,
):
pass
# TODO: Should we do something here related to the retry count?
#   Maybe we should move the incrementing of the retry count to where we actually make a request?
# if any(isinstance(part, _messages.RetryPromptPart) for part in parts):
#     ctx.state.increment_retries(ctx.deps.max_result_retries)
if
parts
:
messages
.
append
(
_messages
.
ModelRequest
(
parts
))
yield
StreamedRunResult
(
messages
,
graph_ctx
.
deps
.
new_message_index
,
graph_ctx
.
deps
.
usage_limits
,
streamed_response
,
graph_ctx
.
deps
.
output_schema
,
_agent_graph
.
build_run_context
(
graph_ctx
),
graph_ctx
.
deps
.
output_validators
,
final_result_details
.
tool_name
,
on_complete
,
)
break
next_node
=
await
agent_run
.
next
(
node
)
if
not
isinstance
(
next_node
,
_agent_graph
.
AgentNode
):
raise
exceptions
.
AgentRunError
(
# pragma: no cover
'Should have produced a StreamedRunResult before getting here'
)
node
=
cast
(
_agent_graph
.
AgentNode
[
Any
,
Any
],
next_node
)
if
not
yielded
:
raise
exceptions
.
AgentRunError
(
'Agent run finished without producing a final result'
)
# pragma: no cover
@contextmanager
def
override
(
self
,
*
,
deps
:
AgentDepsT
|
_utils
.
Unset
=
_utils
.
UNSET
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
_utils
.
Unset
=
_utils
.
UNSET
,
)
->
Iterator
[
None
]:
"""Context manager to temporarily override agent dependencies and model.
This is particularly useful when testing.
You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).
Args:
deps: The dependencies to use instead of the dependencies passed to the agent run.
model: The model to use instead of the model passed to the agent run.
"""
if
_utils
.
is_set
(
deps
):
deps_token
=
self
.
_override_deps
.
set
(
_utils
.
Some
(
deps
))
else
:
deps_token
=
None
if
_utils
.
is_set
(
model
):
model_token
=
self
.
_override_model
.
set
(
_utils
.
Some
(
models
.
infer_model
(
model
)))
else
:
model_token
=
None
try
:
yield
finally
:
if
deps_token
is
not
None
:
self
.
_override_deps
.
reset
(
deps_token
)
if
model_token
is
not
None
:
self
.
_override_model
.
reset
(
model_token
)
@overload
def
instructions
(
self
,
func
:
Callable
[[
RunContext
[
AgentDepsT
]],
str
],
/
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
str
]:
...
@overload
def
instructions
(
self
,
func
:
Callable
[[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]],
/
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]]:
...
@overload
def
instructions
(
self
,
func
:
Callable
[[],
str
],
/
)
->
Callable
[[],
str
]:
...
@overload
def
instructions
(
self
,
func
:
Callable
[[],
Awaitable
[
str
]],
/
)
->
Callable
[[],
Awaitable
[
str
]]:
...
@overload
def
instructions
(
self
,
/
)
->
Callable
[[
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]],
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]:
...
def
instructions
(
self
,
func
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
None
=
None
,
/
,
)
->
(
Callable
[[
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]],
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
):
"""Decorator to register an instructions function.
Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.
Can decorate a sync or async functions.
The decorator can be used bare (`agent.instructions`).
Overloads for every possible signature of `instructions` are included so the decorator doesn't obscure
the type of the function.
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test', deps_type=str)
@agent.instructions
def simple_instructions() -> str:
return 'foobar'
@agent.instructions
async def async_instructions(ctx: RunContext[str]) -> str:
return f'{ctx.deps} is the best'
```
"""
if
func
is
None
:
def
decorator
(
func_
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
],
)
->
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]:
self
.
_instructions_functions
.
append
(
_system_prompt
.
SystemPromptRunner
(
func_
))
return
func_
return
decorator
else
:
self
.
_instructions_functions
.
append
(
_system_prompt
.
SystemPromptRunner
(
func
))
return
func
@overload
def
system_prompt
(
self
,
func
:
Callable
[[
RunContext
[
AgentDepsT
]],
str
],
/
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
str
]:
...
@overload
def
system_prompt
(
self
,
func
:
Callable
[[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]],
/
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]]:
...
@overload
def
system_prompt
(
self
,
func
:
Callable
[[],
str
],
/
)
->
Callable
[[],
str
]:
...
@overload
def
system_prompt
(
self
,
func
:
Callable
[[],
Awaitable
[
str
]],
/
)
->
Callable
[[],
Awaitable
[
str
]]:
...
@overload
def
system_prompt
(
self
,
/
,
*
,
dynamic
:
bool
=
False
)
->
Callable
[[
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]],
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]:
...
def
system_prompt
(
self
,
func
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
None
=
None
,
/
,
*
,
dynamic
:
bool
=
False
,
)
->
(
Callable
[[
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]],
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
):
"""Decorator to register a system prompt function.
Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.
Can decorate a sync or async functions.
The decorator can be used either bare (`agent.system_prompt`) or as a function call
(`agent.system_prompt(...)`), see the examples below.
Overloads for every possible signature of `system_prompt` are included so the decorator doesn't obscure
the type of the function, see `tests/typed_agent.py` for tests.
Args:
func: The function to decorate
dynamic: If True, the system prompt will be reevaluated even when `messages_history` is provided,
see [`SystemPromptPart.dynamic_ref`][pydantic_ai.messages.SystemPromptPart.dynamic_ref]
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test', deps_type=str)
@agent.system_prompt
def simple_system_prompt() -> str:
return 'foobar'
@agent.system_prompt(dynamic=True)
async def async_system_prompt(ctx: RunContext[str]) -> str:
return f'{ctx.deps} is the best'
```
"""
if
func
is
None
:
def
decorator
(
func_
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
],
)
->
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]:
runner
=
_system_prompt
.
SystemPromptRunner
[
AgentDepsT
](
func_
,
dynamic
=
dynamic
)
self
.
_system_prompt_functions
.
append
(
runner
)
if
dynamic
:
# pragma: lax no cover
self
.
_system_prompt_dynamic_functions
[
func_
.
__qualname__
]
=
runner
return
func_
return
decorator
else
:
assert
not
dynamic
,
"dynamic can't be True in this case"
self
.
_system_prompt_functions
.
append
(
_system_prompt
.
SystemPromptRunner
[
AgentDepsT
](
func
,
dynamic
=
dynamic
))
return
func
@overload
def
output_validator
(
self
,
func
:
Callable
[[
RunContext
[
AgentDepsT
],
OutputDataT
],
OutputDataT
],
/
)
->
Callable
[[
RunContext
[
AgentDepsT
],
OutputDataT
],
OutputDataT
]:
...
@overload
def
output_validator
(
self
,
func
:
Callable
[[
RunContext
[
AgentDepsT
],
OutputDataT
],
Awaitable
[
OutputDataT
]],
/
)
->
Callable
[[
RunContext
[
AgentDepsT
],
OutputDataT
],
Awaitable
[
OutputDataT
]]:
...
@overload
def
output_validator
(
self
,
func
:
Callable
[[
OutputDataT
],
OutputDataT
],
/
)
->
Callable
[[
OutputDataT
],
OutputDataT
]:
...
@overload
def
output_validator
(
self
,
func
:
Callable
[[
OutputDataT
],
Awaitable
[
OutputDataT
]],
/
)
->
Callable
[[
OutputDataT
],
Awaitable
[
OutputDataT
]]:
...
def
output_validator
(
self
,
func
:
_output
.
OutputValidatorFunc
[
AgentDepsT
,
OutputDataT
],
/
)
->
_output
.
OutputValidatorFunc
[
AgentDepsT
,
OutputDataT
]:
"""Decorator to register an output validator function.
Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.
Can decorate a sync or async functions.
Overloads for every possible signature of `output_validator` are included so the decorator doesn't obscure
the type of the function, see `tests/typed_agent.py` for tests.
Example:
```python
from pydantic_ai import Agent, ModelRetry, RunContext
agent = Agent('test', deps_type=str)
@agent.output_validator
def output_validator_simple(data: str) -> str:
if 'wrong' in data:
raise ModelRetry('wrong response')
return data
@agent.output_validator
async def output_validator_deps(ctx: RunContext[str], data: str) -> str:
if ctx.deps in data:
raise ModelRetry('wrong response')
return data
result = agent.run_sync('foobar', deps='spam')
print(result.output)
#> success (no tool calls)
```
"""
self
.
_output_validators
.
append
(
_output
.
OutputValidator
[
AgentDepsT
,
Any
](
func
))
return
func
@deprecated
(
'`result_validator` is deprecated, use `output_validator` instead.'
)
def
result_validator
(
self
,
func
:
Any
,
/
)
->
Any
:
warnings
.
warn
(
'`result_validator` is deprecated, use `output_validator` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
return
self
.
output_validator
(
func
)
# type: ignore
@overload
def
tool
(
self
,
func
:
ToolFuncContext
[
AgentDepsT
,
ToolParams
],
/
)
->
ToolFuncContext
[
AgentDepsT
,
ToolParams
]:
...
@overload
def
tool
(
self
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
'auto'
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Callable
[[
ToolFuncContext
[
AgentDepsT
,
ToolParams
]],
ToolFuncContext
[
AgentDepsT
,
ToolParams
]]:
...
def
tool
(
self
,
func
:
ToolFuncContext
[
AgentDepsT
,
ToolParams
]
|
None
=
None
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
'auto'
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Any
:
"""Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.
Can decorate a sync or async functions.
The docstring is inspected to extract both the tool description and description of each parameter,
[learn more](../tools.md#function-tools-and-schema).
We can't add overloads for every possible signature of tool, since the return type is a recursive union
so the signature of functions decorated with `@agent.tool` is obscured.
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test', deps_type=int)
@agent.tool
def foobar(ctx: RunContext[int], x: int) -> int:
return ctx.deps + x
@agent.tool(retries=2)
async def spam(ctx: RunContext[str], y: float) -> float:
return ctx.deps + y
result = agent.run_sync('foobar', deps=1)
print(result.output)
#> {"foobar":1,"spam":1.0}
```
Args:
func: The tool function to register.
name: The name of the tool, defaults to the function name.
retries: The number of retries to allow for this tool, defaults to the agent's default retries,
which defaults to 1.
prepare: custom method to prepare the tool definition for each step, return `None` to omit this
tool from a given step. This is useful if you want to customise a tool at call time,
or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].
docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].
Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.
require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.
schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.
strict: Whether to enforce JSON schema compliance (only affects OpenAI).
See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.
"""
if
func
is
None
:
def
tool_decorator
(
func_
:
ToolFuncContext
[
AgentDepsT
,
ToolParams
],
)
->
ToolFuncContext
[
AgentDepsT
,
ToolParams
]:
# noinspection PyTypeChecker
self
.
_register_function
(
func_
,
True
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func_
return
tool_decorator
else
:
# noinspection PyTypeChecker
self
.
_register_function
(
func
,
True
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func
@overload
def
tool_plain
(
self
,
func
:
ToolFuncPlain
[
ToolParams
],
/
)
->
ToolFuncPlain
[
ToolParams
]:
...
@overload
def
tool_plain
(
self
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
'auto'
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Callable
[[
ToolFuncPlain
[
ToolParams
]],
ToolFuncPlain
[
ToolParams
]]:
...
def
tool_plain
(
self
,
func
:
ToolFuncPlain
[
ToolParams
]
|
None
=
None
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
'auto'
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Any
:
"""Decorator to register a tool function which DOES NOT take `RunContext` as an argument.
Can decorate a sync or async functions.
The docstring is inspected to extract both the tool description and description of each parameter,
[learn more](../tools.md#function-tools-and-schema).
We can't add overloads for every possible signature of tool, since the return type is a recursive union
so the signature of functions decorated with `@agent.tool` is obscured.
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test')
@agent.tool
def foobar(ctx: RunContext[int]) -> int:
return 123
@agent.tool(retries=2)
async def spam(ctx: RunContext[str]) -> float:
return 3.14
result = agent.run_sync('foobar', deps=1)
print(result.output)
#> {"foobar":123,"spam":3.14}
```
Args:
func: The tool function to register.
name: The name of the tool, defaults to the function name.
retries: The number of retries to allow for this tool, defaults to the agent's default retries,
which defaults to 1.
prepare: custom method to prepare the tool definition for each step, return `None` to omit this
tool from a given step. This is useful if you want to customise a tool at call time,
or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].
docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].
Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.
require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.
schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.
strict: Whether to enforce JSON schema compliance (only affects OpenAI).
See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.
"""
if
func
is
None
:
def
tool_decorator
(
func_
:
ToolFuncPlain
[
ToolParams
])
->
ToolFuncPlain
[
ToolParams
]:
# noinspection PyTypeChecker
self
.
_register_function
(
func_
,
False
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func_
return
tool_decorator
else
:
self
.
_register_function
(
func
,
False
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func
def
_register_function
(
self
,
func
:
ToolFuncEither
[
AgentDepsT
,
ToolParams
],
takes_ctx
:
bool
,
name
:
str
|
None
,
retries
:
int
|
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
,
docstring_format
:
DocstringFormat
,
require_parameter_descriptions
:
bool
,
schema_generator
:
type
[
GenerateJsonSchema
],
strict
:
bool
|
None
,
)
->
None
:
"""Private utility to register a function as a tool."""
retries_
=
retries
if
retries
is
not
None
else
self
.
_default_retries
tool
=
Tool
[
AgentDepsT
](
func
,
takes_ctx
=
takes_ctx
,
name
=
name
,
max_retries
=
retries_
,
prepare
=
prepare
,
docstring_format
=
docstring_format
,
require_parameter_descriptions
=
require_parameter_descriptions
,
schema_generator
=
schema_generator
,
strict
=
strict
,
)
self
.
_register_tool
(
tool
)
def
_register_tool
(
self
,
tool
:
Tool
[
AgentDepsT
])
->
None
:
"""Private utility to register a tool instance."""
if
tool
.
max_retries
is
None
:
# noinspection PyTypeChecker
tool
=
dataclasses
.
replace
(
tool
,
max_retries
=
self
.
_default_retries
)
if
tool
.
name
in
self
.
_function_tools
:
raise
exceptions
.
UserError
(
f
'Tool name conflicts with existing tool:
{
tool
.
name
!r}
'
)
if
tool
.
name
in
self
.
_output_schema
.
tools
:
raise
exceptions
.
UserError
(
f
'Tool name conflicts with output tool name:
{
tool
.
name
!r}
'
)
self
.
_function_tools
[
tool
.
name
]
=
tool
def
_get_model
(
self
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
)
->
models
.
Model
:
"""Create a model configured for this agent.
Args:
model: model to use for this run, required if `model` was not set when creating the agent.
Returns:
The model used
"""
model_
:
models
.
Model
if
some_model
:=
self
.
_override_model
.
get
():
# we don't want `override()` to cover up errors from the model not being defined, hence this check
if
model
is
None
and
self
.
model
is
None
:
raise
exceptions
.
UserError
(
'`model` must either be set on the agent or included when calling it. '
'(Even when `override(model=...)` is customizing the model that will actually be called)'
)
model_
=
some_model
.
value
elif
model
is
not
None
:
model_
=
models
.
infer_model
(
model
)
elif
self
.
model
is
not
None
:
# noinspection PyTypeChecker
model_
=
self
.
model
=
models
.
infer_model
(
self
.
model
)
else
:
raise
exceptions
.
UserError
(
'`model` must either be set on the agent or included when calling it.'
)
instrument
=
self
.
instrument
if
instrument
is
None
:
instrument
=
self
.
_instrument_default
return
instrument_model
(
model_
,
instrument
)
def
_get_deps
(
self
:
Agent
[
T
,
OutputDataT
],
deps
:
T
)
->
T
:
"""Get deps for a run.
If we've overridden deps via `_override_deps`, use that, otherwise use the deps passed to the call.
We could do runtime type checking of deps against `self._deps_type`, but that's a slippery slope.
"""
if
some_deps
:=
self
.
_override_deps
.
get
():
return
some_deps
.
value
else
:
return
deps
def
_infer_name
(
self
,
function_frame
:
FrameType
|
None
)
->
None
:
"""Infer the agent name from the call frame.
Usage should be `self._infer_name(inspect.currentframe())`.
"""
assert
self
.
name
is
None
,
'Name already set'
if
function_frame
is
not
None
:
# pragma: no branch
if
parent_frame
:=
function_frame
.
f_back
:
# pragma: no branch
for
name
,
item
in
parent_frame
.
f_locals
.
items
():
if
item
is
self
:
self
.
name
=
name
return
if
parent_frame
.
f_locals
!=
parent_frame
.
f_globals
:
# pragma: no branch
# if we couldn't find the agent in locals and globals are a different dict, try globals
for
name
,
item
in
parent_frame
.
f_globals
.
items
():
if
item
is
self
:
self
.
name
=
name
return
@property
@deprecated
(
'The `last_run_messages` attribute has been removed, use `capture_run_messages` instead.'
,
category
=
None
)
def
last_run_messages
(
self
)
->
list
[
_messages
.
ModelMessage
]:
raise
AttributeError
(
'The `last_run_messages` attribute has been removed, use `capture_run_messages` instead.'
)
def
_prepare_output_schema
(
self
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
,
model_profile
:
ModelProfile
)
->
_output
.
OutputSchema
[
RunOutputDataT
]:
if
output_type
is
not
None
:
if
self
.
_output_validators
:
raise
exceptions
.
UserError
(
'Cannot set a custom run `output_type` when the agent has output validators'
)
schema
=
_output
.
OutputSchema
[
RunOutputDataT
]
.
build
(
output_type
,
name
=
self
.
_deprecated_result_tool_name
,
description
=
self
.
_deprecated_result_tool_description
,
default_mode
=
model_profile
.
default_structured_output_mode
,
)
else
:
schema
=
self
.
_output_schema
.
with_default_mode
(
model_profile
.
default_structured_output_mode
)
schema
.
raise_if_unsupported
(
model_profile
)
return
schema
# pyright: ignore[reportReturnType]
@staticmethod
def
is_model_request_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
_agent_graph
.
ModelRequestNode
[
T
,
S
]]:
"""Check if the node is a `ModelRequestNode`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
_agent_graph
.
ModelRequestNode
)
@staticmethod
def
is_call_tools_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
_agent_graph
.
CallToolsNode
[
T
,
S
]]:
"""Check if the node is a `CallToolsNode`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
_agent_graph
.
CallToolsNode
)
@staticmethod
def
is_user_prompt_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
_agent_graph
.
UserPromptNode
[
T
,
S
]]:
"""Check if the node is a `UserPromptNode`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
_agent_graph
.
UserPromptNode
)
@staticmethod
def
is_end_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
End
[
result
.
FinalResult
[
S
]]]:
"""Check if the node is a `End`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
End
)
@asynccontextmanager
async
def
run_mcp_servers
(
self
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
)
->
AsyncIterator
[
None
]:
"""Run [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] so they can be used by the agent.
Returns: a context manager to start and shutdown the servers.
"""
try
:
sampling_model
:
models
.
Model
|
None
=
self
.
_get_model
(
model
)
except
exceptions
.
UserError
:
# pragma: no cover
sampling_model
=
None
exit_stack
=
AsyncExitStack
()
try
:
for
mcp_server
in
self
.
_mcp_servers
:
if
sampling_model
is
not
None
:
# pragma: no branch
mcp_server
.
sampling_model
=
sampling_model
await
exit_stack
.
enter_async_context
(
mcp_server
)
yield
finally
:
await
exit_stack
.
aclose
()
def
to_a2a
(
self
,
*
,
storage
:
Storage
|
None
=
None
,
broker
:
Broker
|
None
=
None
,
# Agent card
name
:
str
|
None
=
None
,
url
:
str
=
'http://localhost:8000'
,
version
:
str
=
'1.0.0'
,
description
:
str
|
None
=
None
,
provider
:
Provider
|
None
=
None
,
skills
:
list
[
Skill
]
|
None
=
None
,
# Starlette
debug
:
bool
=
False
,
routes
:
Sequence
[
Route
]
|
None
=
None
,
middleware
:
Sequence
[
Middleware
]
|
None
=
None
,
exception_handlers
:
dict
[
Any
,
ExceptionHandler
]
|
None
=
None
,
lifespan
:
Lifespan
[
FastA2A
]
|
None
=
None
,
)
->
FastA2A
:
"""Convert the agent to a FastA2A application.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
app = agent.to_a2a()
```
The `app` is an ASGI application that can be used with any ASGI server.
To run the application, you can use the following command:
```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```
"""
from
._a2a
import
agent_to_a2a
return
agent_to_a2a
(
self
,
storage
=
storage
,
broker
=
broker
,
name
=
name
,
url
=
url
,
version
=
version
,
description
=
description
,
provider
=
provider
,
skills
=
skills
,
debug
=
debug
,
routes
=
routes
,
middleware
=
middleware
,
exception_handlers
=
exception_handlers
,
lifespan
=
lifespan
,
)
async
def
to_cli
(
self
:
Self
,
deps
:
AgentDepsT
=
None
,
prog_name
:
str
=
'pydantic-ai'
)
->
None
:
"""Run the agent in a CLI chat interface.
Args:
deps: The dependencies to pass to the agent.
prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.
Example:
```python {title="agent_to_cli.py" test="skip"}
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')
async def main():
await agent.to_cli()
```
"""
from
rich.console
import
Console
from
pydantic_ai._cli
import
run_chat
await
run_chat
(
stream
=
True
,
agent
=
self
,
deps
=
deps
,
console
=
Console
(),
code_theme
=
'monokai'
,
prog_name
=
prog_name
)
def
to_cli_sync
(
self
:
Self
,
deps
:
AgentDepsT
=
None
,
prog_name
:
str
=
'pydantic-ai'
)
->
None
:
"""Run the agent in a CLI chat interface with the non-async interface.
Args:
deps: The dependencies to pass to the agent.
prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.
```python {title="agent_to_cli_sync.py" test="skip"}
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')
agent.to_cli_sync()
agent.to_cli_sync(prog_name='assistant')
```
"""
return
get_event_loop
()
.
run_until_complete
(
self
.
to_cli
(
deps
=
deps
,
prog_name
=
prog_name
))
model
instance-attribute
model
:
Model
|
KnownModelName
|
str
|
None
The default model configured for this agent.
We allow
str
here since the actual list of allowed models changes frequently.
__init__
__init__
(
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
OutputDataT
]
=
str
,
instructions
:
(
str
|
SystemPromptFunc
[
AgentDepsT
]
|
Sequence
[
str
|
SystemPromptFunc
[
AgentDepsT
]]
|
None
)
=
None
,
system_prompt
:
str
|
Sequence
[
str
]
=
(),
deps_type
:
type
[
AgentDepsT
]
=
NoneType
,
name
:
str
|
None
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
retries
:
int
=
1
,
output_retries
:
int
|
None
=
None
,
tools
:
Sequence
[
Tool
[
AgentDepsT
]
|
ToolFuncEither
[
AgentDepsT
,
...
]
]
=
(),
prepare_tools
:
(
ToolsPrepareFunc
[
AgentDepsT
]
|
None
)
=
None
,
mcp_servers
:
Sequence
[
MCPServer
]
=
(),
defer_model_check
:
bool
=
False
,
end_strategy
:
EndStrategy
=
"early"
,
instrument
:
(
InstrumentationSettings
|
bool
|
None
)
=
None
,
history_processors
:
(
Sequence
[
HistoryProcessor
[
AgentDepsT
]]
|
None
)
=
None
)
->
None
__init__
(
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
*
,
result_type
:
type
[
OutputDataT
]
=
str
,
instructions
:
(
str
|
SystemPromptFunc
[
AgentDepsT
]
|
Sequence
[
str
|
SystemPromptFunc
[
AgentDepsT
]]
|
None
)
=
None
,
system_prompt
:
str
|
Sequence
[
str
]
=
(),
deps_type
:
type
[
AgentDepsT
]
=
NoneType
,
name
:
str
|
None
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
retries
:
int
=
1
,
result_tool_name
:
str
=
DEFAULT_OUTPUT_TOOL_NAME
,
result_tool_description
:
str
|
None
=
None
,
result_retries
:
int
|
None
=
None
,
tools
:
Sequence
[
Tool
[
AgentDepsT
]
|
ToolFuncEither
[
AgentDepsT
,
...
]
]
=
(),
prepare_tools
:
(
ToolsPrepareFunc
[
AgentDepsT
]
|
None
)
=
None
,
mcp_servers
:
Sequence
[
MCPServer
]
=
(),
defer_model_check
:
bool
=
False
,
end_strategy
:
EndStrategy
=
"early"
,
instrument
:
(
InstrumentationSettings
|
bool
|
None
)
=
None
,
history_processors
:
(
Sequence
[
HistoryProcessor
[
AgentDepsT
]]
|
None
)
=
None
)
->
None
__init__
(
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
*
,
output_type
:
Any
=
str
,
instructions
:
(
str
|
SystemPromptFunc
[
AgentDepsT
]
|
Sequence
[
str
|
SystemPromptFunc
[
AgentDepsT
]]
|
None
)
=
None
,
system_prompt
:
str
|
Sequence
[
str
]
=
(),
deps_type
:
type
[
AgentDepsT
]
=
NoneType
,
name
:
str
|
None
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
retries
:
int
=
1
,
output_retries
:
int
|
None
=
None
,
tools
:
Sequence
[
Tool
[
AgentDepsT
]
|
ToolFuncEither
[
AgentDepsT
,
...
]
]
=
(),
prepare_tools
:
(
ToolsPrepareFunc
[
AgentDepsT
]
|
None
)
=
None
,
mcp_servers
:
Sequence
[
MCPServer
]
=
(),
defer_model_check
:
bool
=
False
,
end_strategy
:
EndStrategy
=
"early"
,
instrument
:
(
InstrumentationSettings
|
bool
|
None
)
=
None
,
history_processors
:
(
Sequence
[
HistoryProcessor
[
AgentDepsT
]]
|
None
)
=
None
,
**
_deprecated_kwargs
:
Any
)
Create an agent.
Parameters:
Name
Type
Description
Default
model
Model
|
KnownModelName
|
str
| None
The default model to use for this agent, if not provide,
you must provide the model when calling it. We allow
str
here since the actual list of allowed models changes frequently.
None
output_type
Any
The type of the output data, used to validate the data returned by the model,
defaults to
str
.
str
instructions
str
|
SystemPromptFunc
[
AgentDepsT
] |
Sequence
[
str
|
SystemPromptFunc
[
AgentDepsT
]] | None
Instructions to use for this agent, you can also register instructions via a function with
instructions
.
None
system_prompt
str
|
Sequence
[
str
]
Static system prompts to use for this agent, you can also register system
prompts via a function with
system_prompt
.
()
deps_type
type
[
AgentDepsT
]
The type used for dependency injection, this parameter exists solely to allow you to fully
parameterize the agent, and therefore get the best out of static type checking.
If you're not using deps, but want type checking to pass, you can set
deps=None
to satisfy Pyright
or add a type hint
: Agent[None, <return type>]
.
NoneType
name
str
| None
The name of the agent, used for logging. If
None
, we try to infer the agent name from the call frame
when the agent is first run.
None
model_settings
ModelSettings
| None
Optional model request settings to use for this agent's runs, by default.
None
retries
int
The default number of retries to allow before raising an error.
1
output_retries
int
| None
The maximum number of retries to allow for result validation, defaults to
retries
.
None
tools
Sequence
[
Tool
[
AgentDepsT
] |
ToolFuncEither
[
AgentDepsT
, ...]]
Tools to register with the agent, you can also register tools via the decorators
@agent.tool
and
@agent.tool_plain
.
()
prepare_tools
ToolsPrepareFunc
[
AgentDepsT
] | None
custom method to prepare the tool definition of all tools for each step.
This is useful if you want to customize the definition of multiple tools or you want to register
a subset of tools for a given step. See
ToolsPrepareFunc
None
mcp_servers
Sequence
[
MCPServer
]
MCP servers to register with the agent. You should register a
MCPServer
for each server you want the agent to connect to.
()
defer_model_check
bool
by default, if you provide a
named
model,
it's evaluated to create a
Model
instance immediately,
which checks for the necessary environment variables. Set this to
false
to defer the evaluation until the first run. Useful if you want to
override the model
for testing.
False
end_strategy
EndStrategy
Strategy for handling tool calls that are requested alongside a final result.
See
EndStrategy
for more information.
'early'
instrument
InstrumentationSettings
|
bool
| None
Set to True to automatically instrument with OpenTelemetry,
which will use Logfire if it's configured.
Set to an instance of
InstrumentationSettings
to customize.
If this isn't set, then the last value set by
Agent.instrument_all()
will be used, which defaults to False.
See the
Debugging and Monitoring guide
for more info.
None
history_processors
Sequence
[
HistoryProcessor
[
AgentDepsT
]] | None
Optional list of callables to process the message history before sending it to the model.
Each processor takes a list of messages and returns a modified list of messages.
Processors can be sync or async and are applied in sequence.
None
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
def
__init__
(
self
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
*
,
# TODO change this back to `output_type: _output.OutputType[OutputDataT] = str,` when we remove the overloads
output_type
:
Any
=
str
,
instructions
:
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
Sequence
[
str
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
None
=
None
,
system_prompt
:
str
|
Sequence
[
str
]
=
(),
deps_type
:
type
[
AgentDepsT
]
=
NoneType
,
name
:
str
|
None
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
retries
:
int
=
1
,
output_retries
:
int
|
None
=
None
,
tools
:
Sequence
[
Tool
[
AgentDepsT
]
|
ToolFuncEither
[
AgentDepsT
,
...
]]
=
(),
prepare_tools
:
ToolsPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
mcp_servers
:
Sequence
[
MCPServer
]
=
(),
defer_model_check
:
bool
=
False
,
end_strategy
:
EndStrategy
=
'early'
,
instrument
:
InstrumentationSettings
|
bool
|
None
=
None
,
history_processors
:
Sequence
[
HistoryProcessor
[
AgentDepsT
]]
|
None
=
None
,
**
_deprecated_kwargs
:
Any
,
):
"""Create an agent.
Args:
model: The default model to use for this agent, if not provide,
you must provide the model when calling it. We allow `str` here since the actual list of allowed models changes frequently.
output_type: The type of the output data, used to validate the data returned by the model,
defaults to `str`.
instructions: Instructions to use for this agent, you can also register instructions via a function with
[`instructions`][pydantic_ai.Agent.instructions].
system_prompt: Static system prompts to use for this agent, you can also register system
prompts via a function with [`system_prompt`][pydantic_ai.Agent.system_prompt].
deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully
parameterize the agent, and therefore get the best out of static type checking.
If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright
or add a type hint `: Agent[None, <return type>]`.
name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame
when the agent is first run.
model_settings: Optional model request settings to use for this agent's runs, by default.
retries: The default number of retries to allow before raising an error.
output_retries: The maximum number of retries to allow for result validation, defaults to `retries`.
tools: Tools to register with the agent, you can also register tools via the decorators
[`@agent.tool`][pydantic_ai.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain].
prepare_tools: custom method to prepare the tool definition of all tools for each step.
This is useful if you want to customize the definition of multiple tools or you want to register
a subset of tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]
mcp_servers: MCP servers to register with the agent. You should register a [`MCPServer`][pydantic_ai.mcp.MCPServer]
for each server you want the agent to connect to.
defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,
it's evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,
which checks for the necessary environment variables. Set this to `false`
to defer the evaluation until the first run. Useful if you want to
[override the model][pydantic_ai.Agent.override] for testing.
end_strategy: Strategy for handling tool calls that are requested alongside a final result.
See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.
instrument: Set to True to automatically instrument with OpenTelemetry,
which will use Logfire if it's configured.
Set to an instance of [`InstrumentationSettings`][pydantic_ai.agent.InstrumentationSettings] to customize.
If this isn't set, then the last value set by
[`Agent.instrument_all()`][pydantic_ai.Agent.instrument_all]
will be used, which defaults to False.
See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
history_processors: Optional list of callables to process the message history before sending it to the model.
Each processor takes a list of messages and returns a modified list of messages.
Processors can be sync or async and are applied in sequence.
"""
if
model
is
None
or
defer_model_check
:
self
.
model
=
model
else
:
self
.
model
=
models
.
infer_model
(
model
)
self
.
end_strategy
=
end_strategy
self
.
name
=
name
self
.
model_settings
=
model_settings
if
'result_type'
in
_deprecated_kwargs
:
if
output_type
is
not
str
:
# pragma: no cover
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
self
.
output_type
=
output_type
self
.
instrument
=
instrument
self
.
_deps_type
=
deps_type
self
.
_deprecated_result_tool_name
=
_deprecated_kwargs
.
pop
(
'result_tool_name'
,
None
)
if
self
.
_deprecated_result_tool_name
is
not
None
:
warnings
.
warn
(
'`result_tool_name` is deprecated, use `output_type` with `ToolOutput` instead'
,
DeprecationWarning
,
stacklevel
=
2
,
)
self
.
_deprecated_result_tool_description
=
_deprecated_kwargs
.
pop
(
'result_tool_description'
,
None
)
if
self
.
_deprecated_result_tool_description
is
not
None
:
warnings
.
warn
(
'`result_tool_description` is deprecated, use `output_type` with `ToolOutput` instead'
,
DeprecationWarning
,
stacklevel
=
2
,
)
result_retries
=
_deprecated_kwargs
.
pop
(
'result_retries'
,
None
)
if
result_retries
is
not
None
:
if
output_retries
is
not
None
:
# pragma: no cover
raise
TypeError
(
'`output_retries` and `result_retries` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_retries` is deprecated, use `max_result_retries` instead'
,
DeprecationWarning
,
stacklevel
=
2
)
output_retries
=
result_retries
default_output_mode
=
(
self
.
model
.
profile
.
default_structured_output_mode
if
isinstance
(
self
.
model
,
models
.
Model
)
else
None
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
self
.
_output_schema
=
_output
.
OutputSchema
[
OutputDataT
]
.
build
(
output_type
,
default_mode
=
default_output_mode
,
name
=
self
.
_deprecated_result_tool_name
,
description
=
self
.
_deprecated_result_tool_description
,
)
self
.
_output_validators
=
[]
self
.
_instructions
=
''
self
.
_instructions_functions
=
[]
if
isinstance
(
instructions
,
(
str
,
Callable
)):
instructions
=
[
instructions
]
for
instruction
in
instructions
or
[]:
if
isinstance
(
instruction
,
str
):
self
.
_instructions
+=
instruction
+
'
\n
'
else
:
self
.
_instructions_functions
.
append
(
_system_prompt
.
SystemPromptRunner
(
instruction
))
self
.
_instructions
=
self
.
_instructions
.
strip
()
or
None
self
.
_system_prompts
=
(
system_prompt
,)
if
isinstance
(
system_prompt
,
str
)
else
tuple
(
system_prompt
)
self
.
_system_prompt_functions
=
[]
self
.
_system_prompt_dynamic_functions
=
{}
self
.
_function_tools
=
{}
self
.
_default_retries
=
retries
self
.
_max_result_retries
=
output_retries
if
output_retries
is
not
None
else
retries
self
.
_mcp_servers
=
mcp_servers
self
.
_prepare_tools
=
prepare_tools
self
.
history_processors
=
history_processors
or
[]
for
tool
in
tools
:
if
isinstance
(
tool
,
Tool
):
self
.
_register_tool
(
tool
)
else
:
self
.
_register_tool
(
Tool
(
tool
))
self
.
_override_deps
:
ContextVar
[
_utils
.
Option
[
AgentDepsT
]]
=
ContextVar
(
'_override_deps'
,
default
=
None
)
self
.
_override_model
:
ContextVar
[
_utils
.
Option
[
models
.
Model
]]
=
ContextVar
(
'_override_model'
,
default
=
None
)
end_strategy
instance-attribute
end_strategy
:
EndStrategy
=
end_strategy
Strategy for handling tool calls when a final result is found.
name
instance-attribute
name
:
str
|
None
=
name
The name of the agent, used for logging.
If
None
, we try to infer the agent name from the call frame when the agent is first run.
model_settings
instance-attribute
model_settings
:
ModelSettings
|
None
=
model_settings
Optional model request settings to use for this agents's runs, by default.
Note, if
model_settings
is provided by
run
,
run_sync
, or
run_stream
, those settings will
be merged with this value, with the runtime argument taking priority.
output_type
instance-attribute
output_type
:
OutputSpec
[
OutputDataT
]
=
output_type
The type of data output by agent runs, used to validate the data returned by the model, defaults to
str
.
instrument
instance-attribute
instrument
:
InstrumentationSettings
|
bool
|
None
=
(
instrument
)
Options to automatically instrument with OpenTelemetry.
instrument_all
staticmethod
instrument_all
(
instrument
:
InstrumentationSettings
|
bool
=
True
,
)
->
None
Set the instrumentation options for all agents where
instrument
is not set.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
376
377
378
379
@staticmethod
def
instrument_all
(
instrument
:
InstrumentationSettings
|
bool
=
True
)
->
None
:
"""Set the instrumentation options for all agents where `instrument` is not set."""
Agent
.
_instrument_default
=
instrument
run
async
run
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
output_type
:
None
=
None
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AgentRunResult
[
OutputDataT
]
run
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
],
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AgentRunResult
[
RunOutputDataT
]
run
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AgentRunResult
[
RunOutputDataT
]
run
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
)
->
AgentRunResult
[
Any
]
Run the agent with a user prompt in async mode.
This method builds an internal agent graph (using system prompts, tools and result schemas) and then
runs the graph to completion. The result of the run is returned.
Example:
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
)
async
def
main
():
agent_run
=
await
agent
.
run
(
'What is the capital of France?'
)
print
(
agent_run
.
output
)
#> Paris
Parameters:
Name
Type
Description
Default
user_prompt
str
|
Sequence
[
UserContent
] | None
User input to start/continue the conversation.
None
output_type
OutputSpec
[
RunOutputDataT
] | None
Custom output type to use for this run,
output_type
may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
None
message_history
list
[
ModelMessage
] | None
History of the conversation so far.
None
model
Model
|
KnownModelName
|
str
| None
Optional model to use for this run, required if
model
was not set when creating the agent.
None
deps
AgentDepsT
Optional dependencies to use for this run.
None
model_settings
ModelSettings
| None
Optional settings to use for this model's request.
None
usage_limits
UsageLimits
| None
Optional limits on model request count or token usage.
None
usage
Usage
| None
Optional usage to start with, useful for resuming a conversation or agents used in tools.
None
infer_name
bool
Whether to try to infer the agent name from the call frame if it's not set.
True
Returns:
Type
Description
AgentRunResult
[
Any
]
The result of the run.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
async
def
run
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AgentRunResult
[
Any
]:
"""Run the agent with a user prompt in async mode.
This method builds an internal agent graph (using system prompts, tools and result schemas) and then
runs the graph to completion. The result of the run is returned.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
async def main():
agent_run = await agent.run('What is the capital of France?')
print(agent_run.output)
#> Paris
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
if
infer_name
and
self
.
name
is
None
:
self
.
_infer_name
(
inspect
.
currentframe
())
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
async
with
self
.
iter
(
user_prompt
=
user_prompt
,
output_type
=
output_type
,
message_history
=
message_history
,
model
=
model
,
deps
=
deps
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
usage
=
usage
,
)
as
agent_run
:
async
for
_
in
agent_run
:
pass
assert
agent_run
.
result
is
not
None
,
'The graph run did not finish properly'
return
agent_run
.
result
iter
async
iter
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
,
*
,
output_type
:
None
=
None
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
)
->
AbstractAsyncContextManager
[
AgentRun
[
AgentDepsT
,
OutputDataT
]
]
iter
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
],
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
)
->
AbstractAsyncContextManager
[
AgentRun
[
AgentDepsT
,
RunOutputDataT
]
]
iter
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AbstractAsyncContextManager
[
AgentRun
[
AgentDepsT
,
Any
]]
iter
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
)
->
AsyncIterator
[
AgentRun
[
AgentDepsT
,
Any
]]
A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.
This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an
AgentRun
object. The
AgentRun
can be used to async-iterate over the nodes of the graph as they are
executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the
stream of events coming from the execution of tools.
The
AgentRun
also provides methods to access the full message history, new messages, and usage statistics,
and the final result of the run once it has completed.
For more details, see the documentation of
AgentRun
.
Example:
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
)
async
def
main
():
nodes
=
[]
async
with
agent
.
iter
(
'What is the capital of France?'
)
as
agent_run
:
async
for
node
in
agent_run
:
nodes
.
append
(
node
)
print
(
nodes
)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1, request_tokens=56, response_tokens=1, total_tokens=57
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print
(
agent_run
.
result
.
output
)
#> Paris
Parameters:
Name
Type
Description
Default
user_prompt
str
|
Sequence
[
UserContent
] | None
User input to start/continue the conversation.
None
output_type
OutputSpec
[
RunOutputDataT
] | None
Custom output type to use for this run,
output_type
may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
None
message_history
list
[
ModelMessage
] | None
History of the conversation so far.
None
model
Model
|
KnownModelName
|
str
| None
Optional model to use for this run, required if
model
was not set when creating the agent.
None
deps
AgentDepsT
Optional dependencies to use for this run.
None
model_settings
ModelSettings
| None
Optional settings to use for this model's request.
None
usage_limits
UsageLimits
| None
Optional limits on model request count or token usage.
None
usage
Usage
| None
Optional usage to start with, useful for resuming a conversation or agents used in tools.
None
infer_name
bool
Whether to try to infer the agent name from the call frame if it's not set.
True
Returns:
Type
Description
AsyncIterator
[
AgentRun
[
AgentDepsT
,
Any
]]
The result of the run.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
@asynccontextmanager
async
def
iter
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AsyncIterator
[
AgentRun
[
AgentDepsT
,
Any
]]:
"""A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.
This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an
`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are
executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the
stream of events coming from the execution of tools.
The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,
and the final result of the run once it has completed.
For more details, see the documentation of `AgentRun`.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
async def main():
nodes = []
async with agent.iter('What is the capital of France?') as agent_run:
async for node in agent_run:
nodes.append(node)
print(nodes)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1, request_tokens=56, response_tokens=1, total_tokens=57
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print(agent_run.result.output)
#> Paris
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
if
infer_name
and
self
.
name
is
None
:
self
.
_infer_name
(
inspect
.
currentframe
())
model_used
=
self
.
_get_model
(
model
)
del
model
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
deps
=
self
.
_get_deps
(
deps
)
new_message_index
=
len
(
message_history
)
if
message_history
else
0
output_schema
=
self
.
_prepare_output_schema
(
output_type
,
model_used
.
profile
)
output_type_
=
output_type
or
self
.
output_type
# Build the graph
graph
:
Graph
[
_agent_graph
.
GraphAgentState
,
_agent_graph
.
GraphAgentDeps
[
AgentDepsT
,
Any
],
FinalResult
[
Any
]]
=
(
_agent_graph
.
build_agent_graph
(
self
.
name
,
self
.
_deps_type
,
output_type_
)
)
# Build the initial state
usage
=
usage
or
_usage
.
Usage
()
state
=
_agent_graph
.
GraphAgentState
(
message_history
=
message_history
[:]
if
message_history
else
[],
usage
=
usage
,
retries
=
0
,
run_step
=
0
,
)
# We consider it a user error if a user tries to restrict the result type while having an output validator that
# may change the result type from the restricted type to something else. Therefore, we consider the following
# typecast reasonable, even though it is possible to violate it with otherwise-type-checked code.
output_validators
=
cast
(
list
[
_output
.
OutputValidator
[
AgentDepsT
,
RunOutputDataT
]],
self
.
_output_validators
)
model_settings
=
merge_model_settings
(
self
.
model_settings
,
model_settings
)
usage_limits
=
usage_limits
or
_usage
.
UsageLimits
()
if
isinstance
(
model_used
,
InstrumentedModel
):
instrumentation_settings
=
model_used
.
settings
tracer
=
model_used
.
settings
.
tracer
else
:
instrumentation_settings
=
None
tracer
=
NoOpTracer
()
agent_name
=
self
.
name
or
'agent'
run_span
=
tracer
.
start_span
(
'agent run'
,
attributes
=
{
'model_name'
:
model_used
.
model_name
if
model_used
else
'no-model'
,
'agent_name'
:
agent_name
,
'logfire.msg'
:
f
'
{
agent_name
}
run'
,
},
)
async
def
get_instructions
(
run_context
:
RunContext
[
AgentDepsT
])
->
str
|
None
:
parts
=
[
self
.
_instructions
,
*
[
await
func
.
run
(
run_context
)
for
func
in
self
.
_instructions_functions
],
]
model_profile
=
model_used
.
profile
if
isinstance
(
output_schema
,
_output
.
PromptedOutputSchema
):
instructions
=
output_schema
.
instructions
(
model_profile
.
prompted_output_template
)
parts
.
append
(
instructions
)
parts
=
[
p
for
p
in
parts
if
p
]
if
not
parts
:
return
None
return
'
\n\n
'
.
join
(
parts
)
.
strip
()
# Copy the function tools so that retry state is agent-run-specific
# Note that the retry count is reset to 0 when this happens due to the `default=0` and `init=False`.
run_function_tools
=
{
k
:
dataclasses
.
replace
(
v
)
for
k
,
v
in
self
.
_function_tools
.
items
()}
graph_deps
=
_agent_graph
.
GraphAgentDeps
[
AgentDepsT
,
RunOutputDataT
](
user_deps
=
deps
,
prompt
=
user_prompt
,
new_message_index
=
new_message_index
,
model
=
model_used
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
max_result_retries
=
self
.
_max_result_retries
,
end_strategy
=
self
.
end_strategy
,
output_schema
=
output_schema
,
output_validators
=
output_validators
,
history_processors
=
self
.
history_processors
,
function_tools
=
run_function_tools
,
mcp_servers
=
self
.
_mcp_servers
,
default_retries
=
self
.
_default_retries
,
tracer
=
tracer
,
prepare_tools
=
self
.
_prepare_tools
,
get_instructions
=
get_instructions
,
instrumentation_settings
=
instrumentation_settings
,
)
start_node
=
_agent_graph
.
UserPromptNode
[
AgentDepsT
](
user_prompt
=
user_prompt
,
instructions
=
self
.
_instructions
,
instructions_functions
=
self
.
_instructions_functions
,
system_prompts
=
self
.
_system_prompts
,
system_prompt_functions
=
self
.
_system_prompt_functions
,
system_prompt_dynamic_functions
=
self
.
_system_prompt_dynamic_functions
,
)
try
:
async
with
graph
.
iter
(
start_node
,
state
=
state
,
deps
=
graph_deps
,
span
=
use_span
(
run_span
)
if
run_span
.
is_recording
()
else
None
,
infer_name
=
False
,
)
as
graph_run
:
agent_run
=
AgentRun
(
graph_run
)
yield
agent_run
if
(
final_result
:=
agent_run
.
result
)
is
not
None
and
run_span
.
is_recording
():
run_span
.
set_attribute
(
'final_result'
,
(
final_result
.
output
if
isinstance
(
final_result
.
output
,
str
)
else
json
.
dumps
(
InstrumentedModel
.
serialize_any
(
final_result
.
output
))
),
)
finally
:
try
:
if
instrumentation_settings
and
run_span
.
is_recording
():
run_span
.
set_attributes
(
self
.
_run_span_end_attributes
(
state
,
usage
,
instrumentation_settings
))
finally
:
run_span
.
end
()
run_sync
run_sync
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AgentRunResult
[
OutputDataT
]
run_sync
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AgentRunResult
[
RunOutputDataT
]
run_sync
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AgentRunResult
[
RunOutputDataT
]
run_sync
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
)
->
AgentRunResult
[
Any
]
Synchronously run the agent with a user prompt.
This is a convenience method that wraps
self.run
with
loop.run_until_complete(...)
.
You therefore can't use this method inside async code or if there's an active event loop.
Example:
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
)
result_sync
=
agent
.
run_sync
(
'What is the capital of Italy?'
)
print
(
result_sync
.
output
)
#> Rome
Parameters:
Name
Type
Description
Default
user_prompt
str
|
Sequence
[
UserContent
] | None
User input to start/continue the conversation.
None
output_type
OutputSpec
[
RunOutputDataT
] | None
Custom output type to use for this run,
output_type
may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
None
message_history
list
[
ModelMessage
] | None
History of the conversation so far.
None
model
Model
|
KnownModelName
|
str
| None
Optional model to use for this run, required if
model
was not set when creating the agent.
None
deps
AgentDepsT
Optional dependencies to use for this run.
None
model_settings
ModelSettings
| None
Optional settings to use for this model's request.
None
usage_limits
UsageLimits
| None
Optional limits on model request count or token usage.
None
usage
Usage
| None
Optional usage to start with, useful for resuming a conversation or agents used in tools.
None
infer_name
bool
Whether to try to infer the agent name from the call frame if it's not set.
True
Returns:
Type
Description
AgentRunResult
[
Any
]
The result of the run.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
def
run_sync
(
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AgentRunResult
[
Any
]:
"""Synchronously run the agent with a user prompt.
This is a convenience method that wraps [`self.run`][pydantic_ai.Agent.run] with `loop.run_until_complete(...)`.
You therefore can't use this method inside async code or if there's an active event loop.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
result_sync = agent.run_sync('What is the capital of Italy?')
print(result_sync.output)
#> Rome
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
if
infer_name
and
self
.
name
is
None
:
self
.
_infer_name
(
inspect
.
currentframe
())
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
return
get_event_loop
()
.
run_until_complete
(
self
.
run
(
user_prompt
,
output_type
=
output_type
,
message_history
=
message_history
,
model
=
model
,
deps
=
deps
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
usage
=
usage
,
infer_name
=
False
,
)
)
run_stream
async
run_stream
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AbstractAsyncContextManager
[
StreamedRunResult
[
AgentDepsT
,
OutputDataT
]
]
run_stream
(
user_prompt
:
str
|
Sequence
[
UserContent
],
*
,
output_type
:
OutputSpec
[
RunOutputDataT
],
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AbstractAsyncContextManager
[
StreamedRunResult
[
AgentDepsT
,
RunOutputDataT
]
]
run_stream
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
result_type
:
type
[
RunOutputDataT
],
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
)
->
AbstractAsyncContextManager
[
StreamedRunResult
[
AgentDepsT
,
RunOutputDataT
]
]
run_stream
(
user_prompt
:
str
|
Sequence
[
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
ModelMessage
]
|
None
=
None
,
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
UsageLimits
|
None
=
None
,
usage
:
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
)
->
AsyncIterator
[
StreamedRunResult
[
AgentDepsT
,
Any
]]
Run the agent with a user prompt in async mode, returning a streamed response.
Example:
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
)
async
def
main
():
async
with
agent
.
run_stream
(
'What is the capital of the UK?'
)
as
response
:
print
(
await
response
.
get_output
())
#> London
Parameters:
Name
Type
Description
Default
user_prompt
str
|
Sequence
[
UserContent
] | None
User input to start/continue the conversation.
None
output_type
OutputSpec
[
RunOutputDataT
] | None
Custom output type to use for this run,
output_type
may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
None
message_history
list
[
ModelMessage
] | None
History of the conversation so far.
None
model
Model
|
KnownModelName
|
str
| None
Optional model to use for this run, required if
model
was not set when creating the agent.
None
deps
AgentDepsT
Optional dependencies to use for this run.
None
model_settings
ModelSettings
| None
Optional settings to use for this model's request.
None
usage_limits
UsageLimits
| None
Optional limits on model request count or token usage.
None
usage
Usage
| None
Optional usage to start with, useful for resuming a conversation or agents used in tools.
None
infer_name
bool
Whether to try to infer the agent name from the call frame if it's not set.
True
Returns:
Type
Description
AsyncIterator
[
StreamedRunResult
[
AgentDepsT
,
Any
]]
The result of the run.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
@asynccontextmanager
async
def
run_stream
(
# noqa C901
self
,
user_prompt
:
str
|
Sequence
[
_messages
.
UserContent
]
|
None
=
None
,
*
,
output_type
:
OutputSpec
[
RunOutputDataT
]
|
None
=
None
,
message_history
:
list
[
_messages
.
ModelMessage
]
|
None
=
None
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
,
deps
:
AgentDepsT
=
None
,
model_settings
:
ModelSettings
|
None
=
None
,
usage_limits
:
_usage
.
UsageLimits
|
None
=
None
,
usage
:
_usage
.
Usage
|
None
=
None
,
infer_name
:
bool
=
True
,
**
_deprecated_kwargs
:
Never
,
)
->
AsyncIterator
[
result
.
StreamedRunResult
[
AgentDepsT
,
Any
]]:
"""Run the agent with a user prompt in async mode, returning a streamed response.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
async def main():
async with agent.run_stream('What is the capital of the UK?') as response:
print(await response.get_output())
#> London
```
Args:
user_prompt: User input to start/continue the conversation.
output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
output validators since output validators would expect an argument that matches the agent's output type.
message_history: History of the conversation so far.
model: Optional model to use for this run, required if `model` was not set when creating the agent.
deps: Optional dependencies to use for this run.
model_settings: Optional settings to use for this model's request.
usage_limits: Optional limits on model request count or token usage.
usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
infer_name: Whether to try to infer the agent name from the call frame if it's not set.
Returns:
The result of the run.
"""
# TODO: We need to deprecate this now that we have the `iter` method.
#   Before that, though, we should add an event for when we reach the final result of the stream.
if
infer_name
and
self
.
name
is
None
:
# f_back because `asynccontextmanager` adds one frame
if
frame
:=
inspect
.
currentframe
():
# pragma: no branch
self
.
_infer_name
(
frame
.
f_back
)
if
'result_type'
in
_deprecated_kwargs
:
# pragma: no cover
if
output_type
is
not
str
:
raise
TypeError
(
'`result_type` and `output_type` cannot be set at the same time.'
)
warnings
.
warn
(
'`result_type` is deprecated, use `output_type` instead.'
,
DeprecationWarning
,
stacklevel
=
2
)
output_type
=
_deprecated_kwargs
.
pop
(
'result_type'
)
_utils
.
validate_empty_kwargs
(
_deprecated_kwargs
)
yielded
=
False
async
with
self
.
iter
(
user_prompt
,
output_type
=
output_type
,
message_history
=
message_history
,
model
=
model
,
deps
=
deps
,
model_settings
=
model_settings
,
usage_limits
=
usage_limits
,
usage
=
usage
,
infer_name
=
False
,
)
as
agent_run
:
first_node
=
agent_run
.
next_node
# start with the first node
assert
isinstance
(
first_node
,
_agent_graph
.
UserPromptNode
)
# the first node should be a user prompt node
node
=
first_node
while
True
:
if
self
.
is_model_request_node
(
node
):
graph_ctx
=
agent_run
.
ctx
async
with
node
.
_stream
(
graph_ctx
)
as
streamed_response
:
# pyright: ignore[reportPrivateUsage]
async
def
stream_to_final
(
s
:
models
.
StreamedResponse
,
)
->
FinalResult
[
models
.
StreamedResponse
]
|
None
:
output_schema
=
graph_ctx
.
deps
.
output_schema
async
for
maybe_part_event
in
streamed_response
:
if
isinstance
(
maybe_part_event
,
_messages
.
PartStartEvent
):
new_part
=
maybe_part_event
.
part
if
isinstance
(
new_part
,
_messages
.
TextPart
)
and
isinstance
(
output_schema
,
_output
.
TextOutputSchema
):
return
FinalResult
(
s
,
None
,
None
)
elif
isinstance
(
new_part
,
_messages
.
ToolCallPart
)
and
isinstance
(
output_schema
,
_output
.
ToolOutputSchema
):
# pragma: no branch
for
call
,
_
in
output_schema
.
find_tool
([
new_part
]):
return
FinalResult
(
s
,
call
.
tool_name
,
call
.
tool_call_id
)
return
None
final_result_details
=
await
stream_to_final
(
streamed_response
)
if
final_result_details
is
not
None
:
if
yielded
:
raise
exceptions
.
AgentRunError
(
'Agent run produced final results'
)
# pragma: no cover
yielded
=
True
messages
=
graph_ctx
.
state
.
message_history
.
copy
()
async
def
on_complete
()
->
None
:
"""Called when the stream has completed.
The model response will have been added to messages by now
by `StreamedRunResult._marked_completed`.
"""
last_message
=
messages
[
-
1
]
assert
isinstance
(
last_message
,
_messages
.
ModelResponse
)
tool_calls
=
[
part
for
part
in
last_message
.
parts
if
isinstance
(
part
,
_messages
.
ToolCallPart
)
]
parts
:
list
[
_messages
.
ModelRequestPart
]
=
[]
async
for
_event
in
_agent_graph
.
process_function_tools
(
tool_calls
,
final_result_details
.
tool_name
,
final_result_details
.
tool_call_id
,
graph_ctx
,
parts
,
):
pass
# TODO: Should we do something here related to the retry count?
#   Maybe we should move the incrementing of the retry count to where we actually make a request?
# if any(isinstance(part, _messages.RetryPromptPart) for part in parts):
#     ctx.state.increment_retries(ctx.deps.max_result_retries)
if
parts
:
messages
.
append
(
_messages
.
ModelRequest
(
parts
))
yield
StreamedRunResult
(
messages
,
graph_ctx
.
deps
.
new_message_index
,
graph_ctx
.
deps
.
usage_limits
,
streamed_response
,
graph_ctx
.
deps
.
output_schema
,
_agent_graph
.
build_run_context
(
graph_ctx
),
graph_ctx
.
deps
.
output_validators
,
final_result_details
.
tool_name
,
on_complete
,
)
break
next_node
=
await
agent_run
.
next
(
node
)
if
not
isinstance
(
next_node
,
_agent_graph
.
AgentNode
):
raise
exceptions
.
AgentRunError
(
# pragma: no cover
'Should have produced a StreamedRunResult before getting here'
)
node
=
cast
(
_agent_graph
.
AgentNode
[
Any
,
Any
],
next_node
)
if
not
yielded
:
raise
exceptions
.
AgentRunError
(
'Agent run finished without producing a final result'
)
# pragma: no cover
override
override
(
*
,
deps
:
AgentDepsT
|
Unset
=
UNSET
,
model
:
Model
|
KnownModelName
|
str
|
Unset
=
UNSET
)
->
Iterator
[
None
]
Context manager to temporarily override agent dependencies and model.
This is particularly useful when testing.
You can find an example of this
here
.
Parameters:
Name
Type
Description
Default
deps
AgentDepsT
|
Unset
The dependencies to use instead of the dependencies passed to the agent run.
UNSET
model
Model
|
KnownModelName
|
str
|
Unset
The model to use instead of the model passed to the agent run.
UNSET
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
@contextmanager
def
override
(
self
,
*
,
deps
:
AgentDepsT
|
_utils
.
Unset
=
_utils
.
UNSET
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
_utils
.
Unset
=
_utils
.
UNSET
,
)
->
Iterator
[
None
]:
"""Context manager to temporarily override agent dependencies and model.
This is particularly useful when testing.
You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).
Args:
deps: The dependencies to use instead of the dependencies passed to the agent run.
model: The model to use instead of the model passed to the agent run.
"""
if
_utils
.
is_set
(
deps
):
deps_token
=
self
.
_override_deps
.
set
(
_utils
.
Some
(
deps
))
else
:
deps_token
=
None
if
_utils
.
is_set
(
model
):
model_token
=
self
.
_override_model
.
set
(
_utils
.
Some
(
models
.
infer_model
(
model
)))
else
:
model_token
=
None
try
:
yield
finally
:
if
deps_token
is
not
None
:
self
.
_override_deps
.
reset
(
deps_token
)
if
model_token
is
not
None
:
self
.
_override_model
.
reset
(
model_token
)
instructions
instructions
(
func
:
Callable
[[
RunContext
[
AgentDepsT
]],
str
],
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
str
]
instructions
(
func
:
Callable
[
[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]
],
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]]
instructions
(
func
:
Callable
[[],
str
])
->
Callable
[[],
str
]
instructions
(
func
:
Callable
[[],
Awaitable
[
str
]],
)
->
Callable
[[],
Awaitable
[
str
]]
instructions
()
->
Callable
[
[
SystemPromptFunc
[
AgentDepsT
]],
SystemPromptFunc
[
AgentDepsT
],
]
instructions
(
func
:
SystemPromptFunc
[
AgentDepsT
]
|
None
=
None
,
)
->
(
Callable
[
[
SystemPromptFunc
[
AgentDepsT
]],
SystemPromptFunc
[
AgentDepsT
],
]
|
SystemPromptFunc
[
AgentDepsT
]
)
Decorator to register an instructions function.
Optionally takes
RunContext
as its only argument.
Can decorate a sync or async functions.
The decorator can be used bare (
agent.instructions
).
Overloads for every possible signature of
instructions
are included so the decorator doesn't obscure
the type of the function.
Example:
from
pydantic_ai
import
Agent
,
RunContext
agent
=
Agent
(
'test'
,
deps_type
=
str
)
@agent
.
instructions
def
simple_instructions
()
->
str
:
return
'foobar'
@agent
.
instructions
async
def
async_instructions
(
ctx
:
RunContext
[
str
])
->
str
:
return
f
'
{
ctx
.
deps
}
is the best'
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
def
instructions
(
self
,
func
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
None
=
None
,
/
,
)
->
(
Callable
[[
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]],
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
):
"""Decorator to register an instructions function.
Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.
Can decorate a sync or async functions.
The decorator can be used bare (`agent.instructions`).
Overloads for every possible signature of `instructions` are included so the decorator doesn't obscure
the type of the function.
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test', deps_type=str)
@agent.instructions
def simple_instructions() -> str:
return 'foobar'
@agent.instructions
async def async_instructions(ctx: RunContext[str]) -> str:
return f'{ctx.deps} is the best'
```
"""
if
func
is
None
:
def
decorator
(
func_
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
],
)
->
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]:
self
.
_instructions_functions
.
append
(
_system_prompt
.
SystemPromptRunner
(
func_
))
return
func_
return
decorator
else
:
self
.
_instructions_functions
.
append
(
_system_prompt
.
SystemPromptRunner
(
func
))
return
func
system_prompt
system_prompt
(
func
:
Callable
[[
RunContext
[
AgentDepsT
]],
str
],
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
str
]
system_prompt
(
func
:
Callable
[
[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]
],
)
->
Callable
[[
RunContext
[
AgentDepsT
]],
Awaitable
[
str
]]
system_prompt
(
func
:
Callable
[[],
str
])
->
Callable
[[],
str
]
system_prompt
(
func
:
Callable
[[],
Awaitable
[
str
]],
)
->
Callable
[[],
Awaitable
[
str
]]
system_prompt
(
*
,
dynamic
:
bool
=
False
)
->
Callable
[
[
SystemPromptFunc
[
AgentDepsT
]],
SystemPromptFunc
[
AgentDepsT
],
]
system_prompt
(
func
:
SystemPromptFunc
[
AgentDepsT
]
|
None
=
None
,
/
,
*
,
dynamic
:
bool
=
False
,
)
->
(
Callable
[
[
SystemPromptFunc
[
AgentDepsT
]],
SystemPromptFunc
[
AgentDepsT
],
]
|
SystemPromptFunc
[
AgentDepsT
]
)
Decorator to register a system prompt function.
Optionally takes
RunContext
as its only argument.
Can decorate a sync or async functions.
The decorator can be used either bare (
agent.system_prompt
) or as a function call
(
agent.system_prompt(...)
), see the examples below.
Overloads for every possible signature of
system_prompt
are included so the decorator doesn't obscure
the type of the function, see
tests/typed_agent.py
for tests.
Parameters:
Name
Type
Description
Default
func
SystemPromptFunc
[
AgentDepsT
] | None
The function to decorate
None
dynamic
bool
If True, the system prompt will be reevaluated even when
messages_history
is provided,
see
SystemPromptPart.dynamic_ref
False
Example:
from
pydantic_ai
import
Agent
,
RunContext
agent
=
Agent
(
'test'
,
deps_type
=
str
)
@agent
.
system_prompt
def
simple_system_prompt
()
->
str
:
return
'foobar'
@agent
.
system_prompt
(
dynamic
=
True
)
async
def
async_system_prompt
(
ctx
:
RunContext
[
str
])
->
str
:
return
f
'
{
ctx
.
deps
}
is the best'
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
def
system_prompt
(
self
,
func
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
|
None
=
None
,
/
,
*
,
dynamic
:
bool
=
False
,
)
->
(
Callable
[[
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]],
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]]
|
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]
):
"""Decorator to register a system prompt function.
Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.
Can decorate a sync or async functions.
The decorator can be used either bare (`agent.system_prompt`) or as a function call
(`agent.system_prompt(...)`), see the examples below.
Overloads for every possible signature of `system_prompt` are included so the decorator doesn't obscure
the type of the function, see `tests/typed_agent.py` for tests.
Args:
func: The function to decorate
dynamic: If True, the system prompt will be reevaluated even when `messages_history` is provided,
see [`SystemPromptPart.dynamic_ref`][pydantic_ai.messages.SystemPromptPart.dynamic_ref]
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test', deps_type=str)
@agent.system_prompt
def simple_system_prompt() -> str:
return 'foobar'
@agent.system_prompt(dynamic=True)
async def async_system_prompt(ctx: RunContext[str]) -> str:
return f'{ctx.deps} is the best'
```
"""
if
func
is
None
:
def
decorator
(
func_
:
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
],
)
->
_system_prompt
.
SystemPromptFunc
[
AgentDepsT
]:
runner
=
_system_prompt
.
SystemPromptRunner
[
AgentDepsT
](
func_
,
dynamic
=
dynamic
)
self
.
_system_prompt_functions
.
append
(
runner
)
if
dynamic
:
# pragma: lax no cover
self
.
_system_prompt_dynamic_functions
[
func_
.
__qualname__
]
=
runner
return
func_
return
decorator
else
:
assert
not
dynamic
,
"dynamic can't be True in this case"
self
.
_system_prompt_functions
.
append
(
_system_prompt
.
SystemPromptRunner
[
AgentDepsT
](
func
,
dynamic
=
dynamic
))
return
func
output_validator
output_validator
(
func
:
Callable
[
[
RunContext
[
AgentDepsT
],
OutputDataT
],
OutputDataT
],
)
->
Callable
[
[
RunContext
[
AgentDepsT
],
OutputDataT
],
OutputDataT
]
output_validator
(
func
:
Callable
[
[
RunContext
[
AgentDepsT
],
OutputDataT
],
Awaitable
[
OutputDataT
],
],
)
->
Callable
[
[
RunContext
[
AgentDepsT
],
OutputDataT
],
Awaitable
[
OutputDataT
],
]
output_validator
(
func
:
Callable
[[
OutputDataT
],
OutputDataT
],
)
->
Callable
[[
OutputDataT
],
OutputDataT
]
output_validator
(
func
:
Callable
[[
OutputDataT
],
Awaitable
[
OutputDataT
]],
)
->
Callable
[[
OutputDataT
],
Awaitable
[
OutputDataT
]]
output_validator
(
func
:
OutputValidatorFunc
[
AgentDepsT
,
OutputDataT
],
)
->
OutputValidatorFunc
[
AgentDepsT
,
OutputDataT
]
Decorator to register an output validator function.
Optionally takes
RunContext
as its first argument.
Can decorate a sync or async functions.
Overloads for every possible signature of
output_validator
are included so the decorator doesn't obscure
the type of the function, see
tests/typed_agent.py
for tests.
Example:
from
pydantic_ai
import
Agent
,
ModelRetry
,
RunContext
agent
=
Agent
(
'test'
,
deps_type
=
str
)
@agent
.
output_validator
def
output_validator_simple
(
data
:
str
)
->
str
:
if
'wrong'
in
data
:
raise
ModelRetry
(
'wrong response'
)
return
data
@agent
.
output_validator
async
def
output_validator_deps
(
ctx
:
RunContext
[
str
],
data
:
str
)
->
str
:
if
ctx
.
deps
in
data
:
raise
ModelRetry
(
'wrong response'
)
return
data
result
=
agent
.
run_sync
(
'foobar'
,
deps
=
'spam'
)
print
(
result
.
output
)
#> success (no tool calls)
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
1328
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
def
output_validator
(
self
,
func
:
_output
.
OutputValidatorFunc
[
AgentDepsT
,
OutputDataT
],
/
)
->
_output
.
OutputValidatorFunc
[
AgentDepsT
,
OutputDataT
]:
"""Decorator to register an output validator function.
Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.
Can decorate a sync or async functions.
Overloads for every possible signature of `output_validator` are included so the decorator doesn't obscure
the type of the function, see `tests/typed_agent.py` for tests.
Example:
```python
from pydantic_ai import Agent, ModelRetry, RunContext
agent = Agent('test', deps_type=str)
@agent.output_validator
def output_validator_simple(data: str) -> str:
if 'wrong' in data:
raise ModelRetry('wrong response')
return data
@agent.output_validator
async def output_validator_deps(ctx: RunContext[str], data: str) -> str:
if ctx.deps in data:
raise ModelRetry('wrong response')
return data
result = agent.run_sync('foobar', deps='spam')
print(result.output)
#> success (no tool calls)
```
"""
self
.
_output_validators
.
append
(
_output
.
OutputValidator
[
AgentDepsT
,
Any
](
func
))
return
func
tool
tool
(
func
:
ToolFuncContext
[
AgentDepsT
,
ToolParams
],
)
->
ToolFuncContext
[
AgentDepsT
,
ToolParams
]
tool
(
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
"auto"
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
)
->
Callable
[
[
ToolFuncContext
[
AgentDepsT
,
ToolParams
]],
ToolFuncContext
[
AgentDepsT
,
ToolParams
],
]
tool
(
func
:
(
ToolFuncContext
[
AgentDepsT
,
ToolParams
]
|
None
)
=
None
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
"auto"
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Any
Decorator to register a tool function which takes
RunContext
as its first argument.
Can decorate a sync or async functions.
The docstring is inspected to extract both the tool description and description of each parameter,
learn more
.
We can't add overloads for every possible signature of tool, since the return type is a recursive union
so the signature of functions decorated with
@agent.tool
is obscured.
Example:
from
pydantic_ai
import
Agent
,
RunContext
agent
=
Agent
(
'test'
,
deps_type
=
int
)
@agent
.
tool
def
foobar
(
ctx
:
RunContext
[
int
],
x
:
int
)
->
int
:
return
ctx
.
deps
+
x
@agent
.
tool
(
retries
=
2
)
async
def
spam
(
ctx
:
RunContext
[
str
],
y
:
float
)
->
float
:
return
ctx
.
deps
+
y
result
=
agent
.
run_sync
(
'foobar'
,
deps
=
1
)
print
(
result
.
output
)
#> {"foobar":1,"spam":1.0}
Parameters:
Name
Type
Description
Default
func
ToolFuncContext
[
AgentDepsT
,
ToolParams
] | None
The tool function to register.
None
name
str
| None
The name of the tool, defaults to the function name.
None
retries
int
| None
The number of retries to allow for this tool, defaults to the agent's default retries,
which defaults to 1.
None
prepare
ToolPrepareFunc
[
AgentDepsT
] | None
custom method to prepare the tool definition for each step, return
None
to omit this
tool from a given step. This is useful if you want to customise a tool at call time,
or omit it completely from a step. See
ToolPrepareFunc
.
None
docstring_format
DocstringFormat
The format of the docstring, see
DocstringFormat
.
Defaults to
'auto'
, such that the format is inferred from the structure of the docstring.
'auto'
require_parameter_descriptions
bool
If True, raise an error if a parameter description is missing. Defaults to False.
False
schema_generator
type
[
GenerateJsonSchema
]
The JSON schema generator class to use for this tool. Defaults to
GenerateToolJsonSchema
.
GenerateToolJsonSchema
strict
bool
| None
Whether to enforce JSON schema compliance (only affects OpenAI).
See
ToolDefinition
for more info.
None
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403
1404
1405
1406
1407
1408
1409
1410
1411
1412
1413
1414
1415
1416
1417
1418
1419
1420
1421
1422
1423
1424
1425
1426
1427
1428
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
1439
1440
1441
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
def
tool
(
self
,
func
:
ToolFuncContext
[
AgentDepsT
,
ToolParams
]
|
None
=
None
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
'auto'
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Any
:
"""Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.
Can decorate a sync or async functions.
The docstring is inspected to extract both the tool description and description of each parameter,
[learn more](../tools.md#function-tools-and-schema).
We can't add overloads for every possible signature of tool, since the return type is a recursive union
so the signature of functions decorated with `@agent.tool` is obscured.
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test', deps_type=int)
@agent.tool
def foobar(ctx: RunContext[int], x: int) -> int:
return ctx.deps + x
@agent.tool(retries=2)
async def spam(ctx: RunContext[str], y: float) -> float:
return ctx.deps + y
result = agent.run_sync('foobar', deps=1)
print(result.output)
#> {"foobar":1,"spam":1.0}
```
Args:
func: The tool function to register.
name: The name of the tool, defaults to the function name.
retries: The number of retries to allow for this tool, defaults to the agent's default retries,
which defaults to 1.
prepare: custom method to prepare the tool definition for each step, return `None` to omit this
tool from a given step. This is useful if you want to customise a tool at call time,
or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].
docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].
Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.
require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.
schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.
strict: Whether to enforce JSON schema compliance (only affects OpenAI).
See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.
"""
if
func
is
None
:
def
tool_decorator
(
func_
:
ToolFuncContext
[
AgentDepsT
,
ToolParams
],
)
->
ToolFuncContext
[
AgentDepsT
,
ToolParams
]:
# noinspection PyTypeChecker
self
.
_register_function
(
func_
,
True
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func_
return
tool_decorator
else
:
# noinspection PyTypeChecker
self
.
_register_function
(
func
,
True
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func
tool_plain
tool_plain
(
func
:
ToolFuncPlain
[
ToolParams
],
)
->
ToolFuncPlain
[
ToolParams
]
tool_plain
(
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
"auto"
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
)
->
Callable
[
[
ToolFuncPlain
[
ToolParams
]],
ToolFuncPlain
[
ToolParams
]
]
tool_plain
(
func
:
ToolFuncPlain
[
ToolParams
]
|
None
=
None
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
"auto"
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Any
Decorator to register a tool function which DOES NOT take
RunContext
as an argument.
Can decorate a sync or async functions.
The docstring is inspected to extract both the tool description and description of each parameter,
learn more
.
We can't add overloads for every possible signature of tool, since the return type is a recursive union
so the signature of functions decorated with
@agent.tool
is obscured.
Example:
from
pydantic_ai
import
Agent
,
RunContext
agent
=
Agent
(
'test'
)
@agent
.
tool
def
foobar
(
ctx
:
RunContext
[
int
])
->
int
:
return
123
@agent
.
tool
(
retries
=
2
)
async
def
spam
(
ctx
:
RunContext
[
str
])
->
float
:
return
3.14
result
=
agent
.
run_sync
(
'foobar'
,
deps
=
1
)
print
(
result
.
output
)
#> {"foobar":123,"spam":3.14}
Parameters:
Name
Type
Description
Default
func
ToolFuncPlain
[
ToolParams
] | None
The tool function to register.
None
name
str
| None
The name of the tool, defaults to the function name.
None
retries
int
| None
The number of retries to allow for this tool, defaults to the agent's default retries,
which defaults to 1.
None
prepare
ToolPrepareFunc
[
AgentDepsT
] | None
custom method to prepare the tool definition for each step, return
None
to omit this
tool from a given step. This is useful if you want to customise a tool at call time,
or omit it completely from a step. See
ToolPrepareFunc
.
None
docstring_format
DocstringFormat
The format of the docstring, see
DocstringFormat
.
Defaults to
'auto'
, such that the format is inferred from the structure of the docstring.
'auto'
require_parameter_descriptions
bool
If True, raise an error if a parameter description is missing. Defaults to False.
False
schema_generator
type
[
GenerateJsonSchema
]
The JSON schema generator class to use for this tool. Defaults to
GenerateToolJsonSchema
.
GenerateToolJsonSchema
strict
bool
| None
Whether to enforce JSON schema compliance (only affects OpenAI).
See
ToolDefinition
for more info.
None
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
1534
1535
1536
1537
1538
1539
1540
1541
1542
1543
1544
1545
1546
1547
1548
1549
1550
1551
1552
1553
1554
1555
1556
1557
1558
1559
1560
1561
1562
def
tool_plain
(
self
,
func
:
ToolFuncPlain
[
ToolParams
]
|
None
=
None
,
/
,
*
,
name
:
str
|
None
=
None
,
retries
:
int
|
None
=
None
,
prepare
:
ToolPrepareFunc
[
AgentDepsT
]
|
None
=
None
,
docstring_format
:
DocstringFormat
=
'auto'
,
require_parameter_descriptions
:
bool
=
False
,
schema_generator
:
type
[
GenerateJsonSchema
]
=
GenerateToolJsonSchema
,
strict
:
bool
|
None
=
None
,
)
->
Any
:
"""Decorator to register a tool function which DOES NOT take `RunContext` as an argument.
Can decorate a sync or async functions.
The docstring is inspected to extract both the tool description and description of each parameter,
[learn more](../tools.md#function-tools-and-schema).
We can't add overloads for every possible signature of tool, since the return type is a recursive union
so the signature of functions decorated with `@agent.tool` is obscured.
Example:
```python
from pydantic_ai import Agent, RunContext
agent = Agent('test')
@agent.tool
def foobar(ctx: RunContext[int]) -> int:
return 123
@agent.tool(retries=2)
async def spam(ctx: RunContext[str]) -> float:
return 3.14
result = agent.run_sync('foobar', deps=1)
print(result.output)
#> {"foobar":123,"spam":3.14}
```
Args:
func: The tool function to register.
name: The name of the tool, defaults to the function name.
retries: The number of retries to allow for this tool, defaults to the agent's default retries,
which defaults to 1.
prepare: custom method to prepare the tool definition for each step, return `None` to omit this
tool from a given step. This is useful if you want to customise a tool at call time,
or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].
docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].
Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.
require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.
schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.
strict: Whether to enforce JSON schema compliance (only affects OpenAI).
See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.
"""
if
func
is
None
:
def
tool_decorator
(
func_
:
ToolFuncPlain
[
ToolParams
])
->
ToolFuncPlain
[
ToolParams
]:
# noinspection PyTypeChecker
self
.
_register_function
(
func_
,
False
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func_
return
tool_decorator
else
:
self
.
_register_function
(
func
,
False
,
name
,
retries
,
prepare
,
docstring_format
,
require_parameter_descriptions
,
schema_generator
,
strict
,
)
return
func
is_model_request_node
staticmethod
is_model_request_node
(
node
:
AgentNode
[
T
,
S
]
|
End
[
FinalResult
[
S
]],
)
->
TypeIs
[
ModelRequestNode
[
T
,
S
]]
Check if the node is a
ModelRequestNode
, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to
isinstance
.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1694
1695
1696
1697
1698
1699
1700
1701
1702
@staticmethod
def
is_model_request_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
_agent_graph
.
ModelRequestNode
[
T
,
S
]]:
"""Check if the node is a `ModelRequestNode`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
_agent_graph
.
ModelRequestNode
)
is_call_tools_node
staticmethod
is_call_tools_node
(
node
:
AgentNode
[
T
,
S
]
|
End
[
FinalResult
[
S
]],
)
->
TypeIs
[
CallToolsNode
[
T
,
S
]]
Check if the node is a
CallToolsNode
, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to
isinstance
.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1704
1705
1706
1707
1708
1709
1710
1711
1712
@staticmethod
def
is_call_tools_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
_agent_graph
.
CallToolsNode
[
T
,
S
]]:
"""Check if the node is a `CallToolsNode`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
_agent_graph
.
CallToolsNode
)
is_user_prompt_node
staticmethod
is_user_prompt_node
(
node
:
AgentNode
[
T
,
S
]
|
End
[
FinalResult
[
S
]],
)
->
TypeIs
[
UserPromptNode
[
T
,
S
]]
Check if the node is a
UserPromptNode
, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to
isinstance
.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1714
1715
1716
1717
1718
1719
1720
1721
1722
@staticmethod
def
is_user_prompt_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
_agent_graph
.
UserPromptNode
[
T
,
S
]]:
"""Check if the node is a `UserPromptNode`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
_agent_graph
.
UserPromptNode
)
is_end_node
staticmethod
is_end_node
(
node
:
AgentNode
[
T
,
S
]
|
End
[
FinalResult
[
S
]],
)
->
TypeIs
[
End
[
FinalResult
[
S
]]]
Check if the node is a
End
, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to
isinstance
.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1724
1725
1726
1727
1728
1729
1730
1731
1732
@staticmethod
def
is_end_node
(
node
:
_agent_graph
.
AgentNode
[
T
,
S
]
|
End
[
result
.
FinalResult
[
S
]],
)
->
TypeIs
[
End
[
result
.
FinalResult
[
S
]]]:
"""Check if the node is a `End`, narrowing the type if it is.
This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
"""
return
isinstance
(
node
,
End
)
run_mcp_servers
async
run_mcp_servers
(
model
:
Model
|
KnownModelName
|
str
|
None
=
None
,
)
->
AsyncIterator
[
None
]
Run
MCPServerStdio
s
so they can be used by the agent.
Returns: a context manager to start and shutdown the servers.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1734
1735
1736
1737
1738
1739
1740
1741
1742
1743
1744
1745
1746
1747
1748
1749
1750
1751
1752
1753
1754
1755
@asynccontextmanager
async
def
run_mcp_servers
(
self
,
model
:
models
.
Model
|
models
.
KnownModelName
|
str
|
None
=
None
)
->
AsyncIterator
[
None
]:
"""Run [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] so they can be used by the agent.
Returns: a context manager to start and shutdown the servers.
"""
try
:
sampling_model
:
models
.
Model
|
None
=
self
.
_get_model
(
model
)
except
exceptions
.
UserError
:
# pragma: no cover
sampling_model
=
None
exit_stack
=
AsyncExitStack
()
try
:
for
mcp_server
in
self
.
_mcp_servers
:
if
sampling_model
is
not
None
:
# pragma: no branch
mcp_server
.
sampling_model
=
sampling_model
await
exit_stack
.
enter_async_context
(
mcp_server
)
yield
finally
:
await
exit_stack
.
aclose
()
to_a2a
to_a2a
(
*
,
storage
:
Storage
|
None
=
None
,
broker
:
Broker
|
None
=
None
,
name
:
str
|
None
=
None
,
url
:
str
=
"http://localhost:8000"
,
version
:
str
=
"1.0.0"
,
description
:
str
|
None
=
None
,
provider
:
Provider
|
None
=
None
,
skills
:
list
[
Skill
]
|
None
=
None
,
debug
:
bool
=
False
,
routes
:
Sequence
[
Route
]
|
None
=
None
,
middleware
:
Sequence
[
Middleware
]
|
None
=
None
,
exception_handlers
:
(
dict
[
Any
,
ExceptionHandler
]
|
None
)
=
None
,
lifespan
:
Lifespan
[
FastA2A
]
|
None
=
None
)
->
FastA2A
Convert the agent to a FastA2A application.
Example:
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
)
app
=
agent
.
to_a2a
()
The
app
is an ASGI application that can be used with any ASGI server.
To run the application, you can use the following command:
uvicorn
app:app
--host
0
.0.0.0
--port
8000
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1757
1758
1759
1760
1761
1762
1763
1764
1765
1766
1767
1768
1769
1770
1771
1772
1773
1774
1775
1776
1777
1778
1779
1780
1781
1782
1783
1784
1785
1786
1787
1788
1789
1790
1791
1792
1793
1794
1795
1796
1797
1798
1799
1800
1801
1802
1803
1804
1805
1806
1807
1808
1809
1810
1811
def
to_a2a
(
self
,
*
,
storage
:
Storage
|
None
=
None
,
broker
:
Broker
|
None
=
None
,
# Agent card
name
:
str
|
None
=
None
,
url
:
str
=
'http://localhost:8000'
,
version
:
str
=
'1.0.0'
,
description
:
str
|
None
=
None
,
provider
:
Provider
|
None
=
None
,
skills
:
list
[
Skill
]
|
None
=
None
,
# Starlette
debug
:
bool
=
False
,
routes
:
Sequence
[
Route
]
|
None
=
None
,
middleware
:
Sequence
[
Middleware
]
|
None
=
None
,
exception_handlers
:
dict
[
Any
,
ExceptionHandler
]
|
None
=
None
,
lifespan
:
Lifespan
[
FastA2A
]
|
None
=
None
,
)
->
FastA2A
:
"""Convert the agent to a FastA2A application.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
app = agent.to_a2a()
```
The `app` is an ASGI application that can be used with any ASGI server.
To run the application, you can use the following command:
```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```
"""
from
._a2a
import
agent_to_a2a
return
agent_to_a2a
(
self
,
storage
=
storage
,
broker
=
broker
,
name
=
name
,
url
=
url
,
version
=
version
,
description
=
description
,
provider
=
provider
,
skills
=
skills
,
debug
=
debug
,
routes
=
routes
,
middleware
=
middleware
,
exception_handlers
=
exception_handlers
,
lifespan
=
lifespan
,
)
to_cli
async
to_cli
(
deps
:
AgentDepsT
=
None
,
prog_name
:
str
=
"pydantic-ai"
)
->
None
Run the agent in a CLI chat interface.
Parameters:
Name
Type
Description
Default
deps
AgentDepsT
The dependencies to pass to the agent.
None
prog_name
str
The name of the program to use for the CLI. Defaults to 'pydantic-ai'.
'pydantic-ai'
Example:
agent_to_cli.py
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
,
instructions
=
'You always respond in Italian.'
)
async
def
main
():
await
agent
.
to_cli
()
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1813
1814
1815
1816
1817
1818
1819
1820
1821
1822
1823
1824
1825
1826
1827
1828
1829
1830
1831
1832
1833
1834
async
def
to_cli
(
self
:
Self
,
deps
:
AgentDepsT
=
None
,
prog_name
:
str
=
'pydantic-ai'
)
->
None
:
"""Run the agent in a CLI chat interface.
Args:
deps: The dependencies to pass to the agent.
prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.
Example:
```python {title="agent_to_cli.py" test="skip"}
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')
async def main():
await agent.to_cli()
```
"""
from
rich.console
import
Console
from
pydantic_ai._cli
import
run_chat
await
run_chat
(
stream
=
True
,
agent
=
self
,
deps
=
deps
,
console
=
Console
(),
code_theme
=
'monokai'
,
prog_name
=
prog_name
)
to_cli_sync
to_cli_sync
(
deps
:
AgentDepsT
=
None
,
prog_name
:
str
=
"pydantic-ai"
)
->
None
Run the agent in a CLI chat interface with the non-async interface.
Parameters:
Name
Type
Description
Default
deps
AgentDepsT
The dependencies to pass to the agent.
None
prog_name
str
The name of the program to use for the CLI. Defaults to 'pydantic-ai'.
'pydantic-ai'
agent_to_cli_sync.py
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
,
instructions
=
'You always respond in Italian.'
)
agent
.
to_cli_sync
()
agent
.
to_cli_sync
(
prog_name
=
'assistant'
)
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1836
1837
1838
1839
1840
1841
1842
1843
1844
1845
1846
1847
1848
1849
1850
1851
def
to_cli_sync
(
self
:
Self
,
deps
:
AgentDepsT
=
None
,
prog_name
:
str
=
'pydantic-ai'
)
->
None
:
"""Run the agent in a CLI chat interface with the non-async interface.
Args:
deps: The dependencies to pass to the agent.
prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.
```python {title="agent_to_cli_sync.py" test="skip"}
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')
agent.to_cli_sync()
agent.to_cli_sync(prog_name='assistant')
```
"""
return
get_event_loop
()
.
run_until_complete
(
self
.
to_cli
(
deps
=
deps
,
prog_name
=
prog_name
))
AgentRun
dataclass
Bases:
Generic
[
AgentDepsT
,
OutputDataT
]
A stateful, async-iterable run of an
Agent
.
You generally obtain an
AgentRun
instance by calling
async with my_agent.iter(...) as agent_run:
.
Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an
End
is reached, the run finishes and
result
becomes available.
Example:
from
pydantic_ai
import
Agent
agent
=
Agent
(
'openai:gpt-4o'
)
async
def
main
():
nodes
=
[]
# Iterate through the run, recording each node along the way:
async
with
agent
.
iter
(
'What is the capital of France?'
)
as
agent_run
:
async
for
node
in
agent_run
:
nodes
.
append
(
node
)
print
(
nodes
)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1, request_tokens=56, response_tokens=1, total_tokens=57
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print
(
agent_run
.
result
.
output
)
#> Paris
You can also manually drive the iteration using the
next
method for
more granular control.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1854
1855
1856
1857
1858
1859
1860
1861
1862
1863
1864
1865
1866
1867
1868
1869
1870
1871
1872
1873
1874
1875
1876
1877
1878
1879
1880
1881
1882
1883
1884
1885
1886
1887
1888
1889
1890
1891
1892
1893
1894
1895
1896
1897
1898
1899
1900
1901
1902
1903
1904
1905
1906
1907
1908
1909
1910
1911
1912
1913
1914
1915
1916
1917
1918
1919
1920
1921
1922
1923
1924
1925
1926
1927
1928
1929
1930
1931
1932
1933
1934
1935
1936
1937
1938
1939
1940
1941
1942
1943
1944
1945
1946
1947
1948
1949
1950
1951
1952
1953
1954
1955
1956
1957
1958
1959
1960
1961
1962
1963
1964
1965
1966
1967
1968
1969
1970
1971
1972
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
2025
2026
2027
2028
2029
2030
2031
2032
2033
2034
2035
2036
2037
2038
2039
2040
2041
2042
2043
2044
2045
2046
2047
2048
2049
2050
2051
2052
2053
2054
2055
2056
2057
2058
2059
2060
2061
2062
2063
2064
2065
2066
2067
2068
2069
2070
2071
2072
2073
2074
2075
2076
@dataclasses
.
dataclass
(
repr
=
False
)
class
AgentRun
(
Generic
[
AgentDepsT
,
OutputDataT
]):
"""A stateful, async-iterable run of an [`Agent`][pydantic_ai.agent.Agent].
You generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.
Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an
[`End`][pydantic_graph.nodes.End] is reached, the run finishes and [`result`][pydantic_ai.agent.AgentRun.result]
becomes available.
Example:
```python
from pydantic_ai import Agent
agent = Agent('openai:gpt-4o')
async def main():
nodes = []
# Iterate through the run, recording each node along the way:
async with agent.iter('What is the capital of France?') as agent_run:
async for node in agent_run:
nodes.append(node)
print(nodes)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1, request_tokens=56, response_tokens=1, total_tokens=57
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print(agent_run.result.output)
#> Paris
```
You can also manually drive the iteration using the [`next`][pydantic_ai.agent.AgentRun.next] method for
more granular control.
"""
_graph_run
:
GraphRun
[
_agent_graph
.
GraphAgentState
,
_agent_graph
.
GraphAgentDeps
[
AgentDepsT
,
Any
],
FinalResult
[
OutputDataT
]
]
@overload
def
_traceparent
(
self
,
*
,
required
:
Literal
[
False
])
->
str
|
None
:
...
@overload
def
_traceparent
(
self
)
->
str
:
...
def
_traceparent
(
self
,
*
,
required
:
bool
=
True
)
->
str
|
None
:
traceparent
=
self
.
_graph_run
.
_traceparent
(
required
=
False
)
# type: ignore[reportPrivateUsage]
if
traceparent
is
None
and
required
:
# pragma: no cover
raise
AttributeError
(
'No span was created for this agent run'
)
return
traceparent
@property
def
ctx
(
self
)
->
GraphRunContext
[
_agent_graph
.
GraphAgentState
,
_agent_graph
.
GraphAgentDeps
[
AgentDepsT
,
Any
]]:
"""The current context of the agent run."""
return
GraphRunContext
[
_agent_graph
.
GraphAgentState
,
_agent_graph
.
GraphAgentDeps
[
AgentDepsT
,
Any
]](
self
.
_graph_run
.
state
,
self
.
_graph_run
.
deps
)
@property
def
next_node
(
self
,
)
->
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]:
"""The next node that will be run in the agent graph.
This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.
"""
next_node
=
self
.
_graph_run
.
next_node
if
isinstance
(
next_node
,
End
):
return
next_node
if
_agent_graph
.
is_agent_node
(
next_node
):
return
next_node
raise
exceptions
.
AgentRunError
(
f
'Unexpected node type:
{
type
(
next_node
)
}
'
)
# pragma: no cover
@property
def
result
(
self
)
->
AgentRunResult
[
OutputDataT
]
|
None
:
"""The final result of the run if it has ended, otherwise `None`.
Once the run returns an [`End`][pydantic_graph.nodes.End] node, `result` is populated
with an [`AgentRunResult`][pydantic_ai.agent.AgentRunResult].
"""
graph_run_result
=
self
.
_graph_run
.
result
if
graph_run_result
is
None
:
return
None
return
AgentRunResult
(
graph_run_result
.
output
.
output
,
graph_run_result
.
output
.
tool_name
,
graph_run_result
.
state
,
self
.
_graph_run
.
deps
.
new_message_index
,
self
.
_traceparent
(
required
=
False
),
)
def
__aiter__
(
self
,
)
->
AsyncIterator
[
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]]:
"""Provide async-iteration over the nodes in the agent run."""
return
self
async
def
__anext__
(
self
,
)
->
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]:
"""Advance to the next node automatically based on the last returned node."""
next_node
=
await
self
.
_graph_run
.
__anext__
()
if
_agent_graph
.
is_agent_node
(
next_node
):
return
next_node
assert
isinstance
(
next_node
,
End
),
f
'Unexpected node type:
{
type
(
next_node
)
}
'
return
next_node
async
def
next
(
self
,
node
:
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
],
)
->
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]:
"""Manually drive the agent run by passing in the node you want to run next.
This lets you inspect or mutate the node before continuing execution, or skip certain nodes
under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]
node.
Example:
```python
from pydantic_ai import Agent
from pydantic_graph import End
agent = Agent('openai:gpt-4o')
async def main():
async with agent.iter('What is the capital of France?') as agent_run:
next_node = agent_run.next_node  # start with the first node
nodes = [next_node]
while not isinstance(next_node, End):
next_node = await agent_run.next(next_node)
nodes.append(next_node)
# Once `next_node` is an End, we've finished:
print(nodes)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1,
request_tokens=56,
response_tokens=1,
total_tokens=57,
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print('Final result:', agent_run.result.output)
#> Final result: Paris
```
Args:
node: The node to run next in the graph.
Returns:
The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if
the run has completed.
"""
# Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it
# on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.
next_node
=
await
self
.
_graph_run
.
next
(
node
)
if
_agent_graph
.
is_agent_node
(
next_node
):
return
next_node
assert
isinstance
(
next_node
,
End
),
f
'Unexpected node type:
{
type
(
next_node
)
}
'
return
next_node
def
usage
(
self
)
->
_usage
.
Usage
:
"""Get usage statistics for the run so far, including token usage, model requests, and so on."""
return
self
.
_graph_run
.
state
.
usage
def
__repr__
(
self
)
->
str
:
# pragma: no cover
result
=
self
.
_graph_run
.
result
result_repr
=
'<run not finished>'
if
result
is
None
else
repr
(
result
.
output
)
return
f
'<
{
type
(
self
)
.
__name__
}
result=
{
result_repr
}
usage=
{
self
.
usage
()
}
>'
ctx
property
ctx
:
GraphRunContext
[
GraphAgentState
,
GraphAgentDeps
[
AgentDepsT
,
Any
]
]
The current context of the agent run.
next_node
property
next_node
:
(
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]
)
The next node that will be run in the agent graph.
This is the next node that will be used during async iteration, or if a node is not passed to
self.next(...)
.
result
property
result
:
AgentRunResult
[
OutputDataT
]
|
None
The final result of the run if it has ended, otherwise
None
.
Once the run returns an
End
node,
result
is populated
with an
AgentRunResult
.
__aiter__
__aiter__
()
->
(
AsyncIterator
[
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]
]
)
Provide async-iteration over the nodes in the agent run.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1972
1973
1974
1975
1976
def
__aiter__
(
self
,
)
->
AsyncIterator
[
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]]:
"""Provide async-iteration over the nodes in the agent run."""
return
self
__anext__
async
__anext__
()
->
(
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]
)
Advance to the next node automatically based on the last returned node.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1978
1979
1980
1981
1982
1983
1984
1985
1986
async
def
__anext__
(
self
,
)
->
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]:
"""Advance to the next node automatically based on the last returned node."""
next_node
=
await
self
.
_graph_run
.
__anext__
()
if
_agent_graph
.
is_agent_node
(
next_node
):
return
next_node
assert
isinstance
(
next_node
,
End
),
f
'Unexpected node type:
{
type
(
next_node
)
}
'
return
next_node
next
async
next
(
node
:
AgentNode
[
AgentDepsT
,
OutputDataT
],
)
->
(
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]
)
Manually drive the agent run by passing in the node you want to run next.
This lets you inspect or mutate the node before continuing execution, or skip certain nodes
under dynamic conditions. The agent run should be stopped when you return an
End
node.
Example:
from
pydantic_ai
import
Agent
from
pydantic_graph
import
End
agent
=
Agent
(
'openai:gpt-4o'
)
async
def
main
():
async
with
agent
.
iter
(
'What is the capital of France?'
)
as
agent_run
:
next_node
=
agent_run
.
next_node
# start with the first node
nodes
=
[
next_node
]
while
not
isinstance
(
next_node
,
End
):
next_node
=
await
agent_run
.
next
(
next_node
)
nodes
.
append
(
next_node
)
# Once `next_node` is an End, we've finished:
print
(
nodes
)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1,
request_tokens=56,
response_tokens=1,
total_tokens=57,
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print
(
'Final result:'
,
agent_run
.
result
.
output
)
#> Final result: Paris
Parameters:
Name
Type
Description
Default
node
AgentNode
[
AgentDepsT
,
OutputDataT
]
The node to run next in the graph.
required
Returns:
Type
Description
AgentNode
[
AgentDepsT
,
OutputDataT
] |
End
[
FinalResult
[
OutputDataT
]]
The next node returned by the graph logic, or an
End
node if
AgentNode
[
AgentDepsT
,
OutputDataT
] |
End
[
FinalResult
[
OutputDataT
]]
the run has completed.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
2025
2026
2027
2028
2029
2030
2031
2032
2033
2034
2035
2036
2037
2038
2039
2040
2041
2042
2043
2044
2045
2046
2047
2048
2049
2050
2051
2052
2053
2054
2055
2056
2057
2058
2059
2060
2061
2062
2063
2064
2065
2066
2067
async
def
next
(
self
,
node
:
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
],
)
->
_agent_graph
.
AgentNode
[
AgentDepsT
,
OutputDataT
]
|
End
[
FinalResult
[
OutputDataT
]]:
"""Manually drive the agent run by passing in the node you want to run next.
This lets you inspect or mutate the node before continuing execution, or skip certain nodes
under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]
node.
Example:
```python
from pydantic_ai import Agent
from pydantic_graph import End
agent = Agent('openai:gpt-4o')
async def main():
async with agent.iter('What is the capital of France?') as agent_run:
next_node = agent_run.next_node  # start with the first node
nodes = [next_node]
while not isinstance(next_node, End):
next_node = await agent_run.next(next_node)
nodes.append(next_node)
# Once `next_node` is an End, we've finished:
print(nodes)
'''
[
UserPromptNode(
user_prompt='What is the capital of France?',
instructions=None,
instructions_functions=[],
system_prompts=(),
system_prompt_functions=[],
system_prompt_dynamic_functions={},
),
ModelRequestNode(
request=ModelRequest(
parts=[
UserPromptPart(
content='What is the capital of France?',
timestamp=datetime.datetime(...),
)
]
)
),
CallToolsNode(
model_response=ModelResponse(
parts=[TextPart(content='Paris')],
usage=Usage(
requests=1,
request_tokens=56,
response_tokens=1,
total_tokens=57,
),
model_name='gpt-4o',
timestamp=datetime.datetime(...),
)
),
End(data=FinalResult(output='Paris')),
]
'''
print('Final result:', agent_run.result.output)
#> Final result: Paris
```
Args:
node: The node to run next in the graph.
Returns:
The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if
the run has completed.
"""
# Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it
# on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.
next_node
=
await
self
.
_graph_run
.
next
(
node
)
if
_agent_graph
.
is_agent_node
(
next_node
):
return
next_node
assert
isinstance
(
next_node
,
End
),
f
'Unexpected node type:
{
type
(
next_node
)
}
'
return
next_node
usage
usage
()
->
Usage
Get usage statistics for the run so far, including token usage, model requests, and so on.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
2069
2070
2071
def
usage
(
self
)
->
_usage
.
Usage
:
"""Get usage statistics for the run so far, including token usage, model requests, and so on."""
return
self
.
_graph_run
.
state
.
usage
AgentRunResult
dataclass
Bases:
Generic
[
OutputDataT
]
The final result of an agent run.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
2079
2080
2081
2082
2083
2084
2085
2086
2087
2088
2089
2090
2091
2092
2093
2094
2095
2096
2097
2098
2099
2100
2101
2102
2103
2104
2105
2106
2107
2108
2109
2110
2111
2112
2113
2114
2115
2116
2117
2118
2119
2120
2121
2122
2123
2124
2125
2126
2127
2128
2129
2130
2131
2132
2133
2134
2135
2136
2137
2138
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
2154
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
2169
2170
2171
2172
2173
2174
2175
2176
2177
2178
2179
2180
2181
2182
2183
2184
2185
2186
2187
2188
2189
2190
2191
2192
2193
2194
2195
2196
2197
2198
2199
2200
2201
2202
2203
2204
2205
2206
2207
2208
2209
2210
2211
2212
2213
2214
2215
2216
2217
2218
2219
2220
2221
2222
2223
2224
2225
2226
2227
@dataclasses
.
dataclass
class
AgentRunResult
(
Generic
[
OutputDataT
]):
"""The final result of an agent run."""
output
:
OutputDataT
"""The output data from the agent run."""
_output_tool_name
:
str
|
None
=
dataclasses
.
field
(
repr
=
False
)
_state
:
_agent_graph
.
GraphAgentState
=
dataclasses
.
field
(
repr
=
False
)
_new_message_index
:
int
=
dataclasses
.
field
(
repr
=
False
)
_traceparent_value
:
str
|
None
=
dataclasses
.
field
(
repr
=
False
)
@overload
def
_traceparent
(
self
,
*
,
required
:
Literal
[
False
])
->
str
|
None
:
...
@overload
def
_traceparent
(
self
)
->
str
:
...
def
_traceparent
(
self
,
*
,
required
:
bool
=
True
)
->
str
|
None
:
if
self
.
_traceparent_value
is
None
and
required
:
# pragma: no cover
raise
AttributeError
(
'No span was created for this agent run'
)
return
self
.
_traceparent_value
@property
@deprecated
(
'`result.data` is deprecated, use `result.output` instead.'
)
def
data
(
self
)
->
OutputDataT
:
return
self
.
output
def
_set_output_tool_return
(
self
,
return_content
:
str
)
->
list
[
_messages
.
ModelMessage
]:
"""Set return content for the output tool.
Useful if you want to continue the conversation and want to set the response to the output tool call.
"""
if
not
self
.
_output_tool_name
:
raise
ValueError
(
'Cannot set output tool return content when the return type is `str`.'
)
messages
=
deepcopy
(
self
.
_state
.
message_history
)
last_message
=
messages
[
-
1
]
for
part
in
last_message
.
parts
:
if
isinstance
(
part
,
_messages
.
ToolReturnPart
)
and
part
.
tool_name
==
self
.
_output_tool_name
:
part
.
content
=
return_content
return
messages
raise
LookupError
(
f
'No tool call found with tool name
{
self
.
_output_tool_name
!r}
.'
)
@overload
def
all_messages
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
...
@overload
@deprecated
(
'`result_tool_return_content` is deprecated, use `output_tool_return_content` instead.'
)
def
all_messages
(
self
,
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
...
def
all_messages
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
"""Return the history of _messages.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
List of messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
if
content
is
not
None
:
return
self
.
_set_output_tool_return
(
content
)
else
:
return
self
.
_state
.
message_history
@overload
def
all_messages_json
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
...
@overload
@deprecated
(
'`result_tool_return_content` is deprecated, use `output_tool_return_content` instead.'
)
def
all_messages_json
(
self
,
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
...
def
all_messages_json
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
"""Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
JSON bytes representing the messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
return
_messages
.
ModelMessagesTypeAdapter
.
dump_json
(
self
.
all_messages
(
output_tool_return_content
=
content
))
@overload
def
new_messages
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
...
@overload
@deprecated
(
'`result_tool_return_content` is deprecated, use `output_tool_return_content` instead.'
)
def
new_messages
(
self
,
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
...
def
new_messages
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
"""Return new messages associated with this run.
Messages from older runs are excluded.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
List of new messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
return
self
.
all_messages
(
output_tool_return_content
=
content
)[
self
.
_new_message_index
:]
@overload
def
new_messages_json
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
...
@overload
@deprecated
(
'`result_tool_return_content` is deprecated, use `output_tool_return_content` instead.'
)
def
new_messages_json
(
self
,
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
...
def
new_messages_json
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
"""Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
JSON bytes representing the new messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
return
_messages
.
ModelMessagesTypeAdapter
.
dump_json
(
self
.
new_messages
(
output_tool_return_content
=
content
))
def
usage
(
self
)
->
_usage
.
Usage
:
"""Return the usage of the whole run."""
return
self
.
_state
.
usage
output
instance-attribute
output
:
OutputDataT
The output data from the agent run.
all_messages
all_messages
(
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
list
[
ModelMessage
]
all_messages
(
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
ModelMessage
]
all_messages
(
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
ModelMessage
]
Return the history of _messages.
Parameters:
Name
Type
Description
Default
output_tool_return_content
str
| None
The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If
None
, the last message will
not be modified.
None
result_tool_return_content
str
| None
Deprecated, use
output_tool_return_content
instead.
None
Returns:
Type
Description
list
[
ModelMessage
]
List of messages.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
2127
2128
2129
2130
2131
2132
2133
2134
2135
2136
2137
2138
2139
2140
2141
2142
2143
2144
2145
2146
def
all_messages
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
"""Return the history of _messages.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
List of messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
if
content
is
not
None
:
return
self
.
_set_output_tool_return
(
content
)
else
:
return
self
.
_state
.
message_history
all_messages_json
all_messages_json
(
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
bytes
all_messages_json
(
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
all_messages_json
(
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
Return all messages from
all_messages
as JSON bytes.
Parameters:
Name
Type
Description
Default
output_tool_return_content
str
| None
The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If
None
, the last message will
not be modified.
None
result_tool_return_content
str
| None
Deprecated, use
output_tool_return_content
instead.
None
Returns:
Type
Description
bytes
JSON bytes representing the messages.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
2169
2170
2171
def
all_messages_json
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
"""Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
JSON bytes representing the messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
return
_messages
.
ModelMessagesTypeAdapter
.
dump_json
(
self
.
all_messages
(
output_tool_return_content
=
content
))
new_messages
new_messages
(
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
list
[
ModelMessage
]
new_messages
(
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
ModelMessage
]
new_messages
(
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
ModelMessage
]
Return new messages associated with this run.
Messages from older runs are excluded.
Parameters:
Name
Type
Description
Default
output_tool_return_content
str
| None
The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If
None
, the last message will
not be modified.
None
result_tool_return_content
str
| None
Deprecated, use
output_tool_return_content
instead.
None
Returns:
Type
Description
list
[
ModelMessage
]
List of new messages.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
2180
2181
2182
2183
2184
2185
2186
2187
2188
2189
2190
2191
2192
2193
2194
2195
2196
2197
2198
def
new_messages
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
list
[
_messages
.
ModelMessage
]:
"""Return new messages associated with this run.
Messages from older runs are excluded.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
List of new messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
return
self
.
all_messages
(
output_tool_return_content
=
content
)[
self
.
_new_message_index
:]
new_messages_json
new_messages_json
(
*
,
output_tool_return_content
:
str
|
None
=
None
)
->
bytes
new_messages_json
(
*
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
new_messages_json
(
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
Return new messages from
new_messages
as JSON bytes.
Parameters:
Name
Type
Description
Default
output_tool_return_content
str
| None
The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If
None
, the last message will
not be modified.
None
result_tool_return_content
str
| None
Deprecated, use
output_tool_return_content
instead.
None
Returns:
Type
Description
bytes
JSON bytes representing the new messages.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
2207
2208
2209
2210
2211
2212
2213
2214
2215
2216
2217
2218
2219
2220
2221
2222
2223
def
new_messages_json
(
self
,
*
,
output_tool_return_content
:
str
|
None
=
None
,
result_tool_return_content
:
str
|
None
=
None
)
->
bytes
:
"""Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.
Args:
output_tool_return_content: The return content of the tool call to set in the last message.
This provides a convenient way to modify the content of the output tool call if you want to continue
the conversation and want to set the response to the output tool call. If `None`, the last message will
not be modified.
result_tool_return_content: Deprecated, use `output_tool_return_content` instead.
Returns:
JSON bytes representing the new messages.
"""
content
=
result
.
coalesce_deprecated_return_content
(
output_tool_return_content
,
result_tool_return_content
)
return
_messages
.
ModelMessagesTypeAdapter
.
dump_json
(
self
.
new_messages
(
output_tool_return_content
=
content
))
usage
usage
()
->
Usage
Return the usage of the whole run.
Source code in
pydantic_ai_slim/pydantic_ai/agent.py
2225
2226
2227
def
usage
(
self
)
->
_usage
.
Usage
:
"""Return the usage of the whole run."""
return
self
.
_state
.
usage
EndStrategy
module-attribute
EndStrategy
=
EndStrategy
RunOutputDataT
module-attribute
RunOutputDataT
=
TypeVar
(
'RunOutputDataT'
)
Type variable for the result data of a run where
output_type
was customized on the run call.
capture_run_messages
module-attribute
capture_run_messages
=
capture_run_messages
InstrumentationSettings
dataclass
Options for instrumenting models and agents with OpenTelemetry.
Used in:
Agent(instrument=...)
Agent.instrument_all()
InstrumentedModel
See the
Debugging and Monitoring guide
for more info.
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
@dataclass
(
init
=
False
)
class
InstrumentationSettings
:
"""Options for instrumenting models and agents with OpenTelemetry.
Used in:
- `Agent(instrument=...)`
- [`Agent.instrument_all()`][pydantic_ai.agent.Agent.instrument_all]
- [`InstrumentedModel`][pydantic_ai.models.instrumented.InstrumentedModel]
See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
"""
tracer
:
Tracer
=
field
(
repr
=
False
)
event_logger
:
EventLogger
=
field
(
repr
=
False
)
event_mode
:
Literal
[
'attributes'
,
'logs'
]
=
'attributes'
include_binary_content
:
bool
=
True
def
__init__
(
self
,
*
,
event_mode
:
Literal
[
'attributes'
,
'logs'
]
=
'attributes'
,
tracer_provider
:
TracerProvider
|
None
=
None
,
meter_provider
:
MeterProvider
|
None
=
None
,
event_logger_provider
:
EventLoggerProvider
|
None
=
None
,
include_binary_content
:
bool
=
True
,
include_content
:
bool
=
True
,
):
"""Create instrumentation options.
Args:
event_mode: The mode for emitting events. If `'attributes'`, events are attached to the span as attributes.
If `'logs'`, events are emitted as OpenTelemetry log-based events.
tracer_provider: The OpenTelemetry tracer provider to use.
If not provided, the global tracer provider is used.
Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.
meter_provider: The OpenTelemetry meter provider to use.
If not provided, the global meter provider is used.
Calling `logfire.configure()` sets the global meter provider, so most users don't need this.
event_logger_provider: The OpenTelemetry event logger provider to use.
If not provided, the global event logger provider is used.
Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.
This is only used if `event_mode='logs'`.
include_binary_content: Whether to include binary content in the instrumentation events.
include_content: Whether to include prompts, completions, and tool call arguments and responses
in the instrumentation events.
"""
from
pydantic_ai
import
__version__
tracer_provider
=
tracer_provider
or
get_tracer_provider
()
meter_provider
=
meter_provider
or
get_meter_provider
()
event_logger_provider
=
event_logger_provider
or
get_event_logger_provider
()
scope_name
=
'pydantic-ai'
self
.
tracer
=
tracer_provider
.
get_tracer
(
scope_name
,
__version__
)
self
.
meter
=
meter_provider
.
get_meter
(
scope_name
,
__version__
)
self
.
event_logger
=
event_logger_provider
.
get_event_logger
(
scope_name
,
__version__
)
self
.
event_mode
=
event_mode
self
.
include_binary_content
=
include_binary_content
self
.
include_content
=
include_content
# As specified in the OpenTelemetry GenAI metrics spec:
# https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage
tokens_histogram_kwargs
=
dict
(
name
=
'gen_ai.client.token.usage'
,
unit
=
'
{token}
'
,
description
=
'Measures number of input and output tokens used'
,
)
try
:
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
explicit_bucket_boundaries_advisory
=
TOKEN_HISTOGRAM_BOUNDARIES
,
)
except
TypeError
:
# pragma: lax no cover
# Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
# pyright: ignore
)
def
messages_to_otel_events
(
self
,
messages
:
list
[
ModelMessage
])
->
list
[
Event
]:
"""Convert a list of model messages to OpenTelemetry events.
Args:
messages: The messages to convert.
Returns:
A list of OpenTelemetry events.
"""
events
:
list
[
Event
]
=
[]
instructions
=
InstrumentedModel
.
_get_instructions
(
messages
)
# pyright: ignore [reportPrivateUsage]
if
instructions
is
not
None
:
events
.
append
(
Event
(
'gen_ai.system.message'
,
body
=
{
'content'
:
instructions
,
'role'
:
'system'
}))
for
message_index
,
message
in
enumerate
(
messages
):
message_events
:
list
[
Event
]
=
[]
if
isinstance
(
message
,
ModelRequest
):
for
part
in
message
.
parts
:
if
hasattr
(
part
,
'otel_event'
):
message_events
.
append
(
part
.
otel_event
(
self
))
elif
isinstance
(
message
,
ModelResponse
):
# pragma: no branch
message_events
=
message
.
otel_events
(
self
)
for
event
in
message_events
:
event
.
attributes
=
{
'gen_ai.message.index'
:
message_index
,
**
(
event
.
attributes
or
{}),
}
events
.
extend
(
message_events
)
for
event
in
events
:
event
.
body
=
InstrumentedModel
.
serialize_any
(
event
.
body
)
return
events
__init__
__init__
(
*
,
event_mode
:
Literal
[
"attributes"
,
"logs"
]
=
"attributes"
,
tracer_provider
:
TracerProvider
|
None
=
None
,
meter_provider
:
MeterProvider
|
None
=
None
,
event_logger_provider
:
(
EventLoggerProvider
|
None
)
=
None
,
include_binary_content
:
bool
=
True
,
include_content
:
bool
=
True
)
Create instrumentation options.
Parameters:
Name
Type
Description
Default
event_mode
Literal
['attributes', 'logs']
The mode for emitting events. If
'attributes'
, events are attached to the span as attributes.
If
'logs'
, events are emitted as OpenTelemetry log-based events.
'attributes'
tracer_provider
TracerProvider
| None
The OpenTelemetry tracer provider to use.
If not provided, the global tracer provider is used.
Calling
logfire.configure()
sets the global tracer provider, so most users don't need this.
None
meter_provider
MeterProvider
| None
The OpenTelemetry meter provider to use.
If not provided, the global meter provider is used.
Calling
logfire.configure()
sets the global meter provider, so most users don't need this.
None
event_logger_provider
EventLoggerProvider
| None
The OpenTelemetry event logger provider to use.
If not provided, the global event logger provider is used.
Calling
logfire.configure()
sets the global event logger provider, so most users don't need this.
This is only used if
event_mode='logs'
.
None
include_binary_content
bool
Whether to include binary content in the instrumentation events.
True
include_content
bool
Whether to include prompts, completions, and tool call arguments and responses
in the instrumentation events.
True
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
def
__init__
(
self
,
*
,
event_mode
:
Literal
[
'attributes'
,
'logs'
]
=
'attributes'
,
tracer_provider
:
TracerProvider
|
None
=
None
,
meter_provider
:
MeterProvider
|
None
=
None
,
event_logger_provider
:
EventLoggerProvider
|
None
=
None
,
include_binary_content
:
bool
=
True
,
include_content
:
bool
=
True
,
):
"""Create instrumentation options.
Args:
event_mode: The mode for emitting events. If `'attributes'`, events are attached to the span as attributes.
If `'logs'`, events are emitted as OpenTelemetry log-based events.
tracer_provider: The OpenTelemetry tracer provider to use.
If not provided, the global tracer provider is used.
Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.
meter_provider: The OpenTelemetry meter provider to use.
If not provided, the global meter provider is used.
Calling `logfire.configure()` sets the global meter provider, so most users don't need this.
event_logger_provider: The OpenTelemetry event logger provider to use.
If not provided, the global event logger provider is used.
Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.
This is only used if `event_mode='logs'`.
include_binary_content: Whether to include binary content in the instrumentation events.
include_content: Whether to include prompts, completions, and tool call arguments and responses
in the instrumentation events.
"""
from
pydantic_ai
import
__version__
tracer_provider
=
tracer_provider
or
get_tracer_provider
()
meter_provider
=
meter_provider
or
get_meter_provider
()
event_logger_provider
=
event_logger_provider
or
get_event_logger_provider
()
scope_name
=
'pydantic-ai'
self
.
tracer
=
tracer_provider
.
get_tracer
(
scope_name
,
__version__
)
self
.
meter
=
meter_provider
.
get_meter
(
scope_name
,
__version__
)
self
.
event_logger
=
event_logger_provider
.
get_event_logger
(
scope_name
,
__version__
)
self
.
event_mode
=
event_mode
self
.
include_binary_content
=
include_binary_content
self
.
include_content
=
include_content
# As specified in the OpenTelemetry GenAI metrics spec:
# https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage
tokens_histogram_kwargs
=
dict
(
name
=
'gen_ai.client.token.usage'
,
unit
=
'
{token}
'
,
description
=
'Measures number of input and output tokens used'
,
)
try
:
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
explicit_bucket_boundaries_advisory
=
TOKEN_HISTOGRAM_BOUNDARIES
,
)
except
TypeError
:
# pragma: lax no cover
# Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
# pyright: ignore
)
messages_to_otel_events
messages_to_otel_events
(
messages
:
list
[
ModelMessage
],
)
->
list
[
Event
]
Convert a list of model messages to OpenTelemetry events.
Parameters:
Name
Type
Description
Default
messages
list
[
ModelMessage
]
The messages to convert.
required
Returns:
Type
Description
list
[
Event
]
A list of OpenTelemetry events.
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
def
messages_to_otel_events
(
self
,
messages
:
list
[
ModelMessage
])
->
list
[
Event
]:
"""Convert a list of model messages to OpenTelemetry events.
Args:
messages: The messages to convert.
Returns:
A list of OpenTelemetry events.
"""
events
:
list
[
Event
]
=
[]
instructions
=
InstrumentedModel
.
_get_instructions
(
messages
)
# pyright: ignore [reportPrivateUsage]
if
instructions
is
not
None
:
events
.
append
(
Event
(
'gen_ai.system.message'
,
body
=
{
'content'
:
instructions
,
'role'
:
'system'
}))
for
message_index
,
message
in
enumerate
(
messages
):
message_events
:
list
[
Event
]
=
[]
if
isinstance
(
message
,
ModelRequest
):
for
part
in
message
.
parts
:
if
hasattr
(
part
,
'otel_event'
):
message_events
.
append
(
part
.
otel_event
(
self
))
elif
isinstance
(
message
,
ModelResponse
):
# pragma: no branch
message_events
=
message
.
otel_events
(
self
)
for
event
in
message_events
:
event
.
attributes
=
{
'gen_ai.message.index'
:
message_index
,
**
(
event
.
attributes
or
{}),
}
events
.
extend
(
message_events
)
for
event
in
events
:
event
.
body
=
InstrumentedModel
.
serialize_any
(
event
.
body
)
return
events