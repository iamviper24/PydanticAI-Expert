PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.usage
Table of contents
usage
Usage
requests
request_tokens
response_tokens
total_tokens
details
incr
__add__
opentelemetry_attributes
has_values
UsageLimits
request_limit
request_tokens_limit
response_tokens_limit
total_tokens_limit
has_token_limits
check_before_request
check_tokens
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
usage
Usage
requests
request_tokens
response_tokens
total_tokens
details
incr
__add__
opentelemetry_attributes
has_values
UsageLimits
request_limit
request_tokens_limit
response_tokens_limit
total_tokens_limit
has_token_limits
check_before_request
check_tokens
pydantic_ai.usage
Usage
dataclass
LLM usage associated with a request or run.
Responsibility for calculating usage is on the model; PydanticAI simply sums the usage information across requests.
You'll need to look up the documentation of the model you're using to convert usage to monetary costs.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
@dataclass
(
repr
=
False
)
class
Usage
:
"""LLM usage associated with a request or run.
Responsibility for calculating usage is on the model; PydanticAI simply sums the usage information across requests.
You'll need to look up the documentation of the model you're using to convert usage to monetary costs.
"""
requests
:
int
=
0
"""Number of requests made to the LLM API."""
request_tokens
:
int
|
None
=
None
"""Tokens used in processing requests."""
response_tokens
:
int
|
None
=
None
"""Tokens used in generating responses."""
total_tokens
:
int
|
None
=
None
"""Total tokens used in the whole run, should generally be equal to `request_tokens + response_tokens`."""
details
:
dict
[
str
,
int
]
|
None
=
None
"""Any extra details returned by the model."""
def
incr
(
self
,
incr_usage
:
Usage
)
->
None
:
"""Increment the usage in place.
Args:
incr_usage: The usage to increment by.
"""
for
f
in
'requests'
,
'request_tokens'
,
'response_tokens'
,
'total_tokens'
:
self_value
=
getattr
(
self
,
f
)
other_value
=
getattr
(
incr_usage
,
f
)
if
self_value
is
not
None
or
other_value
is
not
None
:
setattr
(
self
,
f
,
(
self_value
or
0
)
+
(
other_value
or
0
))
if
incr_usage
.
details
:
self
.
details
=
self
.
details
or
{}
for
key
,
value
in
incr_usage
.
details
.
items
():
self
.
details
[
key
]
=
self
.
details
.
get
(
key
,
0
)
+
value
def
__add__
(
self
,
other
:
Usage
)
->
Usage
:
"""Add two Usages together.
This is provided so it's trivial to sum usage information from multiple requests and runs.
"""
new_usage
=
copy
(
self
)
new_usage
.
incr
(
other
)
return
new_usage
def
opentelemetry_attributes
(
self
)
->
dict
[
str
,
int
]:
"""Get the token limits as OpenTelemetry attributes."""
result
=
{
'gen_ai.usage.input_tokens'
:
self
.
request_tokens
,
'gen_ai.usage.output_tokens'
:
self
.
response_tokens
,
}
for
key
,
value
in
(
self
.
details
or
{})
.
items
():
result
[
f
'gen_ai.usage.details.
{
key
}
'
]
=
value
# pragma: no cover
return
{
k
:
v
for
k
,
v
in
result
.
items
()
if
v
}
def
has_values
(
self
)
->
bool
:
"""Whether any values are set and non-zero."""
return
bool
(
self
.
requests
or
self
.
request_tokens
or
self
.
response_tokens
or
self
.
details
)
__repr__
=
_utils
.
dataclasses_no_defaults_repr
requests
class-attribute
instance-attribute
requests
:
int
=
0
Number of requests made to the LLM API.
request_tokens
class-attribute
instance-attribute
request_tokens
:
int
|
None
=
None
Tokens used in processing requests.
response_tokens
class-attribute
instance-attribute
response_tokens
:
int
|
None
=
None
Tokens used in generating responses.
total_tokens
class-attribute
instance-attribute
total_tokens
:
int
|
None
=
None
Total tokens used in the whole run, should generally be equal to
request_tokens + response_tokens
.
details
class-attribute
instance-attribute
details
:
dict
[
str
,
int
]
|
None
=
None
Any extra details returned by the model.
incr
incr
(
incr_usage
:
Usage
)
->
None
Increment the usage in place.
Parameters:
Name
Type
Description
Default
incr_usage
Usage
The usage to increment by.
required
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
def
incr
(
self
,
incr_usage
:
Usage
)
->
None
:
"""Increment the usage in place.
Args:
incr_usage: The usage to increment by.
"""
for
f
in
'requests'
,
'request_tokens'
,
'response_tokens'
,
'total_tokens'
:
self_value
=
getattr
(
self
,
f
)
other_value
=
getattr
(
incr_usage
,
f
)
if
self_value
is
not
None
or
other_value
is
not
None
:
setattr
(
self
,
f
,
(
self_value
or
0
)
+
(
other_value
or
0
))
if
incr_usage
.
details
:
self
.
details
=
self
.
details
or
{}
for
key
,
value
in
incr_usage
.
details
.
items
():
self
.
details
[
key
]
=
self
.
details
.
get
(
key
,
0
)
+
value
__add__
__add__
(
other
:
Usage
)
->
Usage
Add two Usages together.
This is provided so it's trivial to sum usage information from multiple requests and runs.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
49
50
51
52
53
54
55
56
def
__add__
(
self
,
other
:
Usage
)
->
Usage
:
"""Add two Usages together.
This is provided so it's trivial to sum usage information from multiple requests and runs.
"""
new_usage
=
copy
(
self
)
new_usage
.
incr
(
other
)
return
new_usage
opentelemetry_attributes
opentelemetry_attributes
()
->
dict
[
str
,
int
]
Get the token limits as OpenTelemetry attributes.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
58
59
60
61
62
63
64
65
66
def
opentelemetry_attributes
(
self
)
->
dict
[
str
,
int
]:
"""Get the token limits as OpenTelemetry attributes."""
result
=
{
'gen_ai.usage.input_tokens'
:
self
.
request_tokens
,
'gen_ai.usage.output_tokens'
:
self
.
response_tokens
,
}
for
key
,
value
in
(
self
.
details
or
{})
.
items
():
result
[
f
'gen_ai.usage.details.
{
key
}
'
]
=
value
# pragma: no cover
return
{
k
:
v
for
k
,
v
in
result
.
items
()
if
v
}
has_values
has_values
()
->
bool
Whether any values are set and non-zero.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
68
69
70
def
has_values
(
self
)
->
bool
:
"""Whether any values are set and non-zero."""
return
bool
(
self
.
requests
or
self
.
request_tokens
or
self
.
response_tokens
or
self
.
details
)
UsageLimits
dataclass
Limits on model usage.
The request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.
Token counts are provided in responses from the model, and the token limits are checked after each response.
Each of the limits can be set to
None
to disable that limit.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
@dataclass
(
repr
=
False
)
class
UsageLimits
:
"""Limits on model usage.
The request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.
Token counts are provided in responses from the model, and the token limits are checked after each response.
Each of the limits can be set to `None` to disable that limit.
"""
request_limit
:
int
|
None
=
50
"""The maximum number of requests allowed to the model."""
request_tokens_limit
:
int
|
None
=
None
"""The maximum number of tokens allowed in requests to the model."""
response_tokens_limit
:
int
|
None
=
None
"""The maximum number of tokens allowed in responses from the model."""
total_tokens_limit
:
int
|
None
=
None
"""The maximum number of tokens allowed in requests and responses combined."""
def
has_token_limits
(
self
)
->
bool
:
"""Returns `True` if this instance places any limits on token counts.
If this returns `False`, the `check_tokens` method will never raise an error.
This is useful because if we have token limits, we need to check them after receiving each streamed message.
If there are no limits, we can skip that processing in the streaming response iterator.
"""
return
any
(
limit
is
not
None
for
limit
in
(
self
.
request_tokens_limit
,
self
.
response_tokens_limit
,
self
.
total_tokens_limit
)
)
def
check_before_request
(
self
,
usage
:
Usage
)
->
None
:
"""Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit."""
request_limit
=
self
.
request_limit
if
request_limit
is
not
None
and
usage
.
requests
>=
request_limit
:
raise
UsageLimitExceeded
(
f
'The next request would exceed the request_limit of
{
request_limit
}
'
)
def
check_tokens
(
self
,
usage
:
Usage
)
->
None
:
"""Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits."""
request_tokens
=
usage
.
request_tokens
or
0
if
self
.
request_tokens_limit
is
not
None
and
request_tokens
>
self
.
request_tokens_limit
:
raise
UsageLimitExceeded
(
f
'Exceeded the request_tokens_limit of
{
self
.
request_tokens_limit
}
(
{
request_tokens
=}
)'
)
response_tokens
=
usage
.
response_tokens
or
0
if
self
.
response_tokens_limit
is
not
None
and
response_tokens
>
self
.
response_tokens_limit
:
raise
UsageLimitExceeded
(
f
'Exceeded the response_tokens_limit of
{
self
.
response_tokens_limit
}
(
{
response_tokens
=}
)'
)
total_tokens
=
usage
.
total_tokens
or
0
if
self
.
total_tokens_limit
is
not
None
and
total_tokens
>
self
.
total_tokens_limit
:
raise
UsageLimitExceeded
(
f
'Exceeded the total_tokens_limit of
{
self
.
total_tokens_limit
}
(
{
total_tokens
=}
)'
)
__repr__
=
_utils
.
dataclasses_no_defaults_repr
request_limit
class-attribute
instance-attribute
request_limit
:
int
|
None
=
50
The maximum number of requests allowed to the model.
request_tokens_limit
class-attribute
instance-attribute
request_tokens_limit
:
int
|
None
=
None
The maximum number of tokens allowed in requests to the model.
response_tokens_limit
class-attribute
instance-attribute
response_tokens_limit
:
int
|
None
=
None
The maximum number of tokens allowed in responses from the model.
total_tokens_limit
class-attribute
instance-attribute
total_tokens_limit
:
int
|
None
=
None
The maximum number of tokens allowed in requests and responses combined.
has_token_limits
has_token_limits
()
->
bool
Returns
True
if this instance places any limits on token counts.
If this returns
False
, the
check_tokens
method will never raise an error.
This is useful because if we have token limits, we need to check them after receiving each streamed message.
If there are no limits, we can skip that processing in the streaming response iterator.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
94
95
96
97
98
99
100
101
102
103
104
105
def
has_token_limits
(
self
)
->
bool
:
"""Returns `True` if this instance places any limits on token counts.
If this returns `False`, the `check_tokens` method will never raise an error.
This is useful because if we have token limits, we need to check them after receiving each streamed message.
If there are no limits, we can skip that processing in the streaming response iterator.
"""
return
any
(
limit
is
not
None
for
limit
in
(
self
.
request_tokens_limit
,
self
.
response_tokens_limit
,
self
.
total_tokens_limit
)
)
check_before_request
check_before_request
(
usage
:
Usage
)
->
None
Raises a
UsageLimitExceeded
exception if the next request would exceed the request_limit.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
107
108
109
110
111
def
check_before_request
(
self
,
usage
:
Usage
)
->
None
:
"""Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit."""
request_limit
=
self
.
request_limit
if
request_limit
is
not
None
and
usage
.
requests
>=
request_limit
:
raise
UsageLimitExceeded
(
f
'The next request would exceed the request_limit of
{
request_limit
}
'
)
check_tokens
check_tokens
(
usage
:
Usage
)
->
None
Raises a
UsageLimitExceeded
exception if the usage exceeds any of the token limits.
Source code in
pydantic_ai_slim/pydantic_ai/usage.py
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
def
check_tokens
(
self
,
usage
:
Usage
)
->
None
:
"""Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits."""
request_tokens
=
usage
.
request_tokens
or
0
if
self
.
request_tokens_limit
is
not
None
and
request_tokens
>
self
.
request_tokens_limit
:
raise
UsageLimitExceeded
(
f
'Exceeded the request_tokens_limit of
{
self
.
request_tokens_limit
}
(
{
request_tokens
=}
)'
)
response_tokens
=
usage
.
response_tokens
or
0
if
self
.
response_tokens_limit
is
not
None
and
response_tokens
>
self
.
response_tokens_limit
:
raise
UsageLimitExceeded
(
f
'Exceeded the response_tokens_limit of
{
self
.
response_tokens_limit
}
(
{
response_tokens
=}
)'
)
total_tokens
=
usage
.
total_tokens
or
0
if
self
.
total_tokens_limit
is
not
None
and
total_tokens
>
self
.
total_tokens_limit
:
raise
UsageLimitExceeded
(
f
'Exceeded the total_tokens_limit of
{
self
.
total_tokens_limit
}
(
{
total_tokens
=}
)'
)