PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.instrumented
Table of contents
instrumented
instrument_model
InstrumentationSettings
__init__
messages_to_otel_events
InstrumentedModel
settings
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
instrumented
instrument_model
InstrumentationSettings
__init__
messages_to_otel_events
InstrumentedModel
settings
pydantic_ai.models.instrumented
instrument_model
instrument_model
(
model
:
Model
,
instrument
:
InstrumentationSettings
|
bool
)
->
Model
Instrument a model with OpenTelemetry/logfire.
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
58
59
60
61
62
63
64
65
66
def
instrument_model
(
model
:
Model
,
instrument
:
InstrumentationSettings
|
bool
)
->
Model
:
"""Instrument a model with OpenTelemetry/logfire."""
if
instrument
and
not
isinstance
(
model
,
InstrumentedModel
):
if
instrument
is
True
:
instrument
=
InstrumentationSettings
()
model
=
InstrumentedModel
(
model
,
instrument
)
return
model
InstrumentationSettings
dataclass
Options for instrumenting models and agents with OpenTelemetry.
Used in:
Agent(instrument=...)
Agent.instrument_all()
InstrumentedModel
See the
Debugging and Monitoring guide
for more info.
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
@dataclass
(
init
=
False
)
class
InstrumentationSettings
:
"""Options for instrumenting models and agents with OpenTelemetry.
Used in:
- `Agent(instrument=...)`
- [`Agent.instrument_all()`][pydantic_ai.agent.Agent.instrument_all]
- [`InstrumentedModel`][pydantic_ai.models.instrumented.InstrumentedModel]
See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
"""
tracer
:
Tracer
=
field
(
repr
=
False
)
event_logger
:
EventLogger
=
field
(
repr
=
False
)
event_mode
:
Literal
[
'attributes'
,
'logs'
]
=
'attributes'
include_binary_content
:
bool
=
True
def
__init__
(
self
,
*
,
event_mode
:
Literal
[
'attributes'
,
'logs'
]
=
'attributes'
,
tracer_provider
:
TracerProvider
|
None
=
None
,
meter_provider
:
MeterProvider
|
None
=
None
,
event_logger_provider
:
EventLoggerProvider
|
None
=
None
,
include_binary_content
:
bool
=
True
,
include_content
:
bool
=
True
,
):
"""Create instrumentation options.
Args:
event_mode: The mode for emitting events. If `'attributes'`, events are attached to the span as attributes.
If `'logs'`, events are emitted as OpenTelemetry log-based events.
tracer_provider: The OpenTelemetry tracer provider to use.
If not provided, the global tracer provider is used.
Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.
meter_provider: The OpenTelemetry meter provider to use.
If not provided, the global meter provider is used.
Calling `logfire.configure()` sets the global meter provider, so most users don't need this.
event_logger_provider: The OpenTelemetry event logger provider to use.
If not provided, the global event logger provider is used.
Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.
This is only used if `event_mode='logs'`.
include_binary_content: Whether to include binary content in the instrumentation events.
include_content: Whether to include prompts, completions, and tool call arguments and responses
in the instrumentation events.
"""
from
pydantic_ai
import
__version__
tracer_provider
=
tracer_provider
or
get_tracer_provider
()
meter_provider
=
meter_provider
or
get_meter_provider
()
event_logger_provider
=
event_logger_provider
or
get_event_logger_provider
()
scope_name
=
'pydantic-ai'
self
.
tracer
=
tracer_provider
.
get_tracer
(
scope_name
,
__version__
)
self
.
meter
=
meter_provider
.
get_meter
(
scope_name
,
__version__
)
self
.
event_logger
=
event_logger_provider
.
get_event_logger
(
scope_name
,
__version__
)
self
.
event_mode
=
event_mode
self
.
include_binary_content
=
include_binary_content
self
.
include_content
=
include_content
# As specified in the OpenTelemetry GenAI metrics spec:
# https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage
tokens_histogram_kwargs
=
dict
(
name
=
'gen_ai.client.token.usage'
,
unit
=
'
{token}
'
,
description
=
'Measures number of input and output tokens used'
,
)
try
:
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
explicit_bucket_boundaries_advisory
=
TOKEN_HISTOGRAM_BOUNDARIES
,
)
except
TypeError
:
# pragma: lax no cover
# Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
# pyright: ignore
)
def
messages_to_otel_events
(
self
,
messages
:
list
[
ModelMessage
])
->
list
[
Event
]:
"""Convert a list of model messages to OpenTelemetry events.
Args:
messages: The messages to convert.
Returns:
A list of OpenTelemetry events.
"""
events
:
list
[
Event
]
=
[]
instructions
=
InstrumentedModel
.
_get_instructions
(
messages
)
# pyright: ignore [reportPrivateUsage]
if
instructions
is
not
None
:
events
.
append
(
Event
(
'gen_ai.system.message'
,
body
=
{
'content'
:
instructions
,
'role'
:
'system'
}))
for
message_index
,
message
in
enumerate
(
messages
):
message_events
:
list
[
Event
]
=
[]
if
isinstance
(
message
,
ModelRequest
):
for
part
in
message
.
parts
:
if
hasattr
(
part
,
'otel_event'
):
message_events
.
append
(
part
.
otel_event
(
self
))
elif
isinstance
(
message
,
ModelResponse
):
# pragma: no branch
message_events
=
message
.
otel_events
(
self
)
for
event
in
message_events
:
event
.
attributes
=
{
'gen_ai.message.index'
:
message_index
,
**
(
event
.
attributes
or
{}),
}
events
.
extend
(
message_events
)
for
event
in
events
:
event
.
body
=
InstrumentedModel
.
serialize_any
(
event
.
body
)
return
events
__init__
__init__
(
*
,
event_mode
:
Literal
[
"attributes"
,
"logs"
]
=
"attributes"
,
tracer_provider
:
TracerProvider
|
None
=
None
,
meter_provider
:
MeterProvider
|
None
=
None
,
event_logger_provider
:
(
EventLoggerProvider
|
None
)
=
None
,
include_binary_content
:
bool
=
True
,
include_content
:
bool
=
True
)
Create instrumentation options.
Parameters:
Name
Type
Description
Default
event_mode
Literal
['attributes', 'logs']
The mode for emitting events. If
'attributes'
, events are attached to the span as attributes.
If
'logs'
, events are emitted as OpenTelemetry log-based events.
'attributes'
tracer_provider
TracerProvider
| None
The OpenTelemetry tracer provider to use.
If not provided, the global tracer provider is used.
Calling
logfire.configure()
sets the global tracer provider, so most users don't need this.
None
meter_provider
MeterProvider
| None
The OpenTelemetry meter provider to use.
If not provided, the global meter provider is used.
Calling
logfire.configure()
sets the global meter provider, so most users don't need this.
None
event_logger_provider
EventLoggerProvider
| None
The OpenTelemetry event logger provider to use.
If not provided, the global event logger provider is used.
Calling
logfire.configure()
sets the global event logger provider, so most users don't need this.
This is only used if
event_mode='logs'
.
None
include_binary_content
bool
Whether to include binary content in the instrumentation events.
True
include_content
bool
Whether to include prompts, completions, and tool call arguments and responses
in the instrumentation events.
True
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
def
__init__
(
self
,
*
,
event_mode
:
Literal
[
'attributes'
,
'logs'
]
=
'attributes'
,
tracer_provider
:
TracerProvider
|
None
=
None
,
meter_provider
:
MeterProvider
|
None
=
None
,
event_logger_provider
:
EventLoggerProvider
|
None
=
None
,
include_binary_content
:
bool
=
True
,
include_content
:
bool
=
True
,
):
"""Create instrumentation options.
Args:
event_mode: The mode for emitting events. If `'attributes'`, events are attached to the span as attributes.
If `'logs'`, events are emitted as OpenTelemetry log-based events.
tracer_provider: The OpenTelemetry tracer provider to use.
If not provided, the global tracer provider is used.
Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.
meter_provider: The OpenTelemetry meter provider to use.
If not provided, the global meter provider is used.
Calling `logfire.configure()` sets the global meter provider, so most users don't need this.
event_logger_provider: The OpenTelemetry event logger provider to use.
If not provided, the global event logger provider is used.
Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.
This is only used if `event_mode='logs'`.
include_binary_content: Whether to include binary content in the instrumentation events.
include_content: Whether to include prompts, completions, and tool call arguments and responses
in the instrumentation events.
"""
from
pydantic_ai
import
__version__
tracer_provider
=
tracer_provider
or
get_tracer_provider
()
meter_provider
=
meter_provider
or
get_meter_provider
()
event_logger_provider
=
event_logger_provider
or
get_event_logger_provider
()
scope_name
=
'pydantic-ai'
self
.
tracer
=
tracer_provider
.
get_tracer
(
scope_name
,
__version__
)
self
.
meter
=
meter_provider
.
get_meter
(
scope_name
,
__version__
)
self
.
event_logger
=
event_logger_provider
.
get_event_logger
(
scope_name
,
__version__
)
self
.
event_mode
=
event_mode
self
.
include_binary_content
=
include_binary_content
self
.
include_content
=
include_content
# As specified in the OpenTelemetry GenAI metrics spec:
# https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage
tokens_histogram_kwargs
=
dict
(
name
=
'gen_ai.client.token.usage'
,
unit
=
'
{token}
'
,
description
=
'Measures number of input and output tokens used'
,
)
try
:
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
explicit_bucket_boundaries_advisory
=
TOKEN_HISTOGRAM_BOUNDARIES
,
)
except
TypeError
:
# pragma: lax no cover
# Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory
self
.
tokens_histogram
=
self
.
meter
.
create_histogram
(
**
tokens_histogram_kwargs
,
# pyright: ignore
)
messages_to_otel_events
messages_to_otel_events
(
messages
:
list
[
ModelMessage
],
)
->
list
[
Event
]
Convert a list of model messages to OpenTelemetry events.
Parameters:
Name
Type
Description
Default
messages
list
[
ModelMessage
]
The messages to convert.
required
Returns:
Type
Description
list
[
Event
]
A list of OpenTelemetry events.
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
def
messages_to_otel_events
(
self
,
messages
:
list
[
ModelMessage
])
->
list
[
Event
]:
"""Convert a list of model messages to OpenTelemetry events.
Args:
messages: The messages to convert.
Returns:
A list of OpenTelemetry events.
"""
events
:
list
[
Event
]
=
[]
instructions
=
InstrumentedModel
.
_get_instructions
(
messages
)
# pyright: ignore [reportPrivateUsage]
if
instructions
is
not
None
:
events
.
append
(
Event
(
'gen_ai.system.message'
,
body
=
{
'content'
:
instructions
,
'role'
:
'system'
}))
for
message_index
,
message
in
enumerate
(
messages
):
message_events
:
list
[
Event
]
=
[]
if
isinstance
(
message
,
ModelRequest
):
for
part
in
message
.
parts
:
if
hasattr
(
part
,
'otel_event'
):
message_events
.
append
(
part
.
otel_event
(
self
))
elif
isinstance
(
message
,
ModelResponse
):
# pragma: no branch
message_events
=
message
.
otel_events
(
self
)
for
event
in
message_events
:
event
.
attributes
=
{
'gen_ai.message.index'
:
message_index
,
**
(
event
.
attributes
or
{}),
}
events
.
extend
(
message_events
)
for
event
in
events
:
event
.
body
=
InstrumentedModel
.
serialize_any
(
event
.
body
)
return
events
InstrumentedModel
dataclass
Bases:
WrapperModel
Model which wraps another model so that requests are instrumented with OpenTelemetry.
See the
Debugging and Monitoring guide
for more info.
Source code in
pydantic_ai_slim/pydantic_ai/models/instrumented.py
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
@dataclass
class
InstrumentedModel
(
WrapperModel
):
"""Model which wraps another model so that requests are instrumented with OpenTelemetry.
See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
"""
settings
:
InstrumentationSettings
"""Configuration for instrumenting requests."""
def
__init__
(
self
,
wrapped
:
Model
|
KnownModelName
,
options
:
InstrumentationSettings
|
None
=
None
,
)
->
None
:
super
()
.
__init__
(
wrapped
)
self
.
settings
=
options
or
InstrumentationSettings
()
async
def
request
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
ModelResponse
:
with
self
.
_instrument
(
messages
,
model_settings
,
model_request_parameters
)
as
finish
:
response
=
await
super
()
.
request
(
messages
,
model_settings
,
model_request_parameters
)
finish
(
response
)
return
response
@asynccontextmanager
async
def
request_stream
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
AsyncIterator
[
StreamedResponse
]:
with
self
.
_instrument
(
messages
,
model_settings
,
model_request_parameters
)
as
finish
:
response_stream
:
StreamedResponse
|
None
=
None
try
:
async
with
super
()
.
request_stream
(
messages
,
model_settings
,
model_request_parameters
)
as
response_stream
:
yield
response_stream
finally
:
if
response_stream
:
# pragma: no branch
finish
(
response_stream
.
get
())
@contextmanager
def
_instrument
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
Iterator
[
Callable
[[
ModelResponse
],
None
]]:
operation
=
'chat'
span_name
=
f
'
{
operation
}
{
self
.
model_name
}
'
# TODO Missing attributes:
#  - error.type: unclear if we should do something here or just always rely on span exceptions
#  - gen_ai.request.stop_sequences/top_k: model_settings doesn't include these
attributes
:
dict
[
str
,
AttributeValue
]
=
{
'gen_ai.operation.name'
:
operation
,
**
self
.
model_attributes
(
self
.
wrapped
),
'model_request_parameters'
:
json
.
dumps
(
InstrumentedModel
.
serialize_any
(
model_request_parameters
)),
'logfire.json_schema'
:
json
.
dumps
(
{
'type'
:
'object'
,
'properties'
:
{
'model_request_parameters'
:
{
'type'
:
'object'
}},
}
),
}
if
model_settings
:
for
key
in
MODEL_SETTING_ATTRIBUTES
:
if
isinstance
(
value
:=
model_settings
.
get
(
key
),
(
float
,
int
)):
attributes
[
f
'gen_ai.request.
{
key
}
'
]
=
value
record_metrics
:
Callable
[[],
None
]
|
None
=
None
try
:
with
self
.
settings
.
tracer
.
start_as_current_span
(
span_name
,
attributes
=
attributes
)
as
span
:
def
finish
(
response
:
ModelResponse
):
# FallbackModel updates these span attributes.
attributes
.
update
(
getattr
(
span
,
'attributes'
,
{}))
request_model
=
attributes
[
GEN_AI_REQUEST_MODEL_ATTRIBUTE
]
system
=
attributes
[
GEN_AI_SYSTEM_ATTRIBUTE
]
response_model
=
response
.
model_name
or
request_model
def
_record_metrics
():
metric_attributes
=
{
GEN_AI_SYSTEM_ATTRIBUTE
:
system
,
'gen_ai.operation.name'
:
operation
,
'gen_ai.request.model'
:
request_model
,
'gen_ai.response.model'
:
response_model
,
}
if
response
.
usage
.
request_tokens
:
# pragma: no branch
self
.
settings
.
tokens_histogram
.
record
(
response
.
usage
.
request_tokens
,
{
**
metric_attributes
,
'gen_ai.token.type'
:
'input'
},
)
if
response
.
usage
.
response_tokens
:
# pragma: no branch
self
.
settings
.
tokens_histogram
.
record
(
response
.
usage
.
response_tokens
,
{
**
metric_attributes
,
'gen_ai.token.type'
:
'output'
},
)
nonlocal
record_metrics
record_metrics
=
_record_metrics
if
not
span
.
is_recording
():
return
events
=
self
.
settings
.
messages_to_otel_events
(
messages
)
for
event
in
self
.
settings
.
messages_to_otel_events
([
response
]):
events
.
append
(
Event
(
'gen_ai.choice'
,
body
=
{
# TODO finish_reason
'index'
:
0
,
'message'
:
event
.
body
,
},
)
)
span
.
set_attributes
(
{
**
response
.
usage
.
opentelemetry_attributes
(),
'gen_ai.response.model'
:
response_model
,
}
)
span
.
update_name
(
f
'
{
operation
}
{
request_model
}
'
)
for
event
in
events
:
event
.
attributes
=
{
GEN_AI_SYSTEM_ATTRIBUTE
:
system
,
**
(
event
.
attributes
or
{}),
}
self
.
_emit_events
(
span
,
events
)
yield
finish
finally
:
if
record_metrics
:
# We only want to record metrics after the span is finished,
# to prevent them from being redundantly recorded in the span itself by logfire.
record_metrics
()
def
_emit_events
(
self
,
span
:
Span
,
events
:
list
[
Event
])
->
None
:
if
self
.
settings
.
event_mode
==
'logs'
:
for
event
in
events
:
self
.
settings
.
event_logger
.
emit
(
event
)
else
:
attr_name
=
'events'
span
.
set_attributes
(
{
attr_name
:
json
.
dumps
([
self
.
event_to_dict
(
event
)
for
event
in
events
]),
'logfire.json_schema'
:
json
.
dumps
(
{
'type'
:
'object'
,
'properties'
:
{
attr_name
:
{
'type'
:
'array'
},
'model_request_parameters'
:
{
'type'
:
'object'
},
},
}
),
}
)
@staticmethod
def
model_attributes
(
model
:
Model
):
attributes
:
dict
[
str
,
AttributeValue
]
=
{
GEN_AI_SYSTEM_ATTRIBUTE
:
model
.
system
,
GEN_AI_REQUEST_MODEL_ATTRIBUTE
:
model
.
model_name
,
}
if
base_url
:=
model
.
base_url
:
try
:
parsed
=
urlparse
(
base_url
)
except
Exception
:
# pragma: no cover
pass
else
:
if
parsed
.
hostname
:
# pragma: no branch
attributes
[
'server.address'
]
=
parsed
.
hostname
if
parsed
.
port
:
# pragma: no branch
attributes
[
'server.port'
]
=
parsed
.
port
return
attributes
@staticmethod
def
event_to_dict
(
event
:
Event
)
->
dict
[
str
,
Any
]:
if
not
event
.
body
:
body
=
{}
# pragma: no cover
elif
isinstance
(
event
.
body
,
Mapping
):
body
=
event
.
body
# type: ignore
else
:
body
=
{
'body'
:
event
.
body
}
return
{
**
body
,
**
(
event
.
attributes
or
{})}
@staticmethod
def
serialize_any
(
value
:
Any
)
->
str
:
try
:
return
ANY_ADAPTER
.
dump_python
(
value
,
mode
=
'json'
)
except
Exception
:
try
:
return
str
(
value
)
except
Exception
as
e
:
return
f
'Unable to serialize:
{
e
}
'
settings
instance-attribute
settings
:
InstrumentationSettings
=
(
options
or
InstrumentationSettings
()
)
Configuration for instrumenting requests.