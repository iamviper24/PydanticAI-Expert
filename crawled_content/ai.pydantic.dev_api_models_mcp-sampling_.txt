PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.models.mcp_sampling
Table of contents
mcp_sampling
MCPSamplingModelSettings
mcp_model_preferences
MCPSamplingModel
session
default_max_tokens
model_name
system
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
mcp_sampling
MCPSamplingModelSettings
mcp_model_preferences
MCPSamplingModel
session
default_max_tokens
model_name
system
pydantic_ai.models.mcp_sampling
MCPSamplingModelSettings
Bases:
ModelSettings
Settings used for an MCP Sampling model request.
Source code in
pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py
18
19
20
21
22
23
24
class
MCPSamplingModelSettings
(
ModelSettings
,
total
=
False
):
"""Settings used for an MCP Sampling model request."""
# ALL FIELDS MUST BE `mcp_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.
mcp_model_preferences
:
ModelPreferences
"""Model preferences to use for MCP Sampling."""
mcp_model_preferences
instance-attribute
mcp_model_preferences
:
ModelPreferences
Model preferences to use for MCP Sampling.
MCPSamplingModel
dataclass
Bases:
Model
A model that uses MCP Sampling.
MCP Sampling
allows an MCP server to make requests to a model by calling back to the MCP client that connected to it.
Source code in
pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
@dataclass
class
MCPSamplingModel
(
Model
):
"""A model that uses MCP Sampling.
[MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling)
allows an MCP server to make requests to a model by calling back to the MCP client that connected to it.
"""
session
:
ServerSession
"""The MCP server session to use for sampling."""
default_max_tokens
:
int
=
16_384
"""Default max tokens to use if not set in [`ModelSettings`][pydantic_ai.settings.ModelSettings.max_tokens].
Max tokens is a required parameter for MCP Sampling, but optional on
[`ModelSettings`][pydantic_ai.settings.ModelSettings], so this value is used as fallback.
"""
async
def
request
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
ModelResponse
:
system_prompt
,
sampling_messages
=
_mcp
.
map_from_pai_messages
(
messages
)
model_settings
=
cast
(
MCPSamplingModelSettings
,
model_settings
or
{})
result
=
await
self
.
session
.
create_message
(
sampling_messages
,
max_tokens
=
model_settings
.
get
(
'max_tokens'
,
self
.
default_max_tokens
),
system_prompt
=
system_prompt
,
temperature
=
model_settings
.
get
(
'temperature'
),
model_preferences
=
model_settings
.
get
(
'mcp_model_preferences'
),
stop_sequences
=
model_settings
.
get
(
'stop_sequences'
),
)
if
result
.
role
==
'assistant'
:
return
ModelResponse
(
parts
=
[
_mcp
.
map_from_sampling_content
(
result
.
content
)],
usage
=
usage
.
Usage
(
requests
=
1
),
model_name
=
result
.
model
,
)
else
:
raise
exceptions
.
UnexpectedModelBehavior
(
f
'Unexpected result from MCP sampling, expected "assistant" role, got
{
result
.
role
}
.'
)
@asynccontextmanager
async
def
request_stream
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
AsyncIterator
[
StreamedResponse
]:
raise
NotImplementedError
(
'MCP Sampling does not support streaming'
)
yield
@property
def
model_name
(
self
)
->
str
:
"""The model name.
Since the model name isn't known until the request is made, this property always returns `'mcp-sampling'`.
"""
return
'mcp-sampling'
@property
def
system
(
self
)
->
str
:
"""The system / model provider, returns `'MCP'`."""
return
'MCP'
session
instance-attribute
session
:
ServerSession
The MCP server session to use for sampling.
default_max_tokens
class-attribute
instance-attribute
default_max_tokens
:
int
=
16384
Default max tokens to use if not set in
ModelSettings
.
Max tokens is a required parameter for MCP Sampling, but optional on
ModelSettings
, so this value is used as fallback.
model_name
property
model_name
:
str
The model name.
Since the model name isn't known until the request is made, this property always returns
'mcp-sampling'
.
system
property
system
:
str
The system / model provider, returns
'MCP'
.