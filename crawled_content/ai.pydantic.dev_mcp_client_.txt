PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Client
Table of contents
Install
Usage
SSE Client
Streamable HTTP Client
MCP "stdio" Server
Tool call customisation
Using Tool Prefixes to Avoid Naming Conflicts
How It Works
Example with HTTP Server
Example with Stdio Server
MCP Sampling
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
Install
Usage
SSE Client
Streamable HTTP Client
MCP "stdio" Server
Tool call customisation
Using Tool Prefixes to Avoid Naming Conflicts
How It Works
Example with HTTP Server
Example with Stdio Server
MCP Sampling
Client
PydanticAI can act as an
MCP client
, connecting to MCP servers
to use their tools.
Install
You need to either install
pydantic-ai
, or
pydantic-ai-slim
with the
mcp
optional group:
pip
uv
pip
install
"pydantic-ai-slim[mcp]"
uv
add
"pydantic-ai-slim[mcp]"
Note
MCP integration requires Python 3.10 or higher.
Usage
PydanticAI comes with two ways to connect to MCP servers:
MCPServerSSE
which connects to an MCP server using the
HTTP SSE
transport
MCPServerStreamableHTTP
which connects to an MCP server using the
Streamable HTTP
transport
MCPServerStdio
which runs the server as a subprocess and connects to it using the
stdio
transport
Examples of both are shown below;
mcp-run-python
is used as the MCP server in both examples.
SSE Client
MCPServerSSE
connects over HTTP using the
HTTP + Server Sent Events transport
to a server.
Note
MCPServerSSE
requires an MCP server to be running and accepting HTTP connections before calling
agent.run_mcp_servers()
. Running the server is not managed by PydanticAI.
The name "HTTP" is used since this implementation will be adapted in future to use the new
Streamable HTTP
currently in development.
Before creating the SSE client, we need to run the server (docs
here
):
terminal (run sse server)
deno
run
\
-N
-R
=
node_modules
-W
=
node_modules
--node-modules-dir
=
auto
\
jsr:@pydantic/mcp-run-python
sse
mcp_sse_client.py
from
pydantic_ai
import
Agent
from
pydantic_ai.mcp
import
MCPServerSSE
server
=
MCPServerSSE
(
url
=
'http://localhost:3001/sse'
)
# (1)!
agent
=
Agent
(
'openai:gpt-4o'
,
mcp_servers
=
[
server
])
# (2)!
async
def
main
():
async
with
agent
.
run_mcp_servers
():
# (3)!
result
=
await
agent
.
run
(
'How many days between 2000-01-01 and 2025-03-18?'
)
print
(
result
.
output
)
#> There are 9,208 days between January 1, 2000, and March 18, 2025.
Define the MCP server with the URL used to connect.
Create an agent with the MCP server attached.
Create a client session to connect to the server.
(This example is complete, it can be run "as is" with Python 3.10+ — you'll need to add
asyncio.run(main())
to run
main
)
What's happening here?
The model is receiving the prompt "how many days between 2000-01-01 and 2025-03-18?"
The model decides "Oh, I've got this
run_python_code
tool, that will be a good way to answer this question", and writes some python code to calculate the answer.
The model returns a tool call
PydanticAI sends the tool call to the MCP server using the SSE transport
The model is called again with the return value of running the code
The model returns the final answer
You can visualise this clearly, and even see the code that's run by adding three lines of code to instrument the example with
logfire
:
mcp_sse_client_logfire.py
import
logfire
logfire
.
configure
()
logfire
.
instrument_pydantic_ai
()
Will display as follows:
Streamable HTTP Client
MCPServerStreamableHTTP
connects over HTTP using the
Streamable HTTP
transport to a server.
Note
MCPServerStreamableHTTP
requires an MCP server to be
running and accepting HTTP connections before calling
agent.run_mcp_servers()
. Running the server is not
managed by PydanticAI.
Before creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport.
streamable_http_server.py
from
mcp.server.fastmcp
import
FastMCP
app
=
FastMCP
()
@app
.
tool
()
def
add
(
a
:
int
,
b
:
int
)
->
int
:
return
a
+
b
if
__name__
==
'__main__'
:
app
.
run
(
transport
=
'streamable-http'
)
Then we can create the client:
mcp_streamable_http_client.py
from
pydantic_ai
import
Agent
from
pydantic_ai.mcp
import
MCPServerStreamableHTTP
server
=
MCPServerStreamableHTTP
(
'http://localhost:8000/mcp'
)
# (1)!
agent
=
Agent
(
'openai:gpt-4o'
,
mcp_servers
=
[
server
])
# (2)!
async
def
main
():
async
with
agent
.
run_mcp_servers
():
# (3)!
result
=
await
agent
.
run
(
'How many days between 2000-01-01 and 2025-03-18?'
)
print
(
result
.
output
)
#> There are 9,208 days between January 1, 2000, and March 18, 2025.
Define the MCP server with the URL used to connect.
Create an agent with the MCP server attached.
Create a client session to connect to the server.
(This example is complete, it can be run "as is" with Python 3.10+ — you'll need to add
asyncio.run(main())
to run
main
)
MCP "stdio" Server
The other transport offered by MCP is the
stdio transport
where the server is run as a subprocess and communicates with the client over
stdin
and
stdout
. In this case, you'd use the
MCPServerStdio
class.
Note
When using
MCPServerStdio
servers, the
agent.run_mcp_servers()
context manager is responsible for starting and stopping the server.
mcp_stdio_client.py
from
pydantic_ai
import
Agent
from
pydantic_ai.mcp
import
MCPServerStdio
server
=
MCPServerStdio
(
# (1)!
'deno'
,
args
=
[
'run'
,
'-N'
,
'-R=node_modules'
,
'-W=node_modules'
,
'--node-modules-dir=auto'
,
'jsr:@pydantic/mcp-run-python'
,
'stdio'
,
]
)
agent
=
Agent
(
'openai:gpt-4o'
,
mcp_servers
=
[
server
])
async
def
main
():
async
with
agent
.
run_mcp_servers
():
result
=
await
agent
.
run
(
'How many days between 2000-01-01 and 2025-03-18?'
)
print
(
result
.
output
)
#> There are 9,208 days between January 1, 2000, and March 18, 2025.
See
MCP Run Python
for more information.
Tool call customisation
The MCP servers provide the ability to set a
process_tool_call
which allows
the customisation of tool call requests and their responses.
A common use case for this is to inject metadata to the requests which the server
call needs.
mcp_process_tool_call.py
from
typing
import
Any
from
pydantic_ai
import
Agent
from
pydantic_ai.mcp
import
CallToolFunc
,
MCPServerStdio
,
ToolResult
from
pydantic_ai.models.test
import
TestModel
from
pydantic_ai.tools
import
RunContext
async
def
process_tool_call
(
ctx
:
RunContext
[
int
],
call_tool
:
CallToolFunc
,
tool_name
:
str
,
args
:
dict
[
str
,
Any
],
)
->
ToolResult
:
"""A tool call processor that passes along the deps."""
return
await
call_tool
(
tool_name
,
args
,
metadata
=
{
'deps'
:
ctx
.
deps
})
server
=
MCPServerStdio
(
'python'
,
[
'mcp_server.py'
],
process_tool_call
=
process_tool_call
)
agent
=
Agent
(
model
=
TestModel
(
call_tools
=
[
'echo_deps'
]),
deps_type
=
int
,
mcp_servers
=
[
server
]
)
async
def
main
():
async
with
agent
.
run_mcp_servers
():
result
=
await
agent
.
run
(
'Echo with deps set to 42'
,
deps
=
42
)
print
(
result
.
output
)
#> {"echo_deps":{"echo":"This is an echo message","deps":42}}
Using Tool Prefixes to Avoid Naming Conflicts
When connecting to multiple MCP servers that might provide tools with the same name, you can use the
tool_prefix
parameter to avoid naming conflicts. This parameter adds a prefix to all tool names from a specific server.
How It Works
If
tool_prefix
is set, all tools from that server will be prefixed with
{tool_prefix}_
When listing tools, the prefixed names are shown to the model
When calling tools, the prefix is automatically removed before sending the request to the server
This allows you to use multiple servers that might have overlapping tool names without conflicts.
Example with HTTP Server
mcp_tool_prefix_http_client.py
from
pydantic_ai
import
Agent
from
pydantic_ai.mcp
import
MCPServerSSE
# Create two servers with different prefixes
weather_server
=
MCPServerSSE
(
url
=
'http://localhost:3001/sse'
,
tool_prefix
=
'weather'
# Tools will be prefixed with 'weather_'
)
calculator_server
=
MCPServerSSE
(
url
=
'http://localhost:3002/sse'
,
tool_prefix
=
'calc'
# Tools will be prefixed with 'calc_'
)
# Both servers might have a tool named 'get_data', but they'll be exposed as:
# - 'weather_get_data'
# - 'calc_get_data'
agent
=
Agent
(
'openai:gpt-4o'
,
mcp_servers
=
[
weather_server
,
calculator_server
])
Example with Stdio Server
mcp_tool_prefix_stdio_client.py
from
pydantic_ai
import
Agent
from
pydantic_ai.mcp
import
MCPServerStdio
python_server
=
MCPServerStdio
(
'deno'
,
args
=
[
'run'
,
'-N'
,
'jsr:@pydantic/mcp-run-python'
,
'stdio'
,
],
tool_prefix
=
'py'
# Tools will be prefixed with 'py_'
)
js_server
=
MCPServerStdio
(
'node'
,
args
=
[
'run'
,
'mcp-js-server.js'
,
'stdio'
,
],
tool_prefix
=
'js'
# Tools will be prefixed with 'js_'
)
agent
=
Agent
(
'openai:gpt-4o'
,
mcp_servers
=
[
python_server
,
js_server
])
When the model interacts with these servers, it will see the prefixed tool names, but the prefixes will be automatically handled when making tool calls.
MCP Sampling
What is MCP Sampling?
In MCP
sampling
is a system by which an MCP server can make LLM calls via the MCP client - effectively proxying requests to an LLM via the client over whatever transport is being used.
Sampling is extremely useful when MCP servers need to use Gen AI but you don't want to provision them each with their own LLM credentials or when a public MCP server would like the connecting client to pay for LLM calls.
Confusingly it has nothing to do with the concept of "sampling" in observability, or frankly the concept of "sampling" in any other domain.
Sampling Diagram
Here's a mermaid diagram that may or may not make the data flow clearer:
sequenceDiagram
    participant LLM
    participant MCP_Client as MCP client
    participant MCP_Server as MCP server

    MCP_Client->>LLM: LLM call
    LLM->>MCP_Client: LLM tool call response

    MCP_Client->>MCP_Server: tool call
    MCP_Server->>MCP_Client: sampling "create message"

    MCP_Client->>LLM: LLM call
    LLM->>MCP_Client: LLM text response

    MCP_Client->>MCP_Server: sampling response
    MCP_Server->>MCP_Client: tool call response
Pydantic AI supports sampling as both a client and server. See the
server
documentation for details on how to use sampling within a server.
Sampling is automatically supported by Pydantic AI agents when they act as a client.
Let's say we have an MCP server that wants to use sampling (in this case to generate an SVG as per the tool arguments).
Sampling MCP Server
generate_svg.py
import
re
from
pathlib
import
Path
from
mcp
import
SamplingMessage
from
mcp.server.fastmcp
import
Context
,
FastMCP
from
mcp.types
import
TextContent
app
=
FastMCP
()
@app
.
tool
()
async
def
image_generator
(
ctx
:
Context
,
subject
:
str
,
style
:
str
)
->
str
:
prompt
=
f
'
{
subject
=}
{
style
=}
'
# `ctx.session.create_message` is the sampling call
result
=
await
ctx
.
session
.
create_message
(
[
SamplingMessage
(
role
=
'user'
,
content
=
TextContent
(
type
=
'text'
,
text
=
prompt
))],
max_tokens
=
1_024
,
system_prompt
=
'Generate an SVG image as per the user input'
,
)
assert
isinstance
(
result
.
content
,
TextContent
)
path
=
Path
(
f
'
{
subject
}
_
{
style
}
.svg'
)
# remove triple backticks if the svg was returned within markdown
if
m
:=
re
.
search
(
r
'^```\w*$(.+?)```$'
,
result
.
content
.
text
,
re
.
S
|
re
.
M
):
path
.
write_text
(
m
.
group
(
1
))
else
:
path
.
write_text
(
result
.
content
.
text
)
return
f
'See
{
path
}
'
if
__name__
==
'__main__'
:
# run the server via stdio
app
.
run
()
Using this server with an
Agent
will automatically allow sampling:
sampling_mcp_client.py
from
pydantic_ai
import
Agent
from
pydantic_ai.mcp
import
MCPServerStdio
server
=
MCPServerStdio
(
command
=
'python'
,
args
=
[
'generate_svg.py'
])
agent
=
Agent
(
'openai:gpt-4o'
,
mcp_servers
=
[
server
])
async
def
main
():
async
with
agent
.
run_mcp_servers
():
result
=
await
agent
.
run
(
'Create an image of a robot in a punk style.'
)
print
(
result
.
output
)
#> Image file written to robot_punk.svg.
(This example is complete, it can be run "as is" with Python 3.10+)
You can disallow sampling by setting
allow_sampling=False
when creating the server reference, e.g.:
sampling_disallowed.py
from
pydantic_ai.mcp
import
MCPServerStdio
server
=
MCPServerStdio
(
command
=
'python'
,
args
=
[
'generate_svg.py'
],
allow_sampling
=
False
,
)