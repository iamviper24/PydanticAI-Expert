PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models
Table of contents
models
KnownModelName
ModelRequestParameters
Model
request
request_stream
customize_request_parameters
model_name
profile
system
base_url
StreamedResponse
__aiter__
get
usage
model_name
timestamp
ALLOW_MODEL_REQUESTS
check_allow_model_requests
override_allow_model_requests
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
models
KnownModelName
ModelRequestParameters
Model
request
request_stream
customize_request_parameters
model_name
profile
system
base_url
StreamedResponse
__aiter__
get
usage
model_name
timestamp
ALLOW_MODEL_REQUESTS
check_allow_model_requests
override_allow_model_requests
pydantic_ai.models
Logic related to making requests to an LLM.
The aim here is to make a common interface for different LLMs, so that the rest of the code can be agnostic to the
specific LLM being used.
KnownModelName
module-attribute
KnownModelName
=
TypeAliasType
(
"KnownModelName"
,
Literal
[
"anthropic:claude-2.0"
,
"anthropic:claude-2.1"
,
"anthropic:claude-3-5-haiku-20241022"
,
"anthropic:claude-3-5-haiku-latest"
,
"anthropic:claude-3-5-sonnet-20240620"
,
"anthropic:claude-3-5-sonnet-20241022"
,
"anthropic:claude-3-5-sonnet-latest"
,
"anthropic:claude-3-7-sonnet-20250219"
,
"anthropic:claude-3-7-sonnet-latest"
,
"anthropic:claude-3-haiku-20240307"
,
"anthropic:claude-3-opus-20240229"
,
"anthropic:claude-3-opus-latest"
,
"anthropic:claude-3-sonnet-20240229"
,
"anthropic:claude-4-opus-20250514"
,
"anthropic:claude-4-sonnet-20250514"
,
"anthropic:claude-opus-4-0"
,
"anthropic:claude-opus-4-20250514"
,
"anthropic:claude-sonnet-4-0"
,
"anthropic:claude-sonnet-4-20250514"
,
"bedrock:amazon.titan-tg1-large"
,
"bedrock:amazon.titan-text-lite-v1"
,
"bedrock:amazon.titan-text-express-v1"
,
"bedrock:us.amazon.nova-pro-v1:0"
,
"bedrock:us.amazon.nova-lite-v1:0"
,
"bedrock:us.amazon.nova-micro-v1:0"
,
"bedrock:anthropic.claude-3-5-sonnet-20241022-v2:0"
,
"bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0"
,
"bedrock:anthropic.claude-3-5-haiku-20241022-v1:0"
,
"bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0"
,
"bedrock:anthropic.claude-instant-v1"
,
"bedrock:anthropic.claude-v2:1"
,
"bedrock:anthropic.claude-v2"
,
"bedrock:anthropic.claude-3-sonnet-20240229-v1:0"
,
"bedrock:us.anthropic.claude-3-sonnet-20240229-v1:0"
,
"bedrock:anthropic.claude-3-haiku-20240307-v1:0"
,
"bedrock:us.anthropic.claude-3-haiku-20240307-v1:0"
,
"bedrock:anthropic.claude-3-opus-20240229-v1:0"
,
"bedrock:us.anthropic.claude-3-opus-20240229-v1:0"
,
"bedrock:anthropic.claude-3-5-sonnet-20240620-v1:0"
,
"bedrock:us.anthropic.claude-3-5-sonnet-20240620-v1:0"
,
"bedrock:anthropic.claude-3-7-sonnet-20250219-v1:0"
,
"bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0"
,
"bedrock:anthropic.claude-opus-4-20250514-v1:0"
,
"bedrock:us.anthropic.claude-opus-4-20250514-v1:0"
,
"bedrock:anthropic.claude-sonnet-4-20250514-v1:0"
,
"bedrock:us.anthropic.claude-sonnet-4-20250514-v1:0"
,
"bedrock:cohere.command-text-v14"
,
"bedrock:cohere.command-r-v1:0"
,
"bedrock:cohere.command-r-plus-v1:0"
,
"bedrock:cohere.command-light-text-v14"
,
"bedrock:meta.llama3-8b-instruct-v1:0"
,
"bedrock:meta.llama3-70b-instruct-v1:0"
,
"bedrock:meta.llama3-1-8b-instruct-v1:0"
,
"bedrock:us.meta.llama3-1-8b-instruct-v1:0"
,
"bedrock:meta.llama3-1-70b-instruct-v1:0"
,
"bedrock:us.meta.llama3-1-70b-instruct-v1:0"
,
"bedrock:meta.llama3-1-405b-instruct-v1:0"
,
"bedrock:us.meta.llama3-2-11b-instruct-v1:0"
,
"bedrock:us.meta.llama3-2-90b-instruct-v1:0"
,
"bedrock:us.meta.llama3-2-1b-instruct-v1:0"
,
"bedrock:us.meta.llama3-2-3b-instruct-v1:0"
,
"bedrock:us.meta.llama3-3-70b-instruct-v1:0"
,
"bedrock:mistral.mistral-7b-instruct-v0:2"
,
"bedrock:mistral.mixtral-8x7b-instruct-v0:1"
,
"bedrock:mistral.mistral-large-2402-v1:0"
,
"bedrock:mistral.mistral-large-2407-v1:0"
,
"claude-2.0"
,
"claude-2.1"
,
"claude-3-5-haiku-20241022"
,
"claude-3-5-haiku-latest"
,
"claude-3-5-sonnet-20240620"
,
"claude-3-5-sonnet-20241022"
,
"claude-3-5-sonnet-latest"
,
"claude-3-7-sonnet-20250219"
,
"claude-3-7-sonnet-latest"
,
"claude-3-haiku-20240307"
,
"claude-3-opus-20240229"
,
"claude-3-opus-latest"
,
"claude-3-sonnet-20240229"
,
"claude-4-opus-20250514"
,
"claude-4-sonnet-20250514"
,
"claude-opus-4-0"
,
"claude-opus-4-20250514"
,
"claude-sonnet-4-0"
,
"claude-sonnet-4-20250514"
,
"cohere:c4ai-aya-expanse-32b"
,
"cohere:c4ai-aya-expanse-8b"
,
"cohere:command"
,
"cohere:command-light"
,
"cohere:command-light-nightly"
,
"cohere:command-nightly"
,
"cohere:command-r"
,
"cohere:command-r-03-2024"
,
"cohere:command-r-08-2024"
,
"cohere:command-r-plus"
,
"cohere:command-r-plus-04-2024"
,
"cohere:command-r-plus-08-2024"
,
"cohere:command-r7b-12-2024"
,
"deepseek:deepseek-chat"
,
"deepseek:deepseek-reasoner"
,
"google-gla:gemini-1.5-flash"
,
"google-gla:gemini-1.5-flash-8b"
,
"google-gla:gemini-1.5-pro"
,
"google-gla:gemini-1.0-pro"
,
"google-gla:gemini-2.0-flash"
,
"google-gla:gemini-2.0-flash-lite-preview-02-05"
,
"google-gla:gemini-2.0-pro-exp-02-05"
,
"google-gla:gemini-2.5-flash-preview-05-20"
,
"google-gla:gemini-2.5-flash"
,
"google-gla:gemini-2.5-flash-lite-preview-06-17"
,
"google-gla:gemini-2.5-pro-exp-03-25"
,
"google-gla:gemini-2.5-pro-preview-05-06"
,
"google-gla:gemini-2.5-pro"
,
"google-vertex:gemini-1.5-flash"
,
"google-vertex:gemini-1.5-flash-8b"
,
"google-vertex:gemini-1.5-pro"
,
"google-vertex:gemini-1.0-pro"
,
"google-vertex:gemini-2.0-flash"
,
"google-vertex:gemini-2.0-flash-lite-preview-02-05"
,
"google-vertex:gemini-2.0-pro-exp-02-05"
,
"google-vertex:gemini-2.5-flash-preview-05-20"
,
"google-vertex:gemini-2.5-flash"
,
"google-vertex:gemini-2.5-flash-lite-preview-06-17"
,
"google-vertex:gemini-2.5-pro-exp-03-25"
,
"google-vertex:gemini-2.5-pro-preview-05-06"
,
"google-vertex:gemini-2.5-pro"
,
"gpt-3.5-turbo"
,
"gpt-3.5-turbo-0125"
,
"gpt-3.5-turbo-0301"
,
"gpt-3.5-turbo-0613"
,
"gpt-3.5-turbo-1106"
,
"gpt-3.5-turbo-16k"
,
"gpt-3.5-turbo-16k-0613"
,
"gpt-4"
,
"gpt-4-0125-preview"
,
"gpt-4-0314"
,
"gpt-4-0613"
,
"gpt-4-1106-preview"
,
"gpt-4-32k"
,
"gpt-4-32k-0314"
,
"gpt-4-32k-0613"
,
"gpt-4-turbo"
,
"gpt-4-turbo-2024-04-09"
,
"gpt-4-turbo-preview"
,
"gpt-4-vision-preview"
,
"gpt-4.1"
,
"gpt-4.1-2025-04-14"
,
"gpt-4.1-mini"
,
"gpt-4.1-mini-2025-04-14"
,
"gpt-4.1-nano"
,
"gpt-4.1-nano-2025-04-14"
,
"gpt-4o"
,
"gpt-4o-2024-05-13"
,
"gpt-4o-2024-08-06"
,
"gpt-4o-2024-11-20"
,
"gpt-4o-audio-preview"
,
"gpt-4o-audio-preview-2024-10-01"
,
"gpt-4o-audio-preview-2024-12-17"
,
"gpt-4o-mini"
,
"gpt-4o-mini-2024-07-18"
,
"gpt-4o-mini-audio-preview"
,
"gpt-4o-mini-audio-preview-2024-12-17"
,
"gpt-4o-mini-search-preview"
,
"gpt-4o-mini-search-preview-2025-03-11"
,
"gpt-4o-search-preview"
,
"gpt-4o-search-preview-2025-03-11"
,
"groq:distil-whisper-large-v3-en"
,
"groq:gemma2-9b-it"
,
"groq:llama-3.3-70b-versatile"
,
"groq:llama-3.1-8b-instant"
,
"groq:llama-guard-3-8b"
,
"groq:llama3-70b-8192"
,
"groq:llama3-8b-8192"
,
"groq:whisper-large-v3"
,
"groq:whisper-large-v3-turbo"
,
"groq:playai-tts"
,
"groq:playai-tts-arabic"
,
"groq:qwen-qwq-32b"
,
"groq:mistral-saba-24b"
,
"groq:qwen-2.5-coder-32b"
,
"groq:qwen-2.5-32b"
,
"groq:deepseek-r1-distill-qwen-32b"
,
"groq:deepseek-r1-distill-llama-70b"
,
"groq:llama-3.3-70b-specdec"
,
"groq:llama-3.2-1b-preview"
,
"groq:llama-3.2-3b-preview"
,
"groq:llama-3.2-11b-vision-preview"
,
"groq:llama-3.2-90b-vision-preview"
,
"heroku:claude-3-5-haiku"
,
"heroku:claude-3-5-sonnet-latest"
,
"heroku:claude-3-7-sonnet"
,
"heroku:claude-4-sonnet"
,
"heroku:claude-3-haiku"
,
"mistral:codestral-latest"
,
"mistral:mistral-large-latest"
,
"mistral:mistral-moderation-latest"
,
"mistral:mistral-small-latest"
,
"o1"
,
"o1-2024-12-17"
,
"o1-mini"
,
"o1-mini-2024-09-12"
,
"o1-preview"
,
"o1-preview-2024-09-12"
,
"o3"
,
"o3-2025-04-16"
,
"o3-mini"
,
"o3-mini-2025-01-31"
,
"openai:chatgpt-4o-latest"
,
"openai:gpt-3.5-turbo"
,
"openai:gpt-3.5-turbo-0125"
,
"openai:gpt-3.5-turbo-0301"
,
"openai:gpt-3.5-turbo-0613"
,
"openai:gpt-3.5-turbo-1106"
,
"openai:gpt-3.5-turbo-16k"
,
"openai:gpt-3.5-turbo-16k-0613"
,
"openai:gpt-4"
,
"openai:gpt-4-0125-preview"
,
"openai:gpt-4-0314"
,
"openai:gpt-4-0613"
,
"openai:gpt-4-1106-preview"
,
"openai:gpt-4-32k"
,
"openai:gpt-4-32k-0314"
,
"openai:gpt-4-32k-0613"
,
"openai:gpt-4-turbo"
,
"openai:gpt-4-turbo-2024-04-09"
,
"openai:gpt-4-turbo-preview"
,
"openai:gpt-4-vision-preview"
,
"openai:gpt-4.1"
,
"openai:gpt-4.1-2025-04-14"
,
"openai:gpt-4.1-mini"
,
"openai:gpt-4.1-mini-2025-04-14"
,
"openai:gpt-4.1-nano"
,
"openai:gpt-4.1-nano-2025-04-14"
,
"openai:gpt-4o"
,
"openai:gpt-4o-2024-05-13"
,
"openai:gpt-4o-2024-08-06"
,
"openai:gpt-4o-2024-11-20"
,
"openai:gpt-4o-audio-preview"
,
"openai:gpt-4o-audio-preview-2024-10-01"
,
"openai:gpt-4o-audio-preview-2024-12-17"
,
"openai:gpt-4o-mini"
,
"openai:gpt-4o-mini-2024-07-18"
,
"openai:gpt-4o-mini-audio-preview"
,
"openai:gpt-4o-mini-audio-preview-2024-12-17"
,
"openai:gpt-4o-mini-search-preview"
,
"openai:gpt-4o-mini-search-preview-2025-03-11"
,
"openai:gpt-4o-search-preview"
,
"openai:gpt-4o-search-preview-2025-03-11"
,
"openai:o1"
,
"openai:o1-2024-12-17"
,
"openai:o1-mini"
,
"openai:o1-mini-2024-09-12"
,
"openai:o1-preview"
,
"openai:o1-preview-2024-09-12"
,
"openai:o3"
,
"openai:o3-2025-04-16"
,
"openai:o3-mini"
,
"openai:o3-mini-2025-01-31"
,
"openai:o4-mini"
,
"openai:o4-mini-2025-04-16"
,
"test"
,
],
)
Known model names that can be used with the
model
parameter of
Agent
.
KnownModelName
is provided as a concise way to specify a model.
ModelRequestParameters
dataclass
Configuration for an agent's request to a model, specifically related to tools and output handling.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
306
307
308
309
310
311
312
313
314
315
316
317
@dataclass
(
repr
=
False
)
class
ModelRequestParameters
:
"""Configuration for an agent's request to a model, specifically related to tools and output handling."""
function_tools
:
list
[
ToolDefinition
]
=
field
(
default_factory
=
list
)
output_mode
:
OutputMode
=
'text'
output_object
:
OutputObjectDefinition
|
None
=
None
output_tools
:
list
[
ToolDefinition
]
=
field
(
default_factory
=
list
)
allow_text_output
:
bool
=
True
__repr__
=
_utils
.
dataclasses_no_defaults_repr
Model
Bases:
ABC
Abstract class for a model.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
class
Model
(
ABC
):
"""Abstract class for a model."""
_profile
:
ModelProfileSpec
|
None
=
None
@abstractmethod
async
def
request
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
ModelResponse
:
"""Make a request to the model."""
raise
NotImplementedError
()
@asynccontextmanager
async
def
request_stream
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
AsyncIterator
[
StreamedResponse
]:
"""Make a request to the model and return a streaming response."""
# This method is not required, but you need to implement it if you want to support streamed responses
raise
NotImplementedError
(
f
'Streamed requests not supported by this
{
self
.
__class__
.
__name__
}
'
)
# yield is required to make this a generator for type checking
# noinspection PyUnreachableCode
yield
# pragma: no cover
def
customize_request_parameters
(
self
,
model_request_parameters
:
ModelRequestParameters
)
->
ModelRequestParameters
:
"""Customize the request parameters for the model.
This method can be overridden by subclasses to modify the request parameters before sending them to the model.
In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary
for vendor/model-specific reasons.
"""
if
transformer
:=
self
.
profile
.
json_schema_transformer
:
model_request_parameters
=
replace
(
model_request_parameters
,
function_tools
=
[
_customize_tool_def
(
transformer
,
t
)
for
t
in
model_request_parameters
.
function_tools
],
output_tools
=
[
_customize_tool_def
(
transformer
,
t
)
for
t
in
model_request_parameters
.
output_tools
],
)
if
output_object
:=
model_request_parameters
.
output_object
:
model_request_parameters
=
replace
(
model_request_parameters
,
output_object
=
_customize_output_object
(
transformer
,
output_object
),
)
return
model_request_parameters
@property
@abstractmethod
def
model_name
(
self
)
->
str
:
"""The model name."""
raise
NotImplementedError
()
@cached_property
def
profile
(
self
)
->
ModelProfile
:
"""The model profile."""
_profile
=
self
.
_profile
if
callable
(
_profile
):
_profile
=
_profile
(
self
.
model_name
)
if
_profile
is
None
:
return
DEFAULT_PROFILE
return
_profile
@property
@abstractmethod
def
system
(
self
)
->
str
:
"""The system / model provider, ex: openai.
Use to populate the `gen_ai.system` OpenTelemetry semantic convention attribute,
so should use well-known values listed in
https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system
when applicable.
"""
raise
NotImplementedError
()
@property
def
base_url
(
self
)
->
str
|
None
:
"""The base URL for the provider API, if available."""
return
None
@staticmethod
def
_get_instructions
(
messages
:
list
[
ModelMessage
])
->
str
|
None
:
"""Get instructions from the first ModelRequest found when iterating messages in reverse.
In the case that a "mock" request was generated to include a tool-return part for a result tool,
we want to use the instructions from the second-to-most-recent request (which should correspond to the
original request that generated the response that resulted in the tool-return part).
"""
last_two_requests
:
list
[
ModelRequest
]
=
[]
for
message
in
reversed
(
messages
):
if
isinstance
(
message
,
ModelRequest
):
last_two_requests
.
append
(
message
)
if
len
(
last_two_requests
)
==
2
:
break
if
message
.
instructions
is
not
None
:
return
message
.
instructions
# If we don't have two requests, and we didn't already return instructions, there are definitely not any:
if
len
(
last_two_requests
)
!=
2
:
return
None
most_recent_request
=
last_two_requests
[
0
]
second_most_recent_request
=
last_two_requests
[
1
]
# If we've gotten this far and the most recent request consists of only tool-return parts or retry-prompt parts,
# we use the instructions from the second-to-most-recent request. This is necessary because when handling
# result tools, we generate a "mock" ModelRequest with a tool-return part for it, and that ModelRequest will not
# have the relevant instructions from the agent.
# While it's possible that you could have a message history where the most recent request has only tool returns,
# I believe there is no way to achieve that would _change_ the instructions without manually crafting the most
# recent message. That might make sense in principle for some usage pattern, but it's enough of an edge case
# that I think it's not worth worrying about, since you can work around this by inserting another ModelRequest
# with no parts at all immediately before the request that has the tool calls (that works because we only look
# at the two most recent ModelRequests here).
# If you have a use case where this causes pain, please open a GitHub issue and we can discuss alternatives.
if
all
(
p
.
part_kind
==
'tool-return'
or
p
.
part_kind
==
'retry-prompt'
for
p
in
most_recent_request
.
parts
):
return
second_most_recent_request
.
instructions
return
None
request
abstractmethod
async
request
(
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
ModelResponse
Make a request to the model.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
325
326
327
328
329
330
331
332
333
@abstractmethod
async
def
request
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
ModelResponse
:
"""Make a request to the model."""
raise
NotImplementedError
()
request_stream
async
request_stream
(
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
AsyncIterator
[
StreamedResponse
]
Make a request to the model and return a streaming response.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
335
336
337
338
339
340
341
342
343
344
345
346
347
@asynccontextmanager
async
def
request_stream
(
self
,
messages
:
list
[
ModelMessage
],
model_settings
:
ModelSettings
|
None
,
model_request_parameters
:
ModelRequestParameters
,
)
->
AsyncIterator
[
StreamedResponse
]:
"""Make a request to the model and return a streaming response."""
# This method is not required, but you need to implement it if you want to support streamed responses
raise
NotImplementedError
(
f
'Streamed requests not supported by this
{
self
.
__class__
.
__name__
}
'
)
# yield is required to make this a generator for type checking
# noinspection PyUnreachableCode
yield
# pragma: no cover
customize_request_parameters
customize_request_parameters
(
model_request_parameters
:
ModelRequestParameters
,
)
->
ModelRequestParameters
Customize the request parameters for the model.
This method can be overridden by subclasses to modify the request parameters before sending them to the model.
In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary
for vendor/model-specific reasons.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
def
customize_request_parameters
(
self
,
model_request_parameters
:
ModelRequestParameters
)
->
ModelRequestParameters
:
"""Customize the request parameters for the model.
This method can be overridden by subclasses to modify the request parameters before sending them to the model.
In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary
for vendor/model-specific reasons.
"""
if
transformer
:=
self
.
profile
.
json_schema_transformer
:
model_request_parameters
=
replace
(
model_request_parameters
,
function_tools
=
[
_customize_tool_def
(
transformer
,
t
)
for
t
in
model_request_parameters
.
function_tools
],
output_tools
=
[
_customize_tool_def
(
transformer
,
t
)
for
t
in
model_request_parameters
.
output_tools
],
)
if
output_object
:=
model_request_parameters
.
output_object
:
model_request_parameters
=
replace
(
model_request_parameters
,
output_object
=
_customize_output_object
(
transformer
,
output_object
),
)
return
model_request_parameters
model_name
abstractmethod
property
model_name
:
str
The model name.
profile
cached
property
profile
:
ModelProfile
The model profile.
system
abstractmethod
property
system
:
str
The system / model provider, ex: openai.
Use to populate the
gen_ai.system
OpenTelemetry semantic convention attribute,
so should use well-known values listed in
https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system
when applicable.
base_url
property
base_url
:
str
|
None
The base URL for the provider API, if available.
StreamedResponse
dataclass
Bases:
ABC
Streamed response from an LLM when calling a tool.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
@dataclass
class
StreamedResponse
(
ABC
):
"""Streamed response from an LLM when calling a tool."""
_parts_manager
:
ModelResponsePartsManager
=
field
(
default_factory
=
ModelResponsePartsManager
,
init
=
False
)
_event_iterator
:
AsyncIterator
[
ModelResponseStreamEvent
]
|
None
=
field
(
default
=
None
,
init
=
False
)
_usage
:
Usage
=
field
(
default_factory
=
Usage
,
init
=
False
)
def
__aiter__
(
self
)
->
AsyncIterator
[
ModelResponseStreamEvent
]:
"""Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s."""
if
self
.
_event_iterator
is
None
:
self
.
_event_iterator
=
self
.
_get_event_iterator
()
return
self
.
_event_iterator
@abstractmethod
async
def
_get_event_iterator
(
self
)
->
AsyncIterator
[
ModelResponseStreamEvent
]:
"""Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.
This method should be implemented by subclasses to translate the vendor-specific stream of events into
pydantic_ai-format events.
It should use the `_parts_manager` to handle deltas, and should update the `_usage` attributes as it goes.
"""
raise
NotImplementedError
()
# noinspection PyUnreachableCode
yield
def
get
(
self
)
->
ModelResponse
:
"""Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far."""
return
ModelResponse
(
parts
=
self
.
_parts_manager
.
get_parts
(),
model_name
=
self
.
model_name
,
timestamp
=
self
.
timestamp
,
usage
=
self
.
usage
(),
)
def
usage
(
self
)
->
Usage
:
"""Get the usage of the response so far. This will not be the final usage until the stream is exhausted."""
return
self
.
_usage
@property
@abstractmethod
def
model_name
(
self
)
->
str
:
"""Get the model name of the response."""
raise
NotImplementedError
()
@property
@abstractmethod
def
timestamp
(
self
)
->
datetime
:
"""Get the timestamp of the response."""
raise
NotImplementedError
()
__aiter__
__aiter__
()
->
AsyncIterator
[
ModelResponseStreamEvent
]
Stream the response as an async iterable of
ModelResponseStreamEvent
s.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
457
458
459
460
461
def
__aiter__
(
self
)
->
AsyncIterator
[
ModelResponseStreamEvent
]:
"""Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s."""
if
self
.
_event_iterator
is
None
:
self
.
_event_iterator
=
self
.
_get_event_iterator
()
return
self
.
_event_iterator
get
get
()
->
ModelResponse
Build a
ModelResponse
from the data received from the stream so far.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
476
477
478
479
480
481
482
483
def
get
(
self
)
->
ModelResponse
:
"""Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far."""
return
ModelResponse
(
parts
=
self
.
_parts_manager
.
get_parts
(),
model_name
=
self
.
model_name
,
timestamp
=
self
.
timestamp
,
usage
=
self
.
usage
(),
)
usage
usage
()
->
Usage
Get the usage of the response so far. This will not be the final usage until the stream is exhausted.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
485
486
487
def
usage
(
self
)
->
Usage
:
"""Get the usage of the response so far. This will not be the final usage until the stream is exhausted."""
return
self
.
_usage
model_name
abstractmethod
property
model_name
:
str
Get the model name of the response.
timestamp
abstractmethod
property
timestamp
:
datetime
Get the timestamp of the response.
ALLOW_MODEL_REQUESTS
module-attribute
ALLOW_MODEL_REQUESTS
=
True
Whether to allow requests to models.
This global setting allows you to disable request to most models, e.g. to make sure you don't accidentally
make costly requests to a model during tests.
The testing models
TestModel
and
FunctionModel
are no affected by this setting.
check_allow_model_requests
check_allow_model_requests
()
->
None
Check if model requests are allowed.
If you're defining your own models that have costs or latency associated with their use, you should call this in
Model.request
and
Model.request_stream
.
Raises:
Type
Description
RuntimeError
If model requests are not allowed.
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
513
514
515
516
517
518
519
520
521
522
523
def
check_allow_model_requests
()
->
None
:
"""Check if model requests are allowed.
If you're defining your own models that have costs or latency associated with their use, you should call this in
[`Model.request`][pydantic_ai.models.Model.request] and [`Model.request_stream`][pydantic_ai.models.Model.request_stream].
Raises:
RuntimeError: If model requests are not allowed.
"""
if
not
ALLOW_MODEL_REQUESTS
:
raise
RuntimeError
(
'Model requests are not allowed, since ALLOW_MODEL_REQUESTS is False'
)
override_allow_model_requests
override_allow_model_requests
(
allow_model_requests
:
bool
,
)
->
Iterator
[
None
]
Context manager to temporarily override
ALLOW_MODEL_REQUESTS
.
Parameters:
Name
Type
Description
Default
allow_model_requests
bool
Whether to allow model requests within the context.
required
Source code in
pydantic_ai_slim/pydantic_ai/models/__init__.py
526
527
528
529
530
531
532
533
534
535
536
537
538
539
@contextmanager
def
override_allow_model_requests
(
allow_model_requests
:
bool
)
->
Iterator
[
None
]:
"""Context manager to temporarily override [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS].
Args:
allow_model_requests: Whether to allow model requests within the context.
"""
global
ALLOW_MODEL_REQUESTS
old_value
=
ALLOW_MODEL_REQUESTS
ALLOW_MODEL_REQUESTS
=
allow_model_requests
# pyright: ignore[reportConstantRedefinition]
try
:
yield
finally
:
ALLOW_MODEL_REQUESTS
=
old_value
# pyright: ignore[reportConstantRedefinition]