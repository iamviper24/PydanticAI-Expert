PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Multi-agent Applications
Table of contents
Agent delegation
Agent delegation and dependencies
Programmatic agent hand-off
Pydantic Graphs
Examples
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
Agent delegation
Agent delegation and dependencies
Programmatic agent hand-off
Pydantic Graphs
Examples
Multi-agent Applications
There are roughly four levels of complexity when building applications with PydanticAI:
Single agent workflows — what most of the
pydantic_ai
documentation covers
Agent delegation
— agents using another agent via tools
Programmatic agent hand-off
— one agent runs, then application code calls another agent
Graph based control flow
— for the most complex cases, a graph-based state machine can be used to control the execution of multiple agents
Of course, you can combine multiple strategies in a single application.
Agent delegation
"Agent delegation" refers to the scenario where an agent delegates work to another agent, then takes back control when the delegate agent (the agent called from within a tool) finishes.
If you want to hand off control to another agent completely, without coming back to the first agent, you can use an
output function
.
Since agents are stateless and designed to be global, you do not need to include the agent itself in agent
dependencies
.
You'll generally want to pass
ctx.usage
to the
usage
keyword argument of the delegate agent run so usage within that run counts towards the total usage of the parent agent run.
Multiple models
Agent delegation doesn't need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final
result.usage()
of the run will not be possible, but you can still use
UsageLimits
to avoid unexpected costs.
agent_delegation_simple.py
from
pydantic_ai
import
Agent
,
RunContext
from
pydantic_ai.usage
import
UsageLimits
joke_selection_agent
=
Agent
(
# (1)!
'openai:gpt-4o'
,
system_prompt
=
(
'Use the `joke_factory` to generate some jokes, then choose the best. '
'You must return just a single joke.'
),
)
joke_generation_agent
=
Agent
(
# (2)!
'google-gla:gemini-1.5-flash'
,
output_type
=
list
[
str
]
)
@joke_selection_agent
.
tool
async
def
joke_factory
(
ctx
:
RunContext
[
None
],
count
:
int
)
->
list
[
str
]:
r
=
await
joke_generation_agent
.
run
(
# (3)!
f
'Please generate
{
count
}
jokes.'
,
usage
=
ctx
.
usage
,
# (4)!
)
return
r
.
output
# (5)!
result
=
joke_selection_agent
.
run_sync
(
'Tell me a joke.'
,
usage_limits
=
UsageLimits
(
request_limit
=
5
,
total_tokens_limit
=
300
),
)
print
(
result
.
output
)
#> Did you hear about the toothpaste scandal? They called it Colgate.
print
(
result
.
usage
())
#> Usage(requests=3, request_tokens=204, response_tokens=24, total_tokens=228)
The "parent" or controlling agent.
The "delegate" agent, which is called from within a tool of the parent agent.
Call the delegate agent from within a tool of the parent agent.
Pass the usage from the parent agent to the delegate agent so the final
result.usage()
includes the usage from both agents.
Since the function returns
list
[
str
]
, and the
output_type
of
joke_generation_agent
is also
list
[
str
]
, we can simply return
r
.
output
from the tool.
(This example is complete, it can be run "as is")
The control flow for this example is pretty simple and can be summarised as follows:
graph TD
  START --> joke_selection_agent
  joke_selection_agent --> joke_factory["joke_factory (tool)"]
  joke_factory --> joke_generation_agent
  joke_generation_agent --> joke_factory
  joke_factory --> joke_selection_agent
  joke_selection_agent --> END
Agent delegation and dependencies
Generally the delegate agent needs to either have the same
dependencies
as the calling agent, or dependencies which are a subset of the calling agent's dependencies.
Initializing dependencies
We say "generally" above since there's nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent.
agent_delegation_deps.py
from
dataclasses
import
dataclass
import
httpx
from
pydantic_ai
import
Agent
,
RunContext
@dataclass
class
ClientAndKey
:
# (1)!
http_client
:
httpx
.
AsyncClient
api_key
:
str
joke_selection_agent
=
Agent
(
'openai:gpt-4o'
,
deps_type
=
ClientAndKey
,
# (2)!
system_prompt
=
(
'Use the `joke_factory` tool to generate some jokes on the given subject, '
'then choose the best. You must return just a single joke.'
),
)
joke_generation_agent
=
Agent
(
'gemini-1.5-flash'
,
deps_type
=
ClientAndKey
,
# (4)!
output_type
=
list
[
str
],
system_prompt
=
(
'Use the "get_jokes" tool to get some jokes on the given subject, '
'then extract each joke into a list.'
),
)
@joke_selection_agent
.
tool
async
def
joke_factory
(
ctx
:
RunContext
[
ClientAndKey
],
count
:
int
)
->
list
[
str
]:
r
=
await
joke_generation_agent
.
run
(
f
'Please generate
{
count
}
jokes.'
,
deps
=
ctx
.
deps
,
# (3)!
usage
=
ctx
.
usage
,
)
return
r
.
output
@joke_generation_agent
.
tool
# (5)!
async
def
get_jokes
(
ctx
:
RunContext
[
ClientAndKey
],
count
:
int
)
->
str
:
response
=
await
ctx
.
deps
.
http_client
.
get
(
'https://example.com'
,
params
=
{
'count'
:
count
},
headers
=
{
'Authorization'
:
f
'Bearer
{
ctx
.
deps
.
api_key
}
'
},
)
response
.
raise_for_status
()
return
response
.
text
async
def
main
():
async
with
httpx
.
AsyncClient
()
as
client
:
deps
=
ClientAndKey
(
client
,
'foobar'
)
result
=
await
joke_selection_agent
.
run
(
'Tell me a joke.'
,
deps
=
deps
)
print
(
result
.
output
)
#> Did you hear about the toothpaste scandal? They called it Colgate.
print
(
result
.
usage
())
# (6)!
#> Usage(requests=4, request_tokens=309, response_tokens=32, total_tokens=341)
Define a dataclass to hold the client and API key dependencies.
Set the
deps_type
of the calling agent —
joke_selection_agent
here.
Pass the dependencies to the delegate agent's run method within the tool call.
Also set the
deps_type
of the delegate agent —
joke_generation_agent
here.
Define a tool on the delegate agent that uses the dependencies to make an HTTP request.
Usage now includes 4 requests — 2 from the calling agent and 2 from the delegate agent.
(This example is complete, it can be run "as is" — you'll need to add
asyncio.run(main())
to run
main
)
This example shows how even a fairly simple agent delegation can lead to a complex control flow:
graph TD
  START --> joke_selection_agent
  joke_selection_agent --> joke_factory["joke_factory (tool)"]
  joke_factory --> joke_generation_agent
  joke_generation_agent --> get_jokes["get_jokes (tool)"]
  get_jokes --> http_request["HTTP request"]
  http_request --> get_jokes
  get_jokes --> joke_generation_agent
  joke_generation_agent --> joke_factory
  joke_factory --> joke_selection_agent
  joke_selection_agent --> END
Programmatic agent hand-off
"Programmatic agent hand-off" refers to the scenario where multiple agents are called in succession, with application code and/or a human in the loop responsible for deciding which agent to call next.
Here agents don't need to use the same deps.
Here we show two agents used in succession, the first to find a flight and the second to extract the user's seat preference.
programmatic_handoff.py
from
typing
import
Literal
,
Union
from
pydantic
import
BaseModel
,
Field
from
rich.prompt
import
Prompt
from
pydantic_ai
import
Agent
,
RunContext
from
pydantic_ai.messages
import
ModelMessage
from
pydantic_ai.usage
import
Usage
,
UsageLimits
class
FlightDetails
(
BaseModel
):
flight_number
:
str
class
Failed
(
BaseModel
):
"""Unable to find a satisfactory choice."""
flight_search_agent
=
Agent
[
None
,
Union
[
FlightDetails
,
Failed
]](
# (1)!
'openai:gpt-4o'
,
output_type
=
Union
[
FlightDetails
,
Failed
],
# type: ignore
system_prompt
=
(
'Use the "flight_search" tool to find a flight '
'from the given origin to the given destination.'
),
)
@flight_search_agent
.
tool
# (2)!
async
def
flight_search
(
ctx
:
RunContext
[
None
],
origin
:
str
,
destination
:
str
)
->
Union
[
FlightDetails
,
None
]:
# in reality, this would call a flight search API or
# use a browser to scrape a flight search website
return
FlightDetails
(
flight_number
=
'AK456'
)
usage_limits
=
UsageLimits
(
request_limit
=
15
)
# (3)!
async
def
find_flight
(
usage
:
Usage
)
->
Union
[
FlightDetails
,
None
]:
# (4)!
message_history
:
Union
[
list
[
ModelMessage
],
None
]
=
None
for
_
in
range
(
3
):
prompt
=
Prompt
.
ask
(
'Where would you like to fly from and to?'
,
)
result
=
await
flight_search_agent
.
run
(
prompt
,
message_history
=
message_history
,
usage
=
usage
,
usage_limits
=
usage_limits
,
)
if
isinstance
(
result
.
output
,
FlightDetails
):
return
result
.
output
else
:
message_history
=
result
.
all_messages
(
output_tool_return_content
=
'Please try again.'
)
class
SeatPreference
(
BaseModel
):
row
:
int
=
Field
(
ge
=
1
,
le
=
30
)
seat
:
Literal
[
'A'
,
'B'
,
'C'
,
'D'
,
'E'
,
'F'
]
# This agent is responsible for extracting the user's seat selection
seat_preference_agent
=
Agent
[
None
,
Union
[
SeatPreference
,
Failed
]](
# (5)!
'openai:gpt-4o'
,
output_type
=
Union
[
SeatPreference
,
Failed
],
# type: ignore
system_prompt
=
(
"Extract the user's seat preference. "
'Seats A and F are window seats. '
'Row 1 is the front row and has extra leg room. '
'Rows 14, and 20 also have extra leg room. '
),
)
async
def
find_seat
(
usage
:
Usage
)
->
SeatPreference
:
# (6)!
message_history
:
Union
[
list
[
ModelMessage
],
None
]
=
None
while
True
:
answer
=
Prompt
.
ask
(
'What seat would you like?'
)
result
=
await
seat_preference_agent
.
run
(
answer
,
message_history
=
message_history
,
usage
=
usage
,
usage_limits
=
usage_limits
,
)
if
isinstance
(
result
.
output
,
SeatPreference
):
return
result
.
output
else
:
print
(
'Could not understand seat preference. Please try again.'
)
message_history
=
result
.
all_messages
()
async
def
main
():
# (7)!
usage
:
Usage
=
Usage
()
opt_flight_details
=
await
find_flight
(
usage
)
if
opt_flight_details
is
not
None
:
print
(
f
'Flight found:
{
opt_flight_details
.
flight_number
}
'
)
#> Flight found: AK456
seat_preference
=
await
find_seat
(
usage
)
print
(
f
'Seat preference:
{
seat_preference
}
'
)
#> Seat preference: row=1 seat='A'
Define the first agent, which finds a flight. We use an explicit type annotation until
PEP-747
lands, see
structured output
. We use a union as the output type so the model can communicate if it's unable to find a satisfactory choice; internally, each member of the union will be registered as a separate tool.
Define a tool on the agent to find a flight. In this simple case we could dispense with the tool and just define the agent to return structured data, then search for a flight, but in more complex scenarios the tool would be necessary.
Define usage limits for the entire app.
Define a function to find a flight, which asks the user for their preferences and then calls the agent to find a flight.
As with
flight_search_agent
above, we use an explicit type annotation to define the agent.
Define a function to find the user's seat preference, which asks the user for their seat preference and then calls the agent to extract the seat preference.
Now that we've put our logic for running each agent into separate functions, our main app becomes very simple.
(This example is complete, it can be run "as is" — you'll need to add
asyncio.run(main())
to run
main
)
The control flow for this example can be summarised as follows:
graph TB
  START --> ask_user_flight["ask user for flight"]

  subgraph find_flight
    flight_search_agent --> ask_user_flight
    ask_user_flight --> flight_search_agent
  end

  flight_search_agent --> ask_user_seat["ask user for seat"]
  flight_search_agent --> END

  subgraph find_seat
    seat_preference_agent --> ask_user_seat
    ask_user_seat --> seat_preference_agent
  end

  seat_preference_agent --> END
Pydantic Graphs
See the
graph
documentation on when and how to use graphs.
Examples
The following examples demonstrate how to use dependencies in PydanticAI:
Flight booking