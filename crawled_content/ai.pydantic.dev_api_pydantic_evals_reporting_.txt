PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.reporting
Table of contents
reporting
ReportCase
name
inputs
metadata
expected_output
output
ReportCaseAggregate
average
EvaluationReport
name
cases
print
console_table
__str__
RenderValueConfig
RenderNumberConfig
value_formatter
diff_formatter
diff_atol
diff_rtol
diff_increase_style
diff_decrease_style
EvaluationRenderer
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
reporting
ReportCase
name
inputs
metadata
expected_output
output
ReportCaseAggregate
average
EvaluationReport
name
cases
print
console_table
__str__
RenderValueConfig
RenderNumberConfig
value_formatter
diff_formatter
diff_atol
diff_rtol
diff_increase_style
diff_decrease_style
EvaluationRenderer
pydantic_evals.reporting
ReportCase
dataclass
Bases:
Generic
[
InputsT
,
OutputT
,
MetadataT
]
A single case in an evaluation report.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
@dataclass
class
ReportCase
(
Generic
[
InputsT
,
OutputT
,
MetadataT
]):
"""A single case in an evaluation report."""
name
:
str
"""The name of the [case][pydantic_evals.Case]."""
inputs
:
InputsT
"""The inputs to the task, from [`Case.inputs`][pydantic_evals.Case.inputs]."""
metadata
:
MetadataT
|
None
"""Any metadata associated with the case, from [`Case.metadata`][pydantic_evals.Case.metadata]."""
expected_output
:
OutputT
|
None
"""The expected output of the task, from [`Case.expected_output`][pydantic_evals.Case.expected_output]."""
output
:
OutputT
"""The output of the task execution."""
metrics
:
dict
[
str
,
float
|
int
]
attributes
:
dict
[
str
,
Any
]
scores
:
dict
[
str
,
EvaluationResult
[
int
|
float
]]
labels
:
dict
[
str
,
EvaluationResult
[
str
]]
assertions
:
dict
[
str
,
EvaluationResult
[
bool
]]
task_duration
:
float
total_duration
:
float
# includes evaluator execution time
# TODO(DavidM): Drop these once we can reference child spans in details panel:
trace_id
:
str
span_id
:
str
name
instance-attribute
name
:
str
The name of the
case
.
inputs
instance-attribute
inputs
:
InputsT
The inputs to the task, from
Case.inputs
.
metadata
instance-attribute
metadata
:
MetadataT
|
None
Any metadata associated with the case, from
Case.metadata
.
expected_output
instance-attribute
expected_output
:
OutputT
|
None
The expected output of the task, from
Case.expected_output
.
output
instance-attribute
output
:
OutputT
The output of the task execution.
ReportCaseAggregate
Bases:
BaseModel
A synthetic case that summarizes a set of cases.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
class
ReportCaseAggregate
(
BaseModel
):
"""A synthetic case that summarizes a set of cases."""
name
:
str
scores
:
dict
[
str
,
float
|
int
]
labels
:
dict
[
str
,
dict
[
str
,
float
]]
metrics
:
dict
[
str
,
float
|
int
]
assertions
:
float
|
None
task_duration
:
float
total_duration
:
float
@staticmethod
def
average
(
cases
:
list
[
ReportCase
])
->
ReportCaseAggregate
:
"""Produce a synthetic "summary" case by averaging quantitative attributes."""
num_cases
=
len
(
cases
)
if
num_cases
==
0
:
return
ReportCaseAggregate
(
name
=
'Averages'
,
scores
=
{},
labels
=
{},
metrics
=
{},
assertions
=
None
,
task_duration
=
0.0
,
total_duration
=
0.0
,
)
def
_scores_averages
(
scores_by_name
:
list
[
dict
[
str
,
int
|
float
|
bool
]])
->
dict
[
str
,
float
]:
counts_by_name
:
dict
[
str
,
int
]
=
defaultdict
(
int
)
sums_by_name
:
dict
[
str
,
float
]
=
defaultdict
(
float
)
for
sbn
in
scores_by_name
:
for
name
,
score
in
sbn
.
items
():
counts_by_name
[
name
]
+=
1
sums_by_name
[
name
]
+=
score
return
{
name
:
sums_by_name
[
name
]
/
counts_by_name
[
name
]
for
name
in
sums_by_name
}
def
_labels_averages
(
labels_by_name
:
list
[
dict
[
str
,
str
]])
->
dict
[
str
,
dict
[
str
,
float
]]:
counts_by_name
:
dict
[
str
,
int
]
=
defaultdict
(
int
)
sums_by_name
:
dict
[
str
,
dict
[
str
,
float
]]
=
defaultdict
(
lambda
:
defaultdict
(
float
))
for
lbn
in
labels_by_name
:
for
name
,
label
in
lbn
.
items
():
counts_by_name
[
name
]
+=
1
sums_by_name
[
name
][
label
]
+=
1
return
{
name
:
{
value
:
count
/
counts_by_name
[
name
]
for
value
,
count
in
sums_by_name
[
name
]
.
items
()}
for
name
in
sums_by_name
}
average_task_duration
=
sum
(
case
.
task_duration
for
case
in
cases
)
/
num_cases
average_total_duration
=
sum
(
case
.
total_duration
for
case
in
cases
)
/
num_cases
# average_assertions: dict[str, float] = _scores_averages([{k: v.value for k, v in case.scores.items()} for case in cases])
average_scores
:
dict
[
str
,
float
]
=
_scores_averages
(
[{
k
:
v
.
value
for
k
,
v
in
case
.
scores
.
items
()}
for
case
in
cases
]
)
average_labels
:
dict
[
str
,
dict
[
str
,
float
]]
=
_labels_averages
(
[{
k
:
v
.
value
for
k
,
v
in
case
.
labels
.
items
()}
for
case
in
cases
]
)
average_metrics
:
dict
[
str
,
float
]
=
_scores_averages
([
case
.
metrics
for
case
in
cases
])
average_assertions
:
float
|
None
=
None
n_assertions
=
sum
(
len
(
case
.
assertions
)
for
case
in
cases
)
if
n_assertions
>
0
:
n_passing
=
sum
(
1
for
case
in
cases
for
assertion
in
case
.
assertions
.
values
()
if
assertion
.
value
)
average_assertions
=
n_passing
/
n_assertions
return
ReportCaseAggregate
(
name
=
'Averages'
,
scores
=
average_scores
,
labels
=
average_labels
,
metrics
=
average_metrics
,
assertions
=
average_assertions
,
task_duration
=
average_task_duration
,
total_duration
=
average_total_duration
,
)
average
staticmethod
average
(
cases
:
list
[
ReportCase
])
->
ReportCaseAggregate
Produce a synthetic "summary" case by averaging quantitative attributes.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
@staticmethod
def
average
(
cases
:
list
[
ReportCase
])
->
ReportCaseAggregate
:
"""Produce a synthetic "summary" case by averaging quantitative attributes."""
num_cases
=
len
(
cases
)
if
num_cases
==
0
:
return
ReportCaseAggregate
(
name
=
'Averages'
,
scores
=
{},
labels
=
{},
metrics
=
{},
assertions
=
None
,
task_duration
=
0.0
,
total_duration
=
0.0
,
)
def
_scores_averages
(
scores_by_name
:
list
[
dict
[
str
,
int
|
float
|
bool
]])
->
dict
[
str
,
float
]:
counts_by_name
:
dict
[
str
,
int
]
=
defaultdict
(
int
)
sums_by_name
:
dict
[
str
,
float
]
=
defaultdict
(
float
)
for
sbn
in
scores_by_name
:
for
name
,
score
in
sbn
.
items
():
counts_by_name
[
name
]
+=
1
sums_by_name
[
name
]
+=
score
return
{
name
:
sums_by_name
[
name
]
/
counts_by_name
[
name
]
for
name
in
sums_by_name
}
def
_labels_averages
(
labels_by_name
:
list
[
dict
[
str
,
str
]])
->
dict
[
str
,
dict
[
str
,
float
]]:
counts_by_name
:
dict
[
str
,
int
]
=
defaultdict
(
int
)
sums_by_name
:
dict
[
str
,
dict
[
str
,
float
]]
=
defaultdict
(
lambda
:
defaultdict
(
float
))
for
lbn
in
labels_by_name
:
for
name
,
label
in
lbn
.
items
():
counts_by_name
[
name
]
+=
1
sums_by_name
[
name
][
label
]
+=
1
return
{
name
:
{
value
:
count
/
counts_by_name
[
name
]
for
value
,
count
in
sums_by_name
[
name
]
.
items
()}
for
name
in
sums_by_name
}
average_task_duration
=
sum
(
case
.
task_duration
for
case
in
cases
)
/
num_cases
average_total_duration
=
sum
(
case
.
total_duration
for
case
in
cases
)
/
num_cases
# average_assertions: dict[str, float] = _scores_averages([{k: v.value for k, v in case.scores.items()} for case in cases])
average_scores
:
dict
[
str
,
float
]
=
_scores_averages
(
[{
k
:
v
.
value
for
k
,
v
in
case
.
scores
.
items
()}
for
case
in
cases
]
)
average_labels
:
dict
[
str
,
dict
[
str
,
float
]]
=
_labels_averages
(
[{
k
:
v
.
value
for
k
,
v
in
case
.
labels
.
items
()}
for
case
in
cases
]
)
average_metrics
:
dict
[
str
,
float
]
=
_scores_averages
([
case
.
metrics
for
case
in
cases
])
average_assertions
:
float
|
None
=
None
n_assertions
=
sum
(
len
(
case
.
assertions
)
for
case
in
cases
)
if
n_assertions
>
0
:
n_passing
=
sum
(
1
for
case
in
cases
for
assertion
in
case
.
assertions
.
values
()
if
assertion
.
value
)
average_assertions
=
n_passing
/
n_assertions
return
ReportCaseAggregate
(
name
=
'Averages'
,
scores
=
average_scores
,
labels
=
average_labels
,
metrics
=
average_metrics
,
assertions
=
average_assertions
,
task_duration
=
average_task_duration
,
total_duration
=
average_total_duration
,
)
EvaluationReport
dataclass
Bases:
Generic
[
InputsT
,
OutputT
,
MetadataT
]
A report of the results of evaluating a model on a set of cases.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
@dataclass
class
EvaluationReport
(
Generic
[
InputsT
,
OutputT
,
MetadataT
]):
"""A report of the results of evaluating a model on a set of cases."""
name
:
str
"""The name of the report."""
cases
:
list
[
ReportCase
[
InputsT
,
OutputT
,
MetadataT
]]
"""The cases in the report."""
def
averages
(
self
)
->
ReportCaseAggregate
:
return
ReportCaseAggregate
.
average
(
self
.
cases
)
def
print
(
self
,
width
:
int
|
None
=
None
,
baseline
:
EvaluationReport
[
InputsT
,
OutputT
,
MetadataT
]
|
None
=
None
,
include_input
:
bool
=
False
,
include_metadata
:
bool
=
False
,
include_expected_output
:
bool
=
False
,
include_output
:
bool
=
False
,
include_durations
:
bool
=
True
,
include_total_duration
:
bool
=
False
,
include_removed_cases
:
bool
=
False
,
include_averages
:
bool
=
True
,
input_config
:
RenderValueConfig
|
None
=
None
,
metadata_config
:
RenderValueConfig
|
None
=
None
,
output_config
:
RenderValueConfig
|
None
=
None
,
score_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
label_configs
:
dict
[
str
,
RenderValueConfig
]
|
None
=
None
,
metric_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
duration_config
:
RenderNumberConfig
|
None
=
None
,
):
# pragma: no cover
"""Print this report to the console, optionally comparing it to a baseline report.
If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.
"""
table
=
self
.
console_table
(
baseline
=
baseline
,
include_input
=
include_input
,
include_metadata
=
include_metadata
,
include_expected_output
=
include_expected_output
,
include_output
=
include_output
,
include_durations
=
include_durations
,
include_total_duration
=
include_total_duration
,
include_removed_cases
=
include_removed_cases
,
include_averages
=
include_averages
,
input_config
=
input_config
,
metadata_config
=
metadata_config
,
output_config
=
output_config
,
score_configs
=
score_configs
,
label_configs
=
label_configs
,
metric_configs
=
metric_configs
,
duration_config
=
duration_config
,
)
Console
(
width
=
width
)
.
print
(
table
)
def
console_table
(
self
,
baseline
:
EvaluationReport
[
InputsT
,
OutputT
,
MetadataT
]
|
None
=
None
,
include_input
:
bool
=
False
,
include_metadata
:
bool
=
False
,
include_expected_output
:
bool
=
False
,
include_output
:
bool
=
False
,
include_durations
:
bool
=
True
,
include_total_duration
:
bool
=
False
,
include_removed_cases
:
bool
=
False
,
include_averages
:
bool
=
True
,
input_config
:
RenderValueConfig
|
None
=
None
,
metadata_config
:
RenderValueConfig
|
None
=
None
,
output_config
:
RenderValueConfig
|
None
=
None
,
score_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
label_configs
:
dict
[
str
,
RenderValueConfig
]
|
None
=
None
,
metric_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
duration_config
:
RenderNumberConfig
|
None
=
None
,
)
->
Table
:
"""Return a table containing the data from this report, or the diff between this report and a baseline report.
Optionally include input and output details.
"""
renderer
=
EvaluationRenderer
(
include_input
=
include_input
,
include_metadata
=
include_metadata
,
include_expected_output
=
include_expected_output
,
include_output
=
include_output
,
include_durations
=
include_durations
,
include_total_duration
=
include_total_duration
,
include_removed_cases
=
include_removed_cases
,
include_averages
=
include_averages
,
input_config
=
{
**
_DEFAULT_VALUE_CONFIG
,
**
(
input_config
or
{})},
metadata_config
=
{
**
_DEFAULT_VALUE_CONFIG
,
**
(
metadata_config
or
{})},
output_config
=
output_config
or
_DEFAULT_VALUE_CONFIG
,
score_configs
=
score_configs
or
{},
label_configs
=
label_configs
or
{},
metric_configs
=
metric_configs
or
{},
duration_config
=
duration_config
or
_DEFAULT_DURATION_CONFIG
,
)
if
baseline
is
None
:
return
renderer
.
build_table
(
self
)
else
:
# pragma: no cover
return
renderer
.
build_diff_table
(
self
,
baseline
)
def
__str__
(
self
)
->
str
:
# pragma: lax no cover
"""Return a string representation of the report."""
table
=
self
.
console_table
()
io_file
=
StringIO
()
Console
(
file
=
io_file
)
.
print
(
table
)
return
io_file
.
getvalue
()
name
instance-attribute
name
:
str
The name of the report.
cases
instance-attribute
cases
:
list
[
ReportCase
[
InputsT
,
OutputT
,
MetadataT
]]
The cases in the report.
print
print
(
width
:
int
|
None
=
None
,
baseline
:
(
EvaluationReport
[
InputsT
,
OutputT
,
MetadataT
]
|
None
)
=
None
,
include_input
:
bool
=
False
,
include_metadata
:
bool
=
False
,
include_expected_output
:
bool
=
False
,
include_output
:
bool
=
False
,
include_durations
:
bool
=
True
,
include_total_duration
:
bool
=
False
,
include_removed_cases
:
bool
=
False
,
include_averages
:
bool
=
True
,
input_config
:
RenderValueConfig
|
None
=
None
,
metadata_config
:
RenderValueConfig
|
None
=
None
,
output_config
:
RenderValueConfig
|
None
=
None
,
score_configs
:
(
dict
[
str
,
RenderNumberConfig
]
|
None
)
=
None
,
label_configs
:
(
dict
[
str
,
RenderValueConfig
]
|
None
)
=
None
,
metric_configs
:
(
dict
[
str
,
RenderNumberConfig
]
|
None
)
=
None
,
duration_config
:
RenderNumberConfig
|
None
=
None
,
)
Print this report to the console, optionally comparing it to a baseline report.
If you want more control over the output, use
console_table
instead and pass it to
rich.Console.print
.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
def
print
(
self
,
width
:
int
|
None
=
None
,
baseline
:
EvaluationReport
[
InputsT
,
OutputT
,
MetadataT
]
|
None
=
None
,
include_input
:
bool
=
False
,
include_metadata
:
bool
=
False
,
include_expected_output
:
bool
=
False
,
include_output
:
bool
=
False
,
include_durations
:
bool
=
True
,
include_total_duration
:
bool
=
False
,
include_removed_cases
:
bool
=
False
,
include_averages
:
bool
=
True
,
input_config
:
RenderValueConfig
|
None
=
None
,
metadata_config
:
RenderValueConfig
|
None
=
None
,
output_config
:
RenderValueConfig
|
None
=
None
,
score_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
label_configs
:
dict
[
str
,
RenderValueConfig
]
|
None
=
None
,
metric_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
duration_config
:
RenderNumberConfig
|
None
=
None
,
):
# pragma: no cover
"""Print this report to the console, optionally comparing it to a baseline report.
If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.
"""
table
=
self
.
console_table
(
baseline
=
baseline
,
include_input
=
include_input
,
include_metadata
=
include_metadata
,
include_expected_output
=
include_expected_output
,
include_output
=
include_output
,
include_durations
=
include_durations
,
include_total_duration
=
include_total_duration
,
include_removed_cases
=
include_removed_cases
,
include_averages
=
include_averages
,
input_config
=
input_config
,
metadata_config
=
metadata_config
,
output_config
=
output_config
,
score_configs
=
score_configs
,
label_configs
=
label_configs
,
metric_configs
=
metric_configs
,
duration_config
=
duration_config
,
)
Console
(
width
=
width
)
.
print
(
table
)
console_table
console_table
(
baseline
:
(
EvaluationReport
[
InputsT
,
OutputT
,
MetadataT
]
|
None
)
=
None
,
include_input
:
bool
=
False
,
include_metadata
:
bool
=
False
,
include_expected_output
:
bool
=
False
,
include_output
:
bool
=
False
,
include_durations
:
bool
=
True
,
include_total_duration
:
bool
=
False
,
include_removed_cases
:
bool
=
False
,
include_averages
:
bool
=
True
,
input_config
:
RenderValueConfig
|
None
=
None
,
metadata_config
:
RenderValueConfig
|
None
=
None
,
output_config
:
RenderValueConfig
|
None
=
None
,
score_configs
:
(
dict
[
str
,
RenderNumberConfig
]
|
None
)
=
None
,
label_configs
:
(
dict
[
str
,
RenderValueConfig
]
|
None
)
=
None
,
metric_configs
:
(
dict
[
str
,
RenderNumberConfig
]
|
None
)
=
None
,
duration_config
:
RenderNumberConfig
|
None
=
None
,
)
->
Table
Return a table containing the data from this report, or the diff between this report and a baseline report.
Optionally include input and output details.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
def
console_table
(
self
,
baseline
:
EvaluationReport
[
InputsT
,
OutputT
,
MetadataT
]
|
None
=
None
,
include_input
:
bool
=
False
,
include_metadata
:
bool
=
False
,
include_expected_output
:
bool
=
False
,
include_output
:
bool
=
False
,
include_durations
:
bool
=
True
,
include_total_duration
:
bool
=
False
,
include_removed_cases
:
bool
=
False
,
include_averages
:
bool
=
True
,
input_config
:
RenderValueConfig
|
None
=
None
,
metadata_config
:
RenderValueConfig
|
None
=
None
,
output_config
:
RenderValueConfig
|
None
=
None
,
score_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
label_configs
:
dict
[
str
,
RenderValueConfig
]
|
None
=
None
,
metric_configs
:
dict
[
str
,
RenderNumberConfig
]
|
None
=
None
,
duration_config
:
RenderNumberConfig
|
None
=
None
,
)
->
Table
:
"""Return a table containing the data from this report, or the diff between this report and a baseline report.
Optionally include input and output details.
"""
renderer
=
EvaluationRenderer
(
include_input
=
include_input
,
include_metadata
=
include_metadata
,
include_expected_output
=
include_expected_output
,
include_output
=
include_output
,
include_durations
=
include_durations
,
include_total_duration
=
include_total_duration
,
include_removed_cases
=
include_removed_cases
,
include_averages
=
include_averages
,
input_config
=
{
**
_DEFAULT_VALUE_CONFIG
,
**
(
input_config
or
{})},
metadata_config
=
{
**
_DEFAULT_VALUE_CONFIG
,
**
(
metadata_config
or
{})},
output_config
=
output_config
or
_DEFAULT_VALUE_CONFIG
,
score_configs
=
score_configs
or
{},
label_configs
=
label_configs
or
{},
metric_configs
=
metric_configs
or
{},
duration_config
=
duration_config
or
_DEFAULT_DURATION_CONFIG
,
)
if
baseline
is
None
:
return
renderer
.
build_table
(
self
)
else
:
# pragma: no cover
return
renderer
.
build_diff_table
(
self
,
baseline
)
__str__
__str__
()
->
str
Return a string representation of the report.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
256
257
258
259
260
261
def
__str__
(
self
)
->
str
:
# pragma: lax no cover
"""Return a string representation of the report."""
table
=
self
.
console_table
()
io_file
=
StringIO
()
Console
(
file
=
io_file
)
.
print
(
table
)
return
io_file
.
getvalue
()
RenderValueConfig
Bases:
TypedDict
A configuration for rendering a values in an Evaluation report.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
267
268
269
270
271
272
273
class
RenderValueConfig
(
TypedDict
,
total
=
False
):
"""A configuration for rendering a values in an Evaluation report."""
value_formatter
:
str
|
Callable
[[
Any
],
str
]
diff_checker
:
Callable
[[
Any
,
Any
],
bool
]
|
None
diff_formatter
:
Callable
[[
Any
,
Any
],
str
|
None
]
|
None
diff_style
:
str
RenderNumberConfig
Bases:
TypedDict
A configuration for rendering a particular score or metric in an Evaluation report.
See the implementation of
_RenderNumber
for more clarity on how these parameters affect the rendering.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
class
RenderNumberConfig
(
TypedDict
,
total
=
False
):
"""A configuration for rendering a particular score or metric in an Evaluation report.
See the implementation of `_RenderNumber` for more clarity on how these parameters affect the rendering.
"""
value_formatter
:
str
|
Callable
[[
float
|
int
],
str
]
"""The logic to use for formatting values.
* If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures.
* You can also use a custom string format spec, e.g. '{:.3f}'
* You can also use a custom function, e.g. lambda x: f'{x:.3f}'
"""
diff_formatter
:
str
|
Callable
[[
float
|
int
,
float
|
int
],
str
|
None
]
|
None
"""The logic to use for formatting details about the diff.
The strings produced by the value_formatter will always be included in the reports, but the diff_formatter is
used to produce additional text about the difference between the old and new values, such as the absolute or
relative difference.
* If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four
significant figures, and will include the percentage change.
* You can also use a custom string format spec, e.g. '{:+.3f}'
* You can also use a custom function, e.g. lambda x: f'{x:+.3f}'.
If this function returns None, no extra diff text will be added.
* You can also use None to never generate extra diff text.
"""
diff_atol
:
float
"""The absolute tolerance for considering a difference "significant".
A difference is "significant" if `abs(new - old) < self.diff_atol + self.diff_rtol * abs(old)`.
If a difference is not significant, it will not have the diff styles applied. Note that we still show
both the rendered before and after values in the diff any time they differ, even if the difference is not
significant. (If the rendered values are exactly the same, we only show the value once.)
If not provided, use 1e-6.
"""
diff_rtol
:
float
"""The relative tolerance for considering a difference "significant".
See the description of `diff_atol` for more details about what makes a difference "significant".
If not provided, use 0.001 if all values are ints, otherwise 0.05.
"""
diff_increase_style
:
str
"""The style to apply to diffed values that have a significant increase.
See the description of `diff_atol` for more details about what makes a difference "significant".
If not provided, use green for scores and red for metrics. You can also use arbitrary `rich` styles, such as "bold red".
"""
diff_decrease_style
:
str
"""The style to apply to diffed values that have significant decrease.
See the description of `diff_atol` for more details about what makes a difference "significant".
If not provided, use red for scores and green for metrics. You can also use arbitrary `rich` styles, such as "bold red".
"""
value_formatter
instance-attribute
value_formatter
:
str
|
Callable
[[
float
|
int
],
str
]
The logic to use for formatting values.
If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures.
You can also use a custom string format spec, e.g. '{:.3f}'
You can also use a custom function, e.g. lambda x: f'{x:.3f}'
diff_formatter
instance-attribute
diff_formatter
:
(
str
|
Callable
[[
float
|
int
,
float
|
int
],
str
|
None
]
|
None
)
The logic to use for formatting details about the diff.
The strings produced by the value_formatter will always be included in the reports, but the diff_formatter is
used to produce additional text about the difference between the old and new values, such as the absolute or
relative difference.
If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four
    significant figures, and will include the percentage change.
You can also use a custom string format spec, e.g. '{:+.3f}'
You can also use a custom function, e.g. lambda x: f'{x:+.3f}'.
    If this function returns None, no extra diff text will be added.
You can also use None to never generate extra diff text.
diff_atol
instance-attribute
diff_atol
:
float
The absolute tolerance for considering a difference "significant".
A difference is "significant" if
abs(new - old) < self.diff_atol + self.diff_rtol * abs(old)
.
If a difference is not significant, it will not have the diff styles applied. Note that we still show
both the rendered before and after values in the diff any time they differ, even if the difference is not
significant. (If the rendered values are exactly the same, we only show the value once.)
If not provided, use 1e-6.
diff_rtol
instance-attribute
diff_rtol
:
float
The relative tolerance for considering a difference "significant".
See the description of
diff_atol
for more details about what makes a difference "significant".
If not provided, use 0.001 if all values are ints, otherwise 0.05.
diff_increase_style
instance-attribute
diff_increase_style
:
str
The style to apply to diffed values that have a significant increase.
See the description of
diff_atol
for more details about what makes a difference "significant".
If not provided, use green for scores and red for metrics. You can also use arbitrary
rich
styles, such as "bold red".
diff_decrease_style
instance-attribute
diff_decrease_style
:
str
The style to apply to diffed values that have significant decrease.
See the description of
diff_atol
for more details about what makes a difference "significant".
If not provided, use red for scores and green for metrics. You can also use arbitrary
rich
styles, such as "bold red".
EvaluationRenderer
dataclass
A class for rendering an EvalReport or the diff between two EvalReports.
Source code in
pydantic_evals/pydantic_evals/reporting/__init__.py
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
@dataclass
class
EvaluationRenderer
:
"""A class for rendering an EvalReport or the diff between two EvalReports."""
# Columns to include
include_input
:
bool
include_metadata
:
bool
include_expected_output
:
bool
include_output
:
bool
include_durations
:
bool
include_total_duration
:
bool
# Rows to include
include_removed_cases
:
bool
include_averages
:
bool
input_config
:
RenderValueConfig
metadata_config
:
RenderValueConfig
output_config
:
RenderValueConfig
score_configs
:
dict
[
str
,
RenderNumberConfig
]
label_configs
:
dict
[
str
,
RenderValueConfig
]
metric_configs
:
dict
[
str
,
RenderNumberConfig
]
duration_config
:
RenderNumberConfig
def
include_scores
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
=
None
):
return
any
(
case
.
scores
for
case
in
self
.
_all_cases
(
report
,
baseline
))
def
include_labels
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
=
None
):
return
any
(
case
.
labels
for
case
in
self
.
_all_cases
(
report
,
baseline
))
def
include_metrics
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
=
None
):
return
any
(
case
.
metrics
for
case
in
self
.
_all_cases
(
report
,
baseline
))
def
include_assertions
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
=
None
):
return
any
(
case
.
assertions
for
case
in
self
.
_all_cases
(
report
,
baseline
))
def
_all_cases
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
)
->
list
[
ReportCase
]:
if
not
baseline
:
return
report
.
cases
else
:
return
report
.
cases
+
self
.
_baseline_cases_to_include
(
report
,
baseline
)
def
_baseline_cases_to_include
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
)
->
list
[
ReportCase
]:
if
self
.
include_removed_cases
:
return
baseline
.
cases
report_case_names
=
{
case
.
name
for
case
in
report
.
cases
}
return
[
case
for
case
in
baseline
.
cases
if
case
.
name
in
report_case_names
]
def
_get_case_renderer
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
=
None
)
->
ReportCaseRenderer
:
input_renderer
=
_ValueRenderer
.
from_config
(
self
.
input_config
)
metadata_renderer
=
_ValueRenderer
.
from_config
(
self
.
metadata_config
)
output_renderer
=
_ValueRenderer
.
from_config
(
self
.
output_config
)
score_renderers
=
self
.
_infer_score_renderers
(
report
,
baseline
)
label_renderers
=
self
.
_infer_label_renderers
(
report
,
baseline
)
metric_renderers
=
self
.
_infer_metric_renderers
(
report
,
baseline
)
duration_renderer
=
_NumberRenderer
.
infer_from_config
(
self
.
duration_config
,
'duration'
,
[
x
.
task_duration
for
x
in
self
.
_all_cases
(
report
,
baseline
)]
)
return
ReportCaseRenderer
(
include_input
=
self
.
include_input
,
include_metadata
=
self
.
include_metadata
,
include_expected_output
=
self
.
include_expected_output
,
include_output
=
self
.
include_output
,
include_scores
=
self
.
include_scores
(
report
,
baseline
),
include_labels
=
self
.
include_labels
(
report
,
baseline
),
include_metrics
=
self
.
include_metrics
(
report
,
baseline
),
include_assertions
=
self
.
include_assertions
(
report
,
baseline
),
include_durations
=
self
.
include_durations
,
include_total_duration
=
self
.
include_total_duration
,
input_renderer
=
input_renderer
,
metadata_renderer
=
metadata_renderer
,
output_renderer
=
output_renderer
,
score_renderers
=
score_renderers
,
label_renderers
=
label_renderers
,
metric_renderers
=
metric_renderers
,
duration_renderer
=
duration_renderer
,
)
def
build_table
(
self
,
report
:
EvaluationReport
)
->
Table
:
case_renderer
=
self
.
_get_case_renderer
(
report
)
table
=
case_renderer
.
build_base_table
(
f
'Evaluation Summary:
{
report
.
name
}
'
)
for
case
in
report
.
cases
:
table
.
add_row
(
*
case_renderer
.
build_row
(
case
))
if
self
.
include_averages
:
# pragma: no branch
average
=
report
.
averages
()
table
.
add_row
(
*
case_renderer
.
build_aggregate_row
(
average
))
return
table
def
build_diff_table
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
)
->
Table
:
report_cases
=
report
.
cases
baseline_cases
=
self
.
_baseline_cases_to_include
(
report
,
baseline
)
report_cases_by_id
=
{
case
.
name
:
case
for
case
in
report_cases
}
baseline_cases_by_id
=
{
case
.
name
:
case
for
case
in
baseline_cases
}
diff_cases
:
list
[
tuple
[
ReportCase
,
ReportCase
]]
=
[]
removed_cases
:
list
[
ReportCase
]
=
[]
added_cases
:
list
[
ReportCase
]
=
[]
for
case_id
in
sorted
(
set
(
baseline_cases_by_id
.
keys
())
|
set
(
report_cases_by_id
.
keys
())):
maybe_baseline_case
=
baseline_cases_by_id
.
get
(
case_id
)
maybe_report_case
=
report_cases_by_id
.
get
(
case_id
)
if
maybe_baseline_case
and
maybe_report_case
:
diff_cases
.
append
((
maybe_baseline_case
,
maybe_report_case
))
elif
maybe_baseline_case
:
removed_cases
.
append
(
maybe_baseline_case
)
elif
maybe_report_case
:
added_cases
.
append
(
maybe_report_case
)
else
:
# pragma: no cover
assert
False
,
'This should be unreachable'
case_renderer
=
self
.
_get_case_renderer
(
report
,
baseline
)
diff_name
=
baseline
.
name
if
baseline
.
name
==
report
.
name
else
f
'
{
baseline
.
name
}
→
{
report
.
name
}
'
table
=
case_renderer
.
build_base_table
(
f
'Evaluation Diff:
{
diff_name
}
'
)
for
baseline_case
,
new_case
in
diff_cases
:
table
.
add_row
(
*
case_renderer
.
build_diff_row
(
new_case
,
baseline_case
))
for
case
in
added_cases
:
row
=
case_renderer
.
build_row
(
case
)
row
[
0
]
=
f
'[green]+ Added Case[/]
\n
{
row
[
0
]
}
'
table
.
add_row
(
*
row
)
for
case
in
removed_cases
:
row
=
case_renderer
.
build_row
(
case
)
row
[
0
]
=
f
'[red]- Removed Case[/]
\n
{
row
[
0
]
}
'
table
.
add_row
(
*
row
)
if
self
.
include_averages
:
# pragma: no branch
report_average
=
ReportCaseAggregate
.
average
(
report_cases
)
baseline_average
=
ReportCaseAggregate
.
average
(
baseline_cases
)
table
.
add_row
(
*
case_renderer
.
build_diff_aggregate_row
(
report_average
,
baseline_average
))
return
table
def
_infer_score_renderers
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
)
->
dict
[
str
,
_NumberRenderer
]:
all_cases
=
self
.
_all_cases
(
report
,
baseline
)
values_by_name
:
dict
[
str
,
list
[
float
|
int
]]
=
{}
for
case
in
all_cases
:
for
k
,
score
in
case
.
scores
.
items
():
values_by_name
.
setdefault
(
k
,
[])
.
append
(
score
.
value
)
all_renderers
:
dict
[
str
,
_NumberRenderer
]
=
{}
for
name
,
values
in
values_by_name
.
items
():
merged_config
=
_DEFAULT_NUMBER_CONFIG
.
copy
()
merged_config
.
update
(
self
.
score_configs
.
get
(
name
,
{}))
all_renderers
[
name
]
=
_NumberRenderer
.
infer_from_config
(
merged_config
,
'score'
,
values
)
return
all_renderers
def
_infer_label_renderers
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
)
->
dict
[
str
,
_ValueRenderer
]:
all_cases
=
self
.
_all_cases
(
report
,
baseline
)
all_names
:
set
[
str
]
=
set
()
for
case
in
all_cases
:
for
k
in
case
.
labels
:
all_names
.
add
(
k
)
all_renderers
:
dict
[
str
,
_ValueRenderer
]
=
{}
for
name
in
all_names
:
merged_config
=
_DEFAULT_VALUE_CONFIG
.
copy
()
merged_config
.
update
(
self
.
label_configs
.
get
(
name
,
{}))
all_renderers
[
name
]
=
_ValueRenderer
.
from_config
(
merged_config
)
return
all_renderers
def
_infer_metric_renderers
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
)
->
dict
[
str
,
_NumberRenderer
]:
all_cases
=
self
.
_all_cases
(
report
,
baseline
)
values_by_name
:
dict
[
str
,
list
[
float
|
int
]]
=
{}
for
case
in
all_cases
:
for
k
,
v
in
case
.
metrics
.
items
():
values_by_name
.
setdefault
(
k
,
[])
.
append
(
v
)
all_renderers
:
dict
[
str
,
_NumberRenderer
]
=
{}
for
name
,
values
in
values_by_name
.
items
():
merged_config
=
_DEFAULT_NUMBER_CONFIG
.
copy
()
merged_config
.
update
(
self
.
metric_configs
.
get
(
name
,
{}))
all_renderers
[
name
]
=
_NumberRenderer
.
infer_from_config
(
merged_config
,
'metric'
,
values
)
return
all_renderers
def
_infer_duration_renderer
(
self
,
report
:
EvaluationReport
,
baseline
:
EvaluationReport
|
None
)
->
_NumberRenderer
:
# pragma: no cover
all_cases
=
self
.
_all_cases
(
report
,
baseline
)
all_durations
=
[
x
.
task_duration
for
x
in
all_cases
]
if
self
.
include_total_duration
:
all_durations
+=
[
x
.
total_duration
for
x
in
all_cases
]
return
_NumberRenderer
.
infer_from_config
(
self
.
duration_config
,
'duration'
,
all_durations
)