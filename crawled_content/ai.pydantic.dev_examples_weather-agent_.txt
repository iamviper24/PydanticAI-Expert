PydanticAI
pydantic/pydantic-ai
Introduction
Installation
Getting Help
Contributing
Troubleshooting
Upgrade Guide
Documentation
Documentation
Agents
Models
Models
OpenAI
Anthropic
Gemini
Google
Bedrock
Cohere
Groq
Mistral
Dependencies
Function Tools
Common Tools
Output
Messages and chat history
Unit testing
Debugging and Monitoring
Multi-agent Applications
Graphs
Evals
Image, Audio, Video & Document Input
Thinking
Direct Model Requests
MCP
MCP
Client
Server
MCP Run Python
A2A
Command Line Interface (CLI)
Examples
Examples
Pydantic Model
Weather agent
Weather agent
Table of contents
Running the Example
Example Code
Running the UI
UI Code
Bank support
SQL Generation
Flight booking
RAG
Stream markdown
Stream whales
Chat App with FastAPI
Question Graph
Slack Lead Qualifier with Modal
API Reference
API Reference
pydantic_ai.agent
pydantic_ai.tools
pydantic_ai.common_tools
pydantic_ai.output
pydantic_ai.result
pydantic_ai.messages
pydantic_ai.exceptions
pydantic_ai.settings
pydantic_ai.usage
pydantic_ai.mcp
pydantic_ai.format_as_xml
pydantic_ai.format_prompt
pydantic_ai.direct
pydantic_ai.models
pydantic_ai.models.openai
pydantic_ai.models.anthropic
pydantic_ai.models.bedrock
pydantic_ai.models.cohere
pydantic_ai.models.gemini
pydantic_ai.models.google
pydantic_ai.models.groq
pydantic_ai.models.instrumented
pydantic_ai.models.mistral
pydantic_ai.models.test
pydantic_ai.models.function
pydantic_ai.models.fallback
pydantic_ai.models.wrapper
pydantic_ai.models.mcp_sampling
pydantic_ai.profiles
pydantic_ai.providers
pydantic_graph
pydantic_graph.nodes
pydantic_graph.persistence
pydantic_graph.mermaid
pydantic_graph.exceptions
pydantic_evals.dataset
pydantic_evals.evaluators
pydantic_evals.reporting
pydantic_evals.otel
pydantic_evals.generation
fasta2a
Table of contents
Running the Example
Example Code
Running the UI
UI Code
Weather agent
Example of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.
Demonstrates:
tools
agent dependencies
streaming text responses
Building a
Gradio
UI for the agent
In this case the idea is a "weather" agent — the user can ask for the weather in multiple locations,
the agent will use the
get_lat_lng
tool to get the latitude and longitude of the locations, then use
the
get_weather
tool to get the weather for those locations.
Running the Example
To run this example properly, you might want to add two extra API keys
(Note if either key is missing, the code will fall back to dummy data, so they're not required)
:
A weather API key from
tomorrow.io
set via
WEATHER_API_KEY
A geocoding API key from
geocode.maps.co
set via
GEO_API_KEY
With
dependencies installed and environment variables set
, run:
pip
uv
python
-m
pydantic_ai_examples.weather_agent
uv
run
-m
pydantic_ai_examples.weather_agent
Example Code
weather_agent.py
"""Example of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.
In this case the idea is a "weather" agent — the user can ask for the weather in multiple cities,
the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use
the `get_weather` tool to get the weather.
Run with:
uv run -m pydantic_ai_examples.weather_agent
"""
from
__future__
import
annotations
as
_annotations
import
asyncio
from
dataclasses
import
dataclass
from
typing
import
Any
import
logfire
from
httpx
import
AsyncClient
from
pydantic
import
BaseModel
from
pydantic_ai
import
Agent
,
RunContext
# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured
logfire
.
configure
(
send_to_logfire
=
'if-token-present'
)
logfire
.
instrument_pydantic_ai
()
@dataclass
class
Deps
:
client
:
AsyncClient
weather_agent
=
Agent
(
'openai:gpt-4.1-mini'
,
# 'Be concise, reply with one sentence.' is enough for some models (like openai) to use
# the below tools appropriately, but others like anthropic and gemini require a bit more direction.
instructions
=
'Be concise, reply with one sentence.'
,
deps_type
=
Deps
,
retries
=
2
,
)
class
LatLng
(
BaseModel
):
lat
:
float
lng
:
float
@weather_agent
.
tool
async
def
get_lat_lng
(
ctx
:
RunContext
[
Deps
],
location_description
:
str
)
->
LatLng
:
"""Get the latitude and longitude of a location.
Args:
ctx: The context.
location_description: A description of a location.
"""
# NOTE: the response here will be random, and is not related to the location description.
r
=
await
ctx
.
deps
.
client
.
get
(
'https://demo-endpoints.pydantic.workers.dev/latlng'
,
params
=
{
'location'
:
location_description
},
)
r
.
raise_for_status
()
return
LatLng
.
model_validate_json
(
r
.
content
)
@weather_agent
.
tool
async
def
get_weather
(
ctx
:
RunContext
[
Deps
],
lat
:
float
,
lng
:
float
)
->
dict
[
str
,
Any
]:
"""Get the weather at a location.
Args:
ctx: The context.
lat: Latitude of the location.
lng: Longitude of the location.
"""
# NOTE: the responses here will be random, and are not related to the lat and lng.
temp_response
,
descr_response
=
await
asyncio
.
gather
(
ctx
.
deps
.
client
.
get
(
'https://demo-endpoints.pydantic.workers.dev/number'
,
params
=
{
'min'
:
10
,
'max'
:
30
},
),
ctx
.
deps
.
client
.
get
(
'https://demo-endpoints.pydantic.workers.dev/weather'
,
params
=
{
'lat'
:
lat
,
'lng'
:
lng
},
),
)
temp_response
.
raise_for_status
()
descr_response
.
raise_for_status
()
return
{
'temperature'
:
f
'
{
temp_response
.
text
}
°C'
,
'description'
:
descr_response
.
text
,
}
async
def
main
():
async
with
AsyncClient
()
as
client
:
logfire
.
instrument_httpx
(
client
,
capture_all
=
True
)
deps
=
Deps
(
client
=
client
)
result
=
await
weather_agent
.
run
(
'What is the weather like in London and in Wiltshire?'
,
deps
=
deps
)
print
(
'Response:'
,
result
.
output
)
if
__name__
==
'__main__'
:
asyncio
.
run
(
main
())
Running the UI
You can build multi-turn chat applications for your agent with
Gradio
, a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!
Here's what the UI looks like for the weather agent:
Note, to run the UI, you'll need Python 3.10+.
pip
install
gradio>
=
5
.9.0
python/uv-run
-m
pydantic_ai_examples.weather_agent_gradio
UI Code
weather_agent_gradio.py
from
__future__
import
annotations
as
_annotations
import
json
from
httpx
import
AsyncClient
from
pydantic_ai.messages
import
ToolCallPart
,
ToolReturnPart
from
pydantic_ai_examples.weather_agent
import
Deps
,
weather_agent
try
:
import
gradio
as
gr
except
ImportError
as
e
:
raise
ImportError
(
'Please install gradio with `pip install gradio`. You must use python>=3.10.'
)
from
e
TOOL_TO_DISPLAY_NAME
=
{
'get_lat_lng'
:
'Geocoding API'
,
'get_weather'
:
'Weather API'
}
client
=
AsyncClient
()
deps
=
Deps
(
client
=
client
)
async
def
stream_from_agent
(
prompt
:
str
,
chatbot
:
list
[
dict
],
past_messages
:
list
):
chatbot
.
append
({
'role'
:
'user'
,
'content'
:
prompt
})
yield
gr
.
Textbox
(
interactive
=
False
,
value
=
''
),
chatbot
,
gr
.
skip
()
async
with
weather_agent
.
run_stream
(
prompt
,
deps
=
deps
,
message_history
=
past_messages
)
as
result
:
for
message
in
result
.
new_messages
():
for
call
in
message
.
parts
:
if
isinstance
(
call
,
ToolCallPart
):
call_args
=
call
.
args_as_json_str
()
metadata
=
{
'title'
:
f
'🛠️ Using
{
TOOL_TO_DISPLAY_NAME
[
call
.
tool_name
]
}
'
,
}
if
call
.
tool_call_id
is
not
None
:
metadata
[
'id'
]
=
call
.
tool_call_id
gr_message
=
{
'role'
:
'assistant'
,
'content'
:
'Parameters: '
+
call_args
,
'metadata'
:
metadata
,
}
chatbot
.
append
(
gr_message
)
if
isinstance
(
call
,
ToolReturnPart
):
for
gr_message
in
chatbot
:
if
(
gr_message
.
get
(
'metadata'
,
{})
.
get
(
'id'
,
''
)
==
call
.
tool_call_id
):
gr_message
[
'content'
]
+=
(
f
'
\n
Output:
{
json
.
dumps
(
call
.
content
)
}
'
)
yield
gr
.
skip
(),
chatbot
,
gr
.
skip
()
chatbot
.
append
({
'role'
:
'assistant'
,
'content'
:
''
})
async
for
message
in
result
.
stream_text
():
chatbot
[
-
1
][
'content'
]
=
message
yield
gr
.
skip
(),
chatbot
,
gr
.
skip
()
past_messages
=
result
.
all_messages
()
yield
gr
.
Textbox
(
interactive
=
True
),
gr
.
skip
(),
past_messages
async
def
handle_retry
(
chatbot
,
past_messages
:
list
,
retry_data
:
gr
.
RetryData
):
new_history
=
chatbot
[:
retry_data
.
index
]
previous_prompt
=
chatbot
[
retry_data
.
index
][
'content'
]
past_messages
=
past_messages
[:
retry_data
.
index
]
async
for
update
in
stream_from_agent
(
previous_prompt
,
new_history
,
past_messages
):
yield
update
def
undo
(
chatbot
,
past_messages
:
list
,
undo_data
:
gr
.
UndoData
):
new_history
=
chatbot
[:
undo_data
.
index
]
past_messages
=
past_messages
[:
undo_data
.
index
]
return
chatbot
[
undo_data
.
index
][
'content'
],
new_history
,
past_messages
def
select_data
(
message
:
gr
.
SelectData
)
->
str
:
return
message
.
value
[
'text'
]
with
gr
.
Blocks
()
as
demo
:
gr
.
HTML
(
"""
<div style="display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%">
<img src="https://ai.pydantic.dev/img/logo-white.svg" style="max-width: 200px; height: auto">
<div>
<h1 style="margin: 0 0 1rem 0">Weather Assistant</h1>
<h3 style="margin: 0 0 0.5rem 0">
This assistant answer your weather questions.
</h3>
</div>
</div>
"""
)
past_messages
=
gr
.
State
([])
chatbot
=
gr
.
Chatbot
(
label
=
'Packing Assistant'
,
type
=
'messages'
,
avatar_images
=
(
None
,
'https://ai.pydantic.dev/img/logo-white.svg'
),
examples
=
[
{
'text'
:
'What is the weather like in Miami?'
},
{
'text'
:
'What is the weather like in London?'
},
],
)
with
gr
.
Row
():
prompt
=
gr
.
Textbox
(
lines
=
1
,
show_label
=
False
,
placeholder
=
'What is the weather like in New York City?'
,
)
generation
=
prompt
.
submit
(
stream_from_agent
,
inputs
=
[
prompt
,
chatbot
,
past_messages
],
outputs
=
[
prompt
,
chatbot
,
past_messages
],
)
chatbot
.
example_select
(
select_data
,
None
,
[
prompt
])
chatbot
.
retry
(
handle_retry
,
[
chatbot
,
past_messages
],
[
prompt
,
chatbot
,
past_messages
]
)
chatbot
.
undo
(
undo
,
[
chatbot
,
past_messages
],
[
prompt
,
chatbot
,
past_messages
])
if
__name__
==
'__main__'
:
demo
.
launch
()